{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-supervised and Positive Unlabeled Learning\n",
    "## Learning with Limited Labels: Weak Supervision and Uncertainty-Aware Training\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Keypoints\n",
    "\n",
    "- Semi-supervised learning combines a small amount of labeled data with a large amount of unlabeled data to improve model performance.\n",
    "\n",
    "- Key approaches to semi-supervised learning include self-training, co-training, and multi-view learning.\n",
    "\n",
    "- Self-training iteratively uses a model's most confident predictions on unlabeled data to expand the training set.\n",
    "\n",
    "- Label propagation constructs a graph of data points and spreads labels from known instances to unlabeled ones.\n",
    "\n",
    "- Co-training leverages multiple views of the data, training separate models on each view and allowing them to \n",
    "teach each other.\n",
    "\n",
    "- PU learning is a specialized form of semi-supervised learning that uses only positive and unlabeled data, without explicit negative examples.\n",
    "\n",
    "- PU learning relies on key assumptions, including positive label reliability and label flipping independence.\n",
    "\n",
    "- The Elkan and Noto approach to PU learning involves estimating the probability of a sample being labeled and adjusting predictions accordingly.\n",
    "\n",
    "### Takeaways\n",
    "\n",
    "- Semi-supervised and PU learning techniques can significantly improve model performance when labeled data is scarce or expensive to obtain.\n",
    "\n",
    "- The effectiveness of these methods depends on the validity of their underlying assumptions about the data distribution and labeling process.\n",
    "\n",
    "- Choosing the appropriate semi-supervised or PU learning technique requires careful consideration of the specific problem, available data, and computational resources.\n",
    "\n",
    "- While these methods can be powerful, they also introduce additional complexity and potential sources of error compared to fully supervised approaches.\n",
    "\n",
    "- Practitioners should always validate the results of semi-supervised and PU learning methods against a held-out test set or through cross-validation to ensure their effectiveness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Semi-supervised learning\n",
    "\n",
    "Semi-supervised learning is a type of machine learning that uses a small amount of labeled data along with a large amount of unlabeled data to train models. This approach is particularly useful when labeled data is scarce or expensive to obtain. Semi-supervised learning can be applied to a variety of tasks, including image classification, speech recognition, and natural language processing.\n",
    "\n",
    "### Definition and Motivation\n",
    "\n",
    "**Semi-supervised learning** bridges the gap between supervised learning (which relies entirely on labeled data) and unsupervised learning (which uses only unlabeled data). The primary motivation for using semi-supervised learning is to leverage the abundant unlabeled data available in many practical scenarios to improve model performance when labeled data is limited.\n",
    "\n",
    "### Differences Between Supervised, Unsupervised, and Semi-Supervised Learning\n",
    "\n",
    "- **Supervised Learning**: Uses a fully labeled dataset to train models. Each training example is paired with an output label. Examples include classification and regression tasks.\n",
    "- **Unsupervised Learning**: Uses only unlabeled data to find hidden patterns or intrinsic structures in the input data. Examples include clustering and dimensionality reduction.\n",
    "- **Semi-Supervised Learning**: Combines a small amount of labeled data with a large amount of unlabeled data. The goal is to improve learning accuracy by incorporating the unlabeled data, which can provide additional context and structure.\n",
    "\n",
    "### Approaches to Semi-Supervised Learning\n",
    "\n",
    "Several approaches can be employed in semi-supervised learning, each with its own strengths and weaknesses. The choice of approach depends on the specific problem being addressed:\n",
    "\n",
    "1. **Self-Training**:\n",
    "   - The model is initially trained using the labeled data.\n",
    "   - The model then labels the unlabeled data, and these pseudo-labels are used to retrain the model.\n",
    "   - *Strength*: Simple to implement.\n",
    "   - *Weakness*: The initial model's errors can propagate through the pseudo-labels.\n",
    "\n",
    "2. **Co-Training**:\n",
    "   - Two or more models are trained on different views of the data (different feature sets).\n",
    "   - Each model labels the unlabeled data, and these labels are used to train the other models.\n",
    "   - *Strength*: Can leverage complementary information from different views.\n",
    "   - *Weakness*: Requires that the data can be naturally split into distinct views.\n",
    "\n",
    "3. **Multi-View Learning**:\n",
    "   - Similar to co-training but more generalized.\n",
    "   - Combines multiple views of the data in a unified framework.\n",
    "   - *Strength*: More flexible and can handle complex data structures.\n",
    "   - *Weakness*: Computationally more intensive.\n",
    "\n",
    "### Techniques for Combining Labeled and Unlabeled Data\n",
    "\n",
    "Effectively combining labeled and unlabeled data is a key challenge in semi-supervised learning. Several techniques can be used:\n",
    "\n",
    "- **Generative Models**:\n",
    "  - Models the joint probability distribution of the data and labels.\n",
    "  - Examples include Gaussian Mixture Models and Variational Autoencoders.\n",
    "  - *Strength*: Can generate new data points.\n",
    "  - *Weakness*: Often requires strong assumptions about the data distribution.\n",
    "\n",
    "- **Graph-Based Methods**:\n",
    "  - Represents data as a graph, where nodes are data points and edges represent similarities.\n",
    "  - Labels are propagated through the graph based on these similarities.\n",
    "  - *Strength*: Captures the manifold structure of the data.\n",
    "  - *Weakness*: Can be computationally expensive for large datasets.\n",
    "\n",
    "- **Self-Training Algorithms**:\n",
    "  - Iteratively refine the model by using its own predictions as additional training data.\n",
    "  - *Strength*: Simple and effective for many tasks.\n",
    "  - *Weakness*: Risk of reinforcing initial model biases.\n",
    "\n",
    "\n",
    "### Key Assumptions in Semi-Supervised Learning\n",
    "\n",
    "Semi-supervised learning relies on certain assumptions about the data for it to be effective.  Violating these assumptions may lead to inaccurate predictions. Here are some key assumptions to consider:\n",
    "\n",
    "- **Cluster Assumption (Homogeneity):**  Imagine plotting your data points in a high-dimensional space. The cluster assumption suggests that points close together in this space are likely to belong to the same class or have the same label.  Think of it as \"birds of a feather flock together\" – data instances within a cluster tend to share characteristics. \n",
    "\n",
    "- **Continuity Assumption (Smoothness):** This assumption focuses on regions of varying data density:\n",
    "    - **High-Density Regions:** If two points are very close and lie within a high-density region (lots of data points nearby), they are likely to have the same label. \n",
    "    - **Low-Density Regions:** Points in low-density regions may have different labels even if they are close. Imagine two data clusters separated by a low-density gap; points on opposite sides of this gap are likely from different classes, even if they are spatially near each other.\n",
    "\n",
    "- **Manifold Assumption:** This assumption suggests that high-dimensional data often lie on a lower-dimensional manifold. Think of a folded piece of paper in three-dimensional space—while it exists in 3D, the paper itself represents a 2D surface. In the context of machine learning, the manifold assumption implies that the decision boundary between classes should ideally pass through low-density regions, avoiding cuts through high-density clusters. This is because points within a dense cluster are likely to belong to the same class, as per the previous assumptions. \n",
    "\n",
    "> **Important Note**: These assumptions may not apply universally. Always consider your data's characteristics and problem context.\n",
    ">\n",
    "> **Also**: The effectiveness of semi-supervised learning depends on the assumption that the labeled and unlabeled data come from the same distribution and that the unlabeled data provides useful information about the structure of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "We're using the classic \"Dogs vs Cats\" dataset for this image class. It's structured as follows:\n",
    "\n",
    "**Source and Organization:**\n",
    "\n",
    "- Images are stored in the `data/dogs-vs-cats` directory.\n",
    "- Within this directory, we have separate folders for `train`, `valid`, and `test` sets.\n",
    "- Each of these sets is further divided into `DOG` and `CAT` folders containing the respective images in `.jpg` format.\n",
    "\n",
    "**Dataset Split and Size:**\n",
    "\n",
    "- **Training:** Used to train the model (exact numbers of dog and cat images are printed during preprocessing).\n",
    "- **Validation:**  Used for hyperparameter tuning and model evaluation during training.\n",
    "- **Test:**  A held-out set for the final, unbiased evaluation of the trained model.\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "1. **Shuffling:**  We shuffle the image lists within each split using a fixed random seed to prevent order-based learning and ensure diverse training batches.\n",
    "2. **Feature Extraction with Hugging Face:**\n",
    "   - We use the `google/vit-base-patch16-224-in21k` Vision Transformer model.\n",
    "   - Images are loaded, preprocessed using the `AutoImageProcessor`, and fed to the model.\n",
    "   - The average of the last hidden state is extracted as the feature vector for each image.\n",
    "   - Features are saved using `joblib.dump` for efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import joblib\n",
    "\n",
    "dog_files_train = glob('data/dogs-vs-cats/train/DOG/*.jpg')\n",
    "cat_files_train = glob('data/dogs-vs-cats/train/CAT/*.jpg')\n",
    "dog_files_valid = glob('data/dogs-vs-cats/valid/DOG/*.jpg')\n",
    "cat_files_valid = glob('data/dogs-vs-cats/valid/CAT/*.jpg')\n",
    "dog_files_test = glob('data/dogs-vs-cats/test/DOG/*.jpg')\n",
    "cat_files_test = glob('data/dogs-vs-cats/test/CAT/*.jpg')\n",
    "\n",
    "\n",
    "# Define a specific seed\n",
    "seed = 271828\n",
    "\n",
    "# Create a random number generator with the specific seed\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# Shuffle the files using the random number generator\n",
    "rng.shuffle(dog_files_train)\n",
    "rng.shuffle(cat_files_train)\n",
    "rng.shuffle(dog_files_valid)\n",
    "rng.shuffle(cat_files_valid)\n",
    "rng.shuffle(dog_files_test)\n",
    "rng.shuffle(cat_files_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sizes:\n",
      "  Train - Dog: 9976, Cat: 9971\n",
      "  Valid - Dog: 1239, Cat: 1253\n",
      "  Test  - Dog: 1246, Cat: 1246\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Sizes:\n",
    "  Train - Dog: {len(dog_files_train)}, Cat: {len(cat_files_train)}\n",
    "  Valid - Dog: {len(dog_files_valid)}, Cat: {len(cat_files_valid)}\n",
    "  Test  - Dog: {len(dog_files_test)}, Cat: {len(cat_files_test)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning:\n",
      "\n",
      "`resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\n",
      " 42%|████▏     | 8467/19947 [10:00<13:50, 13.82it/s]/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/PIL/TiffImagePlugin.py:900: UserWarning:\n",
      "\n",
      "Truncated File Read\n",
      "\n",
      "100%|██████████| 19947/19947 [23:27<00:00, 14.18it/s]\n",
      "100%|██████████| 2492/2492 [02:59<00:00, 13.89it/s]\n",
      "100%|██████████| 2492/2492 [03:00<00:00, 13.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['outputs/cats-vs-dogs/test_features.joblib']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def extract_features_from_files(files: List[str], processor: AutoImageProcessor, model: AutoModel) -> List[Tuple[int, str, np.ndarray]]:\n",
    "    \"\"\"Extracts features from a list of image files using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "        files (List[str]): List of file paths to the images.\n",
    "        processor (AutoImageProcessor): The image processor from Hugging Face.\n",
    "        model (AutoModel): The pre-trained model from Hugging Face.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, str, np.ndarray]]: List of tuples containing labels, file paths, and their corresponding extracted features.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # Iterate over each file and extract features\n",
    "    for file in tqdm(files):\n",
    "        # Open the image file\n",
    "        image = Image.open(file)\n",
    "        # Process the image to prepare it for the model\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        # Pass the processed image through the model to get outputs\n",
    "        outputs = model(**inputs)\n",
    "        # Extract the last hidden states from the model outputs\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # Compute the mean of the hidden states to get a feature vector\n",
    "        feature_vector = last_hidden_states.mean(axis=1).squeeze().detach().numpy()\n",
    "        # Assign a label based on the file name (1 for DOG, 0 for CAT)\n",
    "        label = 1 if \"DOG\" in file else 0\n",
    "        # Append the label, file path, and feature vector to the features list\n",
    "        features.append((label, file, feature_vector))\n",
    "    return features\n",
    "\n",
    "# Load the pre-trained model and processor from Hugging Face\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Extract features from training, validation, and test image files\n",
    "train_features = extract_features_from_files(dog_files_train + cat_files_train, processor, model)\n",
    "valid_features = extract_features_from_files(dog_files_valid + cat_files_valid, processor, model)\n",
    "test_features = extract_features_from_files(dog_files_test + cat_files_test, processor, model)\n",
    "\n",
    "# Shuffle the features to ensure random distribution\n",
    "rng.shuffle(train_features)\n",
    "rng.shuffle(valid_features)\n",
    "rng.shuffle(test_features)\n",
    "\n",
    "# Save the extracted features to disk using joblib for later use\n",
    "joblib.dump(train_features, 'outputs/cats-vs-dogs/train_features.joblib')\n",
    "joblib.dump(valid_features, 'outputs/cats-vs-dogs/valid_features.joblib')\n",
    "joblib.dump(test_features, 'outputs/cats-vs-dogs/test_features.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/umap/umap_.py:1945: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "label=Dog<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "Dog",
         "marker": {
          "color": "#636efa",
          "opacity": 0.7,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Dog",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          9.374576,
          11.664153,
          9.865637,
          10.312103,
          12.499628,
          9.058861,
          -4.8311133,
          9.602362,
          8.810123,
          9.967304,
          10.665594,
          12.547597,
          10.759385,
          11.2544565,
          11.456573,
          11.39665,
          10.201269,
          9.809888,
          9.024736,
          9.60324,
          8.658779,
          11.638123,
          9.649982,
          11.3455305,
          10.428519,
          10.39384,
          9.852422,
          9.5251045,
          11.888587,
          12.648829,
          10.604397,
          12.689399,
          8.5019865,
          10.7321825,
          12.591793,
          12.607427,
          12.222824,
          10.155426,
          12.006969,
          10.603145,
          10.155885,
          11.225825,
          12.739087,
          11.058471,
          11.008299,
          12.114425,
          9.673509,
          12.651362,
          11.407324,
          10.6845665,
          11.203926,
          11.81604,
          8.774018,
          8.810974,
          10.604383,
          10.221313,
          9.606398,
          9.225284,
          10.142987,
          10.350686,
          8.436897,
          11.543985,
          11.303921,
          10.604458,
          10.618195,
          9.755954,
          12.501968,
          13.217555,
          9.752474,
          8.655605,
          11.848745,
          12.57742,
          10.231975,
          9.376074,
          10.775823,
          8.878108,
          10.109458,
          10.218341,
          9.539921,
          8.340905,
          11.724513,
          10.56739,
          9.461502,
          9.466263,
          10.277877,
          9.751771,
          11.107198,
          12.708456,
          13.0169115,
          11.925215,
          9.788591,
          12.637917,
          12.482611,
          10.392359,
          10.536123,
          11.0907,
          11.131508,
          9.237232,
          9.7646055,
          8.774872,
          11.198737,
          8.798836,
          9.442295,
          10.467665,
          11.954395,
          9.144053,
          11.948794,
          10.003415,
          9.5735855,
          12.076704,
          10.587363,
          9.334867,
          10.287985,
          11.1750965,
          11.962684,
          9.569915,
          9.47941,
          10.593983,
          9.675892,
          11.259455,
          11.38174,
          10.528152,
          11.386179,
          9.45438,
          11.577645,
          10.452601,
          9.213205,
          12.415732,
          8.466796,
          11.061401,
          9.46121,
          9.577496,
          11.391385,
          12.082205,
          8.590366,
          9.577045,
          9.527692,
          11.373986,
          11.449158,
          12.531991,
          11.636703,
          11.843799,
          11.063353,
          8.809567,
          10.704194,
          12.565459,
          8.893027,
          12.45754,
          9.646777,
          11.214811,
          12.707256,
          9.401738,
          12.481858,
          10.749526,
          11.504996,
          11.640162,
          11.129208,
          11.190167,
          8.971674,
          8.677972,
          11.604097,
          9.447301,
          9.388123,
          9.93747,
          7.3308344,
          10.717713,
          12.327223,
          10.446527,
          9.58409,
          11.12174,
          10.611118,
          10.1520605,
          10.059169,
          12.254628,
          9.563511,
          11.319999,
          11.39681,
          10.298494,
          9.489368,
          12.491463,
          9.405704,
          10.111393,
          8.832863,
          8.4106455,
          12.644372,
          9.44291,
          9.582951,
          9.816214,
          9.544248,
          11.220314,
          9.205283,
          10.314472,
          12.402173,
          9.706954,
          9.645242,
          8.746789,
          12.562019,
          12.63091,
          9.768603,
          9.020623,
          9.456043,
          11.876625,
          8.747432,
          11.390949,
          9.504687,
          8.407416,
          11.032316,
          10.928408,
          12.530276,
          9.982475,
          9.070616,
          8.443099,
          12.201566,
          9.6086645,
          13.196266,
          10.67003,
          10.785904,
          9.93119,
          11.202248,
          11.309414,
          10.903404,
          8.893323,
          11.173387,
          9.254805,
          12.721126,
          11.416752,
          11.700916,
          12.4702425,
          9.868975,
          10.822079,
          11.9263,
          11.136352,
          9.462731,
          8.803405,
          11.448971,
          10.690933,
          10.281317,
          9.430084,
          13.172813,
          9.715711,
          10.640638,
          9.920751,
          10.401981,
          8.424079,
          10.439503,
          8.6870365,
          12.622719,
          12.624333,
          9.645836,
          10.387962,
          12.487219,
          9.378176,
          12.601833,
          9.206104,
          9.856611,
          12.454953,
          10.101888,
          9.302824,
          9.487972,
          11.382568,
          9.345036,
          10.034905,
          9.251897,
          9.451137,
          10.149068,
          10.250022,
          8.490629,
          9.688293,
          10.954486,
          8.926434,
          9.549215,
          12.106051,
          12.000337,
          12.7029505,
          9.530317,
          9.658587,
          9.192922,
          11.563167,
          12.555785,
          12.222757,
          9.943809,
          10.045787,
          9.723103,
          10.165974,
          10.175188,
          12.62138,
          11.679813,
          11.241094,
          11.918924,
          13.179307,
          11.621355,
          10.642039,
          10.518301,
          12.154979,
          12.706792,
          9.617324,
          10.129586,
          11.97546,
          9.893812,
          10.362724,
          12.51103,
          12.005003,
          9.369878,
          11.601947,
          12.283611,
          10.426425,
          10.918849,
          7.3322372,
          9.353343,
          8.857627,
          10.132133,
          9.748957,
          11.34723,
          10.732167,
          12.203293,
          8.704468,
          9.518755,
          11.563194,
          8.633683,
          8.929024,
          9.516285,
          12.629174,
          8.618742,
          9.796865,
          10.32994,
          10.132915,
          9.437654,
          11.648362,
          9.529486,
          12.806064,
          12.118117,
          8.442268,
          11.104362,
          12.569568,
          9.864938,
          8.469657,
          9.82514,
          9.397669,
          10.145265,
          9.854187,
          12.033269,
          11.172558,
          9.354893,
          12.287763,
          9.590646,
          11.083768,
          10.160413,
          12.197133,
          11.02478,
          11.339694,
          12.654846,
          10.110463,
          11.660306,
          9.696242,
          8.723731,
          10.960397,
          13.111755,
          10.584322,
          12.033653,
          12.619227,
          8.428701,
          11.659854,
          10.172877,
          11.245335,
          10.224099,
          10.516684,
          10.303133,
          11.066364,
          8.821754,
          8.615385,
          9.85521,
          11.07374,
          10.396665,
          11.546469,
          12.498086,
          10.004447,
          8.768287,
          9.492311,
          11.1629715,
          12.522201,
          11.085359,
          12.563017,
          10.191258,
          9.439049,
          8.851086,
          9.121806,
          12.811753,
          9.338949,
          12.628545,
          9.866261,
          11.788298,
          11.253642,
          12.623959,
          10.897499,
          13.08763,
          12.261191,
          12.62798,
          13.13653,
          10.787224,
          9.758489,
          11.460318,
          10.975831,
          9.572503,
          10.3544855,
          12.236637,
          9.249052,
          9.5081215,
          9.769501,
          8.467142,
          9.341997,
          12.513409,
          9.749426,
          9.633983,
          11.05203,
          10.97796,
          12.418348,
          8.591653,
          12.611717,
          9.161408,
          11.049745,
          10.547229,
          7.3350782,
          11.743366,
          8.634676,
          12.40333,
          9.463832,
          11.848377,
          9.310811,
          9.731793,
          8.709019,
          12.3542185,
          11.32471,
          9.631568,
          11.648265,
          11.714146,
          10.784294,
          11.045459,
          10.799309,
          12.5745325,
          12.355822,
          10.113781,
          11.444938,
          10.5644245,
          10.385964,
          9.310634,
          9.688365,
          10.033969,
          11.162275,
          10.525592,
          10.030502,
          9.699486,
          11.509383,
          11.22546,
          9.169395,
          11.222782,
          11.630824,
          11.261352,
          9.987841,
          8.787816,
          12.675235,
          11.9297285,
          9.723648,
          8.83135,
          9.800223,
          11.173219,
          9.718976,
          10.241125,
          9.654693,
          12.423866,
          9.593757,
          10.921527,
          9.828367,
          9.994635,
          -2.5157974,
          12.583332,
          10.097241,
          12.513271,
          10.337933,
          9.697414,
          11.551312,
          9.422224,
          10.066885,
          11.123847,
          11.868304,
          11.378221,
          10.374976,
          10.851866,
          9.700091,
          9.946508,
          9.731539,
          8.7405615,
          12.445545,
          8.690085,
          11.603016,
          8.551123,
          9.724075,
          12.697399,
          9.863047,
          12.566675,
          9.669548,
          10.607836,
          9.679455,
          8.725993,
          10.801033,
          12.418048,
          12.337239,
          10.06007,
          10.490417,
          8.819282,
          12.3817425,
          10.421014,
          8.809311,
          11.238326,
          8.404288,
          9.7364435,
          10.83741,
          9.105713,
          8.5111685,
          9.555266,
          10.011656,
          11.573754,
          8.807974,
          8.894805,
          11.790231,
          8.441028,
          12.004434,
          7.330279,
          11.822685,
          12.366872,
          10.542514,
          11.634298,
          9.631145,
          9.496539,
          9.572807,
          8.536809,
          10.32749,
          12.200634,
          9.713158,
          9.521029,
          12.020345,
          9.811754,
          11.822147,
          12.461233,
          10.512418,
          11.554931,
          12.651429,
          9.44977,
          9.731125,
          11.911994,
          8.903211,
          11.734308,
          9.489183,
          9.602552,
          11.171191,
          9.641614,
          12.517627,
          9.872352,
          10.495942,
          12.593652,
          8.920707,
          8.597936,
          11.201576,
          11.940777,
          9.690756,
          8.357936,
          11.183126,
          9.675819,
          12.269674,
          8.921039,
          11.511774,
          12.610909,
          11.688418,
          9.641046,
          12.020366,
          8.714286,
          10.305671,
          12.349709,
          10.8872595,
          9.456396,
          8.786659,
          10.46465,
          11.365016,
          9.729256,
          9.995266,
          9.636697,
          9.927789,
          11.443369,
          12.338174,
          11.044418,
          11.935249,
          8.342067,
          10.090785,
          10.990205,
          12.725809,
          9.495427,
          11.631767,
          11.042414,
          10.822533,
          9.480975,
          10.88689,
          9.745502,
          9.03303,
          13.230339,
          11.527782,
          10.604124,
          12.579321,
          12.517178,
          13.149801,
          9.792838,
          11.913633,
          9.326177,
          9.808183,
          11.580033,
          11.532475,
          11.4036875,
          12.079481,
          10.1457615,
          12.56175,
          10.017414,
          9.928715,
          11.650298,
          8.40668,
          8.873001,
          11.63665,
          12.255007,
          8.859183,
          12.589363,
          9.248873,
          10.158745,
          9.849391,
          12.385522,
          10.162347,
          9.738053,
          11.335523,
          9.056868,
          8.886076,
          9.681782,
          10.552034,
          9.56315,
          12.550327,
          12.509792,
          9.111655,
          9.686536,
          9.852517,
          11.323027,
          10.197621,
          8.990612,
          8.404414,
          12.382553,
          12.483025,
          12.014877,
          10.60116,
          9.69996,
          9.411061,
          11.050987,
          10.908483,
          11.117481,
          9.061687,
          9.097182,
          9.417508,
          8.482739,
          9.6321535,
          9.832768,
          10.016044,
          11.05144,
          13.198101,
          10.306378,
          10.920571,
          11.651,
          12.663597,
          11.963513,
          10.267946,
          9.632223,
          11.483129,
          11.46672,
          12.629626,
          9.699118,
          9.262125,
          10.204467,
          10.896692,
          9.555058,
          10.755048,
          9.886787,
          11.187696,
          9.193026,
          9.705302,
          9.729812,
          11.115586,
          9.715353,
          9.493419,
          8.882227,
          12.605871,
          12.644402,
          11.060264,
          8.698155,
          9.900158,
          9.810261,
          10.888851,
          11.818396,
          12.690219,
          9.685143,
          10.554162,
          7.339557,
          8.781495,
          10.966574,
          12.628352,
          8.892281,
          8.872952,
          10.42714,
          9.646745,
          12.549585,
          12.16515,
          9.748546,
          11.21984,
          9.544617,
          11.435158,
          12.550738,
          9.8277,
          9.672107,
          12.639499,
          8.60245,
          10.60165,
          12.179479,
          8.342354,
          11.029229,
          9.69809,
          8.81016,
          11.286309,
          11.777842,
          12.549371,
          8.409929,
          12.340428,
          12.166239,
          9.562025,
          10.541522,
          12.231582,
          8.337123,
          10.243832,
          10.316271,
          9.754547,
          9.365959,
          11.735329,
          12.581899,
          10.366994,
          8.594576,
          -4.1271563,
          9.334357,
          9.120896,
          11.333738,
          9.790381,
          8.746347,
          9.764854,
          9.827506,
          11.143387,
          10.567613,
          12.283661,
          8.3166485,
          11.773213,
          9.74192,
          12.213676,
          12.20813,
          9.679566,
          10.861885,
          11.589964,
          11.321096,
          11.562243,
          11.462612,
          8.904942,
          11.405072,
          8.823824,
          12.586005,
          11.77937,
          11.454112,
          8.851317,
          12.436345,
          11.326265,
          9.88615,
          9.662546,
          11.56203,
          9.653656,
          11.316468,
          9.367652,
          10.314657,
          11.196573,
          9.790577,
          8.361253,
          10.578239,
          10.013381,
          9.75242,
          12.44394,
          10.666247,
          11.213975,
          9.610043,
          12.706018,
          11.270196,
          10.195907,
          9.630135,
          12.578959,
          9.445203,
          10.9514885,
          9.544724,
          10.639922,
          9.574605,
          9.893145,
          10.649062,
          11.002584,
          10.82778,
          9.425792,
          9.888187,
          9.461346,
          8.616568,
          9.472414,
          12.21297,
          11.353279,
          12.653455,
          8.707595,
          12.24903,
          8.703381,
          10.968306,
          12.638453,
          7.3225613,
          9.432111,
          12.355397,
          11.275695,
          9.6090145,
          10.902552,
          9.704591,
          9.535183,
          9.809523,
          9.891502,
          12.229651,
          12.655541,
          9.249021,
          11.423009,
          11.62523,
          10.695369,
          10.053779,
          10.065279,
          11.641225,
          9.640688,
          9.356091,
          8.698618,
          10.469066,
          9.711876,
          9.463376,
          9.644092,
          9.462475,
          10.583717,
          13.226088,
          8.6557455,
          10.0884075,
          11.485598,
          9.793558,
          10.525295,
          11.27782,
          11.65625,
          12.613426,
          8.586162,
          9.571754,
          8.8046255,
          10.309156,
          10.078216,
          9.462521,
          9.427304,
          12.551581,
          11.773685,
          12.546025,
          9.15773,
          9.528886,
          10.017067,
          11.701146,
          10.316229,
          11.925958,
          10.825023,
          12.8334255,
          12.11526,
          9.188293,
          12.302395,
          9.635688,
          11.292468,
          9.66917,
          9.047851,
          12.36979,
          12.406905,
          10.802133,
          11.154042,
          9.742421,
          10.89817,
          8.586788,
          9.868141,
          12.619745,
          9.736232,
          10.631478,
          11.643726,
          10.863099,
          11.064101,
          8.582397,
          10.042648,
          9.6105585,
          10.07639,
          8.57735,
          10.648601,
          10.244759,
          11.863196,
          11.256187,
          11.171177,
          8.441794,
          11.020486,
          12.0084305,
          12.4324,
          12.234765,
          11.32641,
          11.071283,
          8.3093195,
          9.583841,
          9.794449,
          12.013033,
          8.854529,
          9.457831,
          9.928024,
          10.999536,
          12.54632,
          10.198223,
          10.648273,
          10.8567,
          11.567789,
          10.820138,
          11.617586,
          9.494962,
          12.5344925,
          12.624791,
          9.475651,
          9.135806,
          9.404983,
          12.193863,
          11.730827,
          9.856768,
          11.465685,
          11.478601,
          9.35426,
          11.232186,
          10.18053,
          9.587327,
          12.665882,
          11.724187,
          10.183266,
          10.21539,
          8.785691,
          9.989639,
          12.620364,
          10.689774,
          10.145037,
          11.12834,
          13.208141,
          9.164773,
          9.714349,
          12.0106945,
          8.825206,
          11.5611105,
          9.439907,
          13.019282,
          8.772695,
          11.704325,
          8.407055,
          10.420546,
          9.53279,
          11.11346,
          11.52182,
          9.658547,
          9.374635,
          9.735807,
          11.276842,
          9.310962,
          10.17392,
          9.937276,
          9.677627,
          10.94594,
          10.736837,
          12.599823,
          11.544026,
          8.980317,
          9.836704,
          9.713696,
          12.314048,
          9.227415,
          10.612823,
          9.570947,
          10.6711855,
          11.448643,
          9.322512,
          10.532073,
          11.414982,
          9.720326,
          11.904881,
          9.690711,
          11.696005,
          9.560999,
          12.483718,
          8.671908,
          8.778636,
          10.872536,
          8.440441,
          8.7906685,
          12.369392,
          9.851591,
          11.906624,
          10.539718,
          10.472,
          10.456074,
          9.341838,
          12.487066,
          12.735611,
          9.738638,
          11.570753,
          12.620041,
          12.70558,
          8.859716,
          11.267213,
          10.299598,
          9.802544,
          12.577744,
          10.524857,
          12.552597,
          11.04809,
          11.675251,
          9.413491,
          10.507807,
          9.570891,
          10.033926,
          12.329143,
          9.24171,
          11.424995,
          10.366324,
          8.604091,
          11.3511095,
          9.514219,
          9.745359,
          10.628145,
          11.594145,
          11.018046,
          10.194104,
          11.571978,
          10.091616,
          10.791032,
          8.360254,
          12.552612,
          9.139725,
          12.616854,
          9.580676,
          10.673939,
          9.783007,
          12.534656,
          9.456701,
          10.727883,
          9.617877,
          11.660155,
          11.361048,
          8.748131,
          12.681841,
          11.578616,
          10.020125,
          12.646796,
          9.737776,
          10.012029,
          12.589057,
          9.218868,
          10.065574,
          11.623716,
          11.490578,
          8.825355,
          10.624568,
          10.260048,
          9.626395,
          8.602388,
          9.835636,
          8.373354,
          9.31172,
          12.019965,
          12.394322,
          8.904009,
          11.560618,
          9.714322,
          9.597749,
          10.679038,
          8.692872,
          10.981461,
          9.443546,
          12.1623745,
          11.440008,
          9.018753,
          10.980949,
          10.806662,
          9.664831,
          11.683664,
          11.700374,
          11.71915,
          9.532426,
          10.36497,
          8.879545,
          10.1332855,
          11.230991,
          11.924522,
          12.307625,
          9.935554,
          11.129208,
          8.848253,
          9.915061,
          10.008135,
          12.695096,
          10.813338,
          12.658735,
          12.528286,
          9.886799,
          8.910892,
          10.218699,
          9.869023,
          10.814978,
          11.862564,
          9.57461,
          10.402138,
          12.2959175,
          9.992428,
          8.750475,
          12.184827,
          11.082478,
          11.779184,
          12.654517,
          11.246702,
          10.249315,
          12.608197,
          12.456038,
          10.576485,
          9.438554,
          12.554587,
          9.645674,
          8.707652,
          10.067439,
          11.8799095,
          9.770102,
          10.204338,
          12.032227,
          13.209645,
          9.0421095,
          12.168849,
          10.734196,
          10.501201,
          9.991072,
          9.963108,
          8.76966,
          10.076856,
          12.430699,
          9.829982,
          9.6658125,
          9.895287,
          8.516064,
          9.961023,
          9.823311,
          9.9726,
          9.640277,
          11.463377,
          10.437648,
          9.741984,
          11.356534,
          9.362675,
          12.620665,
          8.702565,
          10.819038,
          12.3523245,
          9.994608,
          9.407546,
          10.382807,
          8.417149,
          12.52087,
          11.77921,
          9.498095,
          9.515274,
          8.576187,
          12.053669,
          11.826385,
          11.32397,
          11.864534,
          12.622843,
          9.607962,
          12.293678,
          9.765524,
          9.79865,
          11.919916,
          9.891884,
          8.825301,
          8.781287,
          10.037982,
          9.852238,
          12.6853,
          9.668116,
          12.017534,
          9.719557,
          9.454256,
          10.4973955,
          8.513109,
          9.4341345,
          9.8172455,
          12.523524,
          9.419821,
          9.90131,
          11.619283,
          8.966636,
          9.504717,
          11.395886,
          9.382107,
          8.4273615,
          11.739544,
          9.094825,
          8.958591,
          11.883331,
          9.641087,
          9.866294,
          11.7669,
          9.431921,
          9.6025095,
          9.316937,
          8.979705,
          12.459291,
          10.029397,
          11.89031,
          9.573305,
          11.759638,
          9.356553,
          10.478313,
          10.137822,
          10.546406,
          9.52427,
          9.553232,
          12.696017,
          10.528886,
          9.283734,
          12.352286,
          9.512371,
          11.922989,
          10.2615595
         ],
         "xaxis": "x",
         "y": [
          6.604148,
          3.7584264,
          3.7379966,
          3.561493,
          5.415899,
          3.4310014,
          -8.788725,
          4.9201646,
          3.5335398,
          3.1873255,
          2.9256525,
          6.0087934,
          5.020589,
          6.134492,
          5.6126256,
          4.823803,
          3.8908916,
          3.573445,
          3.54191,
          4.877772,
          4.4883876,
          2.3950183,
          5.8906593,
          5.519247,
          3.8595066,
          4.977317,
          0.9421182,
          5.1697946,
          3.5593247,
          6.133783,
          3.2285888,
          6.276565,
          0.73999953,
          2.1901417,
          5.5838118,
          6.3033605,
          4.7545056,
          4.3865094,
          4.2924795,
          2.2060995,
          4.758752,
          6.2185526,
          6.2069798,
          1.8968306,
          5.635702,
          6.37202,
          4.6281652,
          3.1842947,
          3.7335374,
          5.4500613,
          3.9247408,
          3.7554245,
          2.9341853,
          5.619495,
          4.6430798,
          6.429401,
          3.0611362,
          4.16793,
          2.9947598,
          5.2864947,
          1.1607786,
          3.811701,
          2.3941123,
          3.5817838,
          2.281284,
          5.796883,
          4.554188,
          6.160136,
          6.05363,
          5.6183505,
          5.1474357,
          4.517663,
          6.4573736,
          5.047465,
          2.2434208,
          4.760891,
          4.246091,
          3.796867,
          5.4897985,
          4.4862766,
          3.4318917,
          3.0135267,
          5.2641506,
          0.27812368,
          5.873924,
          5.7969913,
          3.2620523,
          3.1534352,
          6.086455,
          4.040778,
          3.1809778,
          2.1280468,
          2.585319,
          5.2592053,
          3.528213,
          1.9452734,
          4.4113393,
          3.8383315,
          2.9757326,
          5.485673,
          5.9318542,
          0.57707244,
          0.18998486,
          3.2033179,
          3.5140765,
          3.3153596,
          5.3053465,
          5.9412804,
          4.760695,
          5.2097044,
          3.1183846,
          6.6350718,
          2.047688,
          5.219728,
          5.0948195,
          0.56445885,
          2.9084873,
          2.647033,
          0.6569267,
          6.281078,
          6.4539146,
          3.1168883,
          5.8487973,
          5.122657,
          2.4048407,
          6.041587,
          5.2474084,
          3.0873492,
          4.4588,
          5.2159314,
          0.3029459,
          6.2126813,
          3.7468162,
          6.03697,
          4.556199,
          2.821859,
          4.2338257,
          5.5342236,
          3.632803,
          6.170242,
          6.196747,
          3.9634624,
          2.0639732,
          0.5656971,
          4.5952225,
          6.148228,
          5.5284605,
          4.465677,
          4.5334945,
          6.3813224,
          3.2683132,
          4.9545836,
          3.2782161,
          4.306421,
          2.3229635,
          2.29801,
          4.0799875,
          3.8133314,
          4.886725,
          4.360272,
          2.2809582,
          0.2897031,
          6.594021,
          4.6653514,
          5.094324,
          5.957203,
          4.622855,
          4.4804387,
          4.5941505,
          3.5442598,
          5.2081585,
          1.5787234,
          5.688315,
          3.4942517,
          4.384784,
          3.5945206,
          6.515419,
          3.7899652,
          2.961128,
          4.596109,
          0.38077828,
          5.187216,
          3.760024,
          1.4042387,
          3.2091327,
          3.9425693,
          0.59322816,
          5.8271155,
          4.5188737,
          2.5141113,
          3.5709863,
          3.224435,
          5.425323,
          3.067814,
          4.5407104,
          5.6180058,
          6.3783774,
          6.3013697,
          0.8519677,
          3.8118832,
          0.28866518,
          3.2690694,
          0.7135855,
          5.590847,
          6.1476135,
          1.4094417,
          5.050795,
          4.4276686,
          5.4994774,
          5.514032,
          3.8440957,
          4.633367,
          4.7843995,
          2.1337214,
          6.1492553,
          3.2576513,
          2.1761916,
          0.71565866,
          2.4316957,
          3.5853922,
          3.660832,
          3.769031,
          5.790232,
          3.8033836,
          6.2280703,
          6.5424676,
          5.6035976,
          6.0358195,
          1.3205045,
          3.8908005,
          6.0170684,
          5.6730843,
          5.443033,
          3.856427,
          5.670126,
          4.3343754,
          4.3428636,
          5.1178823,
          6.169802,
          5.8473353,
          2.2948565,
          5.9201016,
          3.1035168,
          4.4329863,
          3.2304156,
          2.9396405,
          2.1661267,
          2.1516097,
          5.6360106,
          4.3417387,
          2.2821577,
          6.63337,
          4.509016,
          5.7159557,
          5.9289145,
          5.9034586,
          3.6917949,
          6.646602,
          0.32525614,
          3.3069806,
          6.5931544,
          5.914056,
          3.6994832,
          4.7588024,
          5.2601676,
          6.4551387,
          1.2979345,
          5.1430025,
          5.3053966,
          4.797596,
          4.369285,
          6.1817384,
          5.573326,
          3.2141087,
          0.967722,
          0.7204985,
          3.7638965,
          3.3229792,
          6.2201777,
          5.081516,
          4.528146,
          6.2057667,
          3.2004004,
          3.9631064,
          5.2707467,
          2.1368272,
          2.5333703,
          2.8787167,
          5.6122413,
          6.13526,
          2.4947076,
          4.8469334,
          5.9143524,
          5.449043,
          3.2454894,
          4.587794,
          5.884636,
          5.668067,
          5.4352393,
          3.869542,
          4.58792,
          3.5558212,
          6.5819116,
          2.2683644,
          5.474406,
          5.161968,
          4.7303243,
          5.092221,
          5.6258116,
          3.746745,
          4.074782,
          3.8721824,
          6.4137583,
          4.2646375,
          2.8572092,
          0.5764957,
          2.0382636,
          2.9564357,
          5.628111,
          4.8252306,
          6.4172425,
          2.1498253,
          5.614516,
          4.608249,
          2.5014572,
          5.0475693,
          6.4595594,
          2.5211012,
          6.2186813,
          6.201145,
          4.9748616,
          1.2966278,
          5.673503,
          4.5301137,
          0.8921979,
          4.34577,
          5.664344,
          4.3323126,
          5.587461,
          0.91394633,
          4.2989936,
          2.8660197,
          5.5674496,
          3.3372622,
          3.7897954,
          4.371957,
          4.7557306,
          4.8887596,
          1.8952882,
          4.5649185,
          6.3359323,
          3.0481834,
          2.390356,
          2.8805141,
          5.6429896,
          5.931113,
          6.106798,
          4.8204656,
          5.6927676,
          3.248136,
          1.3789903,
          2.5526943,
          5.833437,
          6.302609,
          6.420648,
          3.3615196,
          3.5221918,
          3.218669,
          3.7172315,
          5.606695,
          5.897626,
          4.2414303,
          4.572597,
          3.8506513,
          4.4699097,
          0.5739138,
          3.9645658,
          5.7932525,
          5.831921,
          2.2567666,
          2.9261172,
          3.274857,
          4.0644584,
          0.4425373,
          3.8424509,
          3.307427,
          6.131739,
          5.5080404,
          3.1794283,
          3.260968,
          5.6109695,
          6.3243437,
          3.1688337,
          3.0206654,
          6.151981,
          3.5477333,
          6.279627,
          6.089873,
          4.315778,
          5.982975,
          3.709099,
          3.5220945,
          3.0950005,
          3.441006,
          5.299135,
          3.7182055,
          5.8504496,
          3.229094,
          0.75982034,
          6.646772,
          3.249394,
          5.7526813,
          4.8007374,
          4.947099,
          2.745548,
          6.096889,
          5.6171494,
          3.5310168,
          5.068702,
          5.24986,
          3.1049075,
          5.092557,
          3.492365,
          0.79396516,
          6.4281874,
          5.724568,
          4.073411,
          3.6309788,
          5.2009673,
          0.67973226,
          5.476004,
          6.1678247,
          0.89493203,
          2.4284825,
          4.433518,
          4.5365186,
          1.8748262,
          2.327064,
          6.18816,
          2.3301802,
          1.2978435,
          5.6623516,
          3.6100962,
          3.5442274,
          5.500236,
          5.9115233,
          5.126092,
          4.6227,
          4.51005,
          0.5395143,
          1.4663365,
          2.8824754,
          5.836973,
          3.2674468,
          5.4535375,
          2.546942,
          5.718965,
          3.5073342,
          5.691149,
          3.2616248,
          4.8249187,
          4.747171,
          0.599317,
          1.410562,
          5.6445284,
          3.1656983,
          6.467919,
          6.09972,
          6.4353323,
          2.9542298,
          3.7358754,
          1.2802671,
          2.6936486,
          -6.4248652,
          5.612578,
          5.612287,
          6.426236,
          3.8326046,
          3.6535537,
          5.5391674,
          5.6233377,
          2.7409997,
          3.2676766,
          4.214315,
          6.4938974,
          2.5059085,
          5.4067187,
          4.652653,
          5.3075995,
          3.3050582,
          3.086193,
          3.278502,
          0.6498405,
          2.5784407,
          0.7172527,
          2.9917283,
          6.2380233,
          0.9515443,
          4.52648,
          3.0795388,
          3.8050897,
          3.0911634,
          5.2083473,
          3.4355674,
          6.4560966,
          3.384163,
          3.928683,
          3.181237,
          4.650889,
          6.4604135,
          5.6972857,
          3.8856263,
          6.1458883,
          1.3885833,
          2.9453053,
          3.5241365,
          3.8463683,
          0.794721,
          3.0479484,
          0.55798846,
          2.3511946,
          0.5578274,
          3.723876,
          3.8844132,
          1.4302791,
          5.662487,
          5.093434,
          3.7428584,
          4.671376,
          3.3324518,
          2.3061438,
          3.9755564,
          6.4150367,
          4.871915,
          0.7696691,
          3.331455,
          4.2103868,
          0.99908054,
          1.0123445,
          6.024977,
          4.3375216,
          4.9058003,
          2.281134,
          2.9843729,
          3.3841925,
          2.1033394,
          4.0876827,
          5.9821057,
          3.959818,
          5.4515004,
          3.6938286,
          4.333485,
          4.628186,
          5.4086833,
          3.0134819,
          6.446114,
          5.9421997,
          3.095019,
          6.1117377,
          4.832521,
          0.80727774,
          3.50838,
          4.24019,
          3.247303,
          4.3972692,
          5.798934,
          4.5276036,
          4.906316,
          4.825094,
          5.6000323,
          6.2373176,
          3.876471,
          3.2635117,
          3.782769,
          5.6559873,
          3.8684747,
          3.4348001,
          2.3868551,
          1.0459634,
          4.097273,
          5.39399,
          3.7901862,
          2.9670253,
          3.681482,
          4.660302,
          1.1085306,
          5.6828337,
          5.2012596,
          1.8861535,
          4.239048,
          4.4238944,
          6.2586856,
          5.0228677,
          6.0823474,
          4.858594,
          2.5636144,
          1.8800176,
          2.3381937,
          6.3784723,
          3.453059,
          5.969264,
          3.741525,
          6.162283,
          6.549039,
          2.979989,
          3.2099516,
          2.178129,
          6.1194186,
          0.82454467,
          6.390649,
          5.1893463,
          0.8229044,
          2.644816,
          3.507361,
          3.2089422,
          4.929989,
          4.786346,
          3.15423,
          3.2933788,
          0.7645578,
          2.621409,
          1.3908571,
          4.867844,
          2.2949514,
          3.5217721,
          3.7206435,
          2.361815,
          5.1099668,
          5.636533,
          5.671661,
          6.467506,
          1.406383,
          4.6695395,
          2.961201,
          5.5194197,
          3.6805956,
          5.7643113,
          3.3741248,
          3.2161946,
          3.1920025,
          3.2681203,
          3.7082057,
          5.9579144,
          4.9898725,
          4.086826,
          4.2521777,
          3.9224231,
          1.3974187,
          6.3741045,
          3.2915485,
          3.413072,
          5.6885085,
          3.6735299,
          3.7383041,
          4.0805826,
          3.5740082,
          5.599263,
          3.6642365,
          3.8818884,
          0.23736963,
          0.75614053,
          5.6430016,
          5.2471595,
          0.7362846,
          3.0133767,
          6.1428866,
          1.8270944,
          5.4632983,
          2.5469391,
          3.2522283,
          4.5789437,
          4.5784774,
          5.8713055,
          3.452411,
          4.083147,
          2.1375163,
          5.9246926,
          4.5795946,
          3.0900705,
          4.635383,
          5.890742,
          3.4572709,
          4.3868084,
          4.311745,
          3.4013624,
          2.8232799,
          5.726486,
          3.3187997,
          3.683252,
          0.9376696,
          3.8013978,
          2.239069,
          2.1139088,
          5.7967415,
          2.8792422,
          6.02791,
          5.8444343,
          5.4915285,
          6.3414145,
          3.25164,
          2.7501292,
          3.006528,
          5.093427,
          0.5603907,
          5.737069,
          2.1704032,
          0.5755944,
          4.8154917,
          5.0213647,
          0.5772379,
          3.0308723,
          4.8589487,
          5.428106,
          6.2564497,
          4.5574603,
          3.7828481,
          5.37461,
          0.9495527,
          5.8326087,
          2.1259952,
          0.6762862,
          3.5593798,
          4.6522675,
          4.404561,
          4.077976,
          5.837124,
          4.973191,
          6.3729434,
          4.5322347,
          2.1611385,
          1.4221134,
          5.981557,
          5.882208,
          4.723898,
          4.852406,
          5.4274187,
          4.4631147,
          4.3980236,
          4.6776624,
          5.8274646,
          6.6096387,
          3.3918848,
          4.507929,
          1.4755476,
          0.7586153,
          -6.4798474,
          6.614194,
          5.379183,
          5.905374,
          1.0832955,
          2.8519127,
          0.7898636,
          6.1033363,
          2.9393144,
          2.9886346,
          3.4908524,
          4.428035,
          5.5036654,
          5.7331038,
          5.527042,
          5.4365387,
          5.763666,
          4.8633356,
          2.2854664,
          2.59193,
          3.3667464,
          2.5579283,
          3.5682309,
          3.8294218,
          3.8273354,
          4.5061836,
          3.348911,
          3.3890564,
          3.7797656,
          5.2850437,
          3.1879025,
          6.007589,
          0.9525614,
          2.3985367,
          5.2904725,
          6.4047837,
          3.5915384,
          2.7633655,
          3.6081665,
          4.128355,
          4.415246,
          4.777494,
          5.606279,
          5.311608,
          6.4596114,
          4.009887,
          5.886098,
          2.9875453,
          3.1957474,
          6.3916216,
          3.7530859,
          3.2832677,
          4.5111732,
          0.24995135,
          4.6865554,
          0.9222992,
          3.7739573,
          6.239126,
          0.99876034,
          4.3412843,
          3.6536927,
          2.3748865,
          0.20184718,
          5.454969,
          5.0042067,
          5.570359,
          0.26708478,
          4.8399715,
          3.2716272,
          2.1189137,
          2.879753,
          3.3325746,
          2.8841407,
          4.9492655,
          2.126837,
          5.0964913,
          0.1985112,
          6.5176053,
          6.274579,
          3.1021729,
          6.047479,
          3.543363,
          3.1380708,
          1.061097,
          3.5966892,
          4.560767,
          3.1911347,
          3.507611,
          5.6415706,
          6.5752945,
          4.768375,
          5.2903957,
          3.9350023,
          3.6177926,
          3.155677,
          6.5661674,
          2.8843908,
          4.7578216,
          0.9877411,
          5.7734566,
          3.0109344,
          6.3979754,
          5.623189,
          6.1735573,
          0.6174892,
          3.6266932,
          6.559209,
          0.6774047,
          3.6201978,
          3.0693653,
          2.3418245,
          6.1811204,
          5.5984273,
          2.7512844,
          4.448102,
          4.298867,
          0.4730681,
          6.0234327,
          2.5653856,
          3.4206862,
          4.0371685,
          3.2140584,
          4.0480433,
          4.457652,
          5.2086563,
          2.5685902,
          3.857397,
          5.585172,
          5.0408664,
          6.0774517,
          5.43,
          5.367643,
          5.4080696,
          2.8967507,
          3.4735932,
          4.552093,
          3.3794596,
          6.0162463,
          3.3753998,
          4.0573864,
          5.6364126,
          3.0767012,
          5.444575,
          0.805229,
          5.677493,
          3.182373,
          0.753598,
          2.2214034,
          2.8435614,
          2.1344955,
          1.8802218,
          0.7113886,
          4.1015034,
          2.9971778,
          3.3712206,
          0.71990556,
          4.688889,
          6.438521,
          4.834324,
          5.2891107,
          3.3016188,
          1.4570056,
          3.057821,
          3.2590628,
          5.1983747,
          5.267379,
          3.9152381,
          3.2025063,
          4.41733,
          5.5749464,
          3.8713875,
          3.4970791,
          0.57019466,
          5.9586906,
          3.5949826,
          5.2181807,
          4.4933486,
          2.992993,
          3.338251,
          3.3875198,
          5.166154,
          5.444867,
          2.5551,
          0.29951164,
          5.389702,
          3.2049367,
          0.23399693,
          5.5239816,
          6.3702283,
          5.7927012,
          6.593457,
          2.7866855,
          3.3846312,
          2.3794897,
          6.559527,
          3.2048616,
          4.76656,
          4.762953,
          2.1016724,
          6.5835648,
          5.682191,
          5.085424,
          4.1789947,
          0.61410934,
          3.2564273,
          4.713986,
          3.7867992,
          5.82581,
          6.1566005,
          3.2868514,
          4.6736097,
          5.6464124,
          3.6958308,
          4.318688,
          0.95840394,
          6.075792,
          4.583788,
          4.488346,
          1.4102743,
          6.0826173,
          5.778154,
          5.7936163,
          2.3488708,
          0.9708869,
          3.6184268,
          5.006812,
          6.41604,
          6.6816397,
          3.1231332,
          3.6547992,
          5.5274715,
          5.3446,
          4.4987845,
          3.2298434,
          2.3372145,
          4.7509265,
          4.580332,
          2.917828,
          4.6006584,
          3.5884907,
          3.1197371,
          4.3518124,
          2.2612705,
          5.508855,
          3.8262699,
          3.0828989,
          5.6985035,
          3.1543963,
          3.7712889,
          4.3904095,
          3.1861625,
          3.1689665,
          6.2727404,
          5.612139,
          0.61344814,
          5.3907833,
          1.4859922,
          0.5361406,
          6.4744606,
          0.62582296,
          4.1312003,
          2.9588363,
          3.0582247,
          5.699334,
          3.7358723,
          5.511168,
          6.1714854,
          3.1792226,
          2.350222,
          6.2034206,
          3.2568824,
          3.7126749,
          6.2980866,
          2.7633169,
          4.4219384,
          2.2710917,
          5.353339,
          6.382513,
          5.0342026,
          2.5654726,
          6.4783125,
          2.9928896,
          4.5382757,
          3.752119,
          4.6334476,
          5.3974853,
          6.449173,
          2.4271631,
          5.6499686,
          6.3280697,
          2.9393277,
          5.6804886,
          4.2804885,
          6.5624785,
          4.7828608,
          4.830035,
          2.8887658,
          4.3407445,
          2.1744525,
          4.450121,
          4.539104,
          3.3155172,
          2.125496,
          5.979862,
          3.0854678,
          5.886137,
          6.3128896,
          5.765786,
          4.7361064,
          4.4431925,
          5.957486,
          3.305125,
          4.974565,
          2.0760558,
          5.5350404,
          5.2474136,
          6.3514705,
          0.7459893,
          2.7462966,
          5.8709064,
          5.75911,
          5.9520917,
          2.7898865,
          3.3560283,
          4.206782,
          3.1001375,
          4.9113507,
          5.8020153,
          5.6441073,
          1.216799,
          1.360654,
          4.521839,
          5.6166024,
          6.473286,
          3.5937548,
          2.6985323,
          1.225773,
          4.4447403,
          2.1567645,
          0.59522194,
          5.803297,
          6.6032147,
          5.2371206,
          5.0192165,
          3.772301,
          5.081695,
          2.225534,
          3.149051,
          5.3788257,
          2.7322795,
          5.41949,
          6.024886,
          4.6767354,
          3.6607304,
          5.439157,
          4.69922,
          4.0811815,
          3.211021,
          5.4840016,
          5.191817,
          0.50909156,
          5.8152018,
          5.270708,
          6.143155,
          2.3003824,
          2.1271222,
          4.5183053,
          5.4993668,
          4.2904043,
          3.9954405,
          4.216397,
          2.8697915,
          3.837389,
          5.728325,
          4.3121023,
          3.5291255,
          0.5384637,
          4.98284,
          5.3502574,
          6.3324,
          6.6098986,
          2.1254714,
          6.293412,
          4.6876445,
          5.6985455,
          4.650152,
          2.3410044,
          3.2417564,
          6.422785,
          6.086643,
          5.677309,
          0.4734649,
          3.784652,
          4.8230214,
          5.143947,
          4.9423747,
          6.147633,
          3.3689892,
          4.8532553,
          4.518334,
          3.3502543,
          3.4296281,
          4.7357492,
          5.649017,
          3.776565,
          5.426312,
          6.1195636,
          0.72811615,
          3.0034802,
          0.7286468,
          4.8158216,
          3.1198318,
          2.8193848,
          3.1065364,
          3.4286416,
          3.0426447,
          5.96508,
          3.7120092,
          1.0854622,
          6.2770157,
          2.8822317,
          2.161454,
          6.4768004,
          2.7906806,
          3.7073767,
          4.456703,
          1.3708851,
          5.3468065,
          3.6746714,
          0.9131581,
          0.9928846,
          0.6750487,
          3.4685764,
          6.433328,
          4.1740627,
          4.2192254,
          6.4132504,
          5.998631,
          6.510157,
          3.9621017,
          6.017107,
          4.499083,
          0.7790445,
          4.961645,
          3.720339,
          0.50612795,
          4.0872617,
          6.29492,
          5.617026,
          4.0916467,
          6.0081882,
          4.200694,
          3.6808534,
          0.8138795,
          5.575452,
          1.4597005,
          4.797979,
          3.1571193,
          0.8839767,
          5.4360266,
          3.7055979,
          4.2221956,
          6.008835,
          6.6018605,
          1.3546036,
          2.8229227,
          3.6525939,
          3.9293697,
          3.8145845,
          6.029241,
          4.4083953,
          2.701502,
          5.0345216,
          0.9737638,
          4.9962773,
          0.5669358,
          6.256547,
          5.718006,
          5.6699686,
          4.868014,
          5.976898,
          0.39007008,
          2.9663224,
          3.9800427,
          6.2792754,
          0.56606275,
          5.734677,
          3.2247486,
          5.8332148,
          6.3882594,
          5.1879554,
          4.490272,
          6.631586,
          5.7899857
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "label=Cat<br>x=%{x}<br>y=%{y}<extra></extra>",
         "legendgroup": "Cat",
         "marker": {
          "color": "#EF553B",
          "opacity": 0.7,
          "symbol": "circle"
         },
         "mode": "markers",
         "name": "Cat",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          -5.053914,
          -4.1924887,
          -4.144672,
          -5.085069,
          -4.8372555,
          -2.2907465,
          -5.293836,
          -4.0055947,
          -4.766608,
          -2.013208,
          -3.9936278,
          -5.2097416,
          -5.4139104,
          -5.791187,
          -5.3124647,
          -6.150733,
          -3.2575233,
          -4.708004,
          -5.2208385,
          -7.02423,
          -4.5873933,
          -4.8509665,
          -4.549844,
          -3.7008333,
          -2.5538695,
          -5.174922,
          -6.556973,
          -5.287911,
          -4.803562,
          -3.6537488,
          -4.8209157,
          -4.8059535,
          -2.0852785,
          -5.081088,
          -5.5824113,
          -7.158357,
          -3.4613135,
          -4.642931,
          -3.8218138,
          -4.7486525,
          -2.1603484,
          -2.65854,
          -3.7579405,
          -5.155369,
          -7.1114063,
          -3.8328087,
          -5.1519933,
          -5.5787573,
          -5.2686305,
          -4.140438,
          -2.5361953,
          -3.8707337,
          -5.3253875,
          -4.660884,
          -4.5786905,
          -2.1705904,
          -4.235619,
          -4.5614758,
          -5.5228558,
          -5.008288,
          -5.7351975,
          -4.499389,
          -4.68788,
          -5.8428926,
          -3.7891395,
          -2.9150033,
          -7.167877,
          -4.793401,
          -4.877652,
          -2.7336307,
          -6.819147,
          -4.887045,
          -5.168584,
          -5.2176604,
          -5.1375713,
          -4.88876,
          -2.507762,
          -5.5817213,
          -2.6520674,
          -3.6770582,
          -4.5658474,
          -6.919939,
          -5.0574417,
          -5.2363987,
          -2.0389543,
          -4.10917,
          -4.81834,
          -3.8846529,
          -4.733402,
          -4.210498,
          -4.1077495,
          -5.686602,
          -2.7530189,
          -4.569791,
          -5.9997215,
          -6.083587,
          -2.1497815,
          -4.4944444,
          -5.1624856,
          -6.1011386,
          -4.856674,
          -4.0693784,
          -5.116525,
          -4.989719,
          -4.480902,
          -1.9864148,
          -4.952461,
          -6.7527723,
          -5.2554903,
          -4.7535834,
          -3.9728727,
          -5.5093136,
          -4.3724537,
          -4.720222,
          -3.9241416,
          -4.7724714,
          -6.562945,
          -3.7525065,
          -5.5412254,
          -4.58727,
          -4.9150357,
          -7.17789,
          -3.7690408,
          -5.648027,
          -4.6643276,
          -5.1495943,
          -3.4438634,
          -4.5552945,
          -3.4355166,
          -5.179761,
          -3.49542,
          -2.13776,
          -2.8699498,
          -1.9695377,
          -4.9924417,
          -5.559659,
          -4.965447,
          -2.9344153,
          -3.8435,
          -5.454635,
          -2.9079447,
          -3.8094294,
          -5.0720487,
          -3.3406658,
          -2.8315227,
          -3.0013595,
          -5.455775,
          -4.834991,
          -4.0499487,
          -4.669195,
          -6.4715786,
          -3.3414972,
          -6.0289226,
          -2.4159384,
          -4.830134,
          -7.1414986,
          -4.4530272,
          -5.352101,
          -6.586973,
          -5.133542,
          -3.768645,
          -5.4703116,
          -5.3309135,
          -5.358112,
          -3.7590048,
          -3.9581301,
          -4.1620474,
          -5.97199,
          -3.4521616,
          -3.4193826,
          -2.8250973,
          -5.7111278,
          -2.3148293,
          -3.7389028,
          -5.1090074,
          -3.076484,
          -3.514095,
          -4.494913,
          -5.2009025,
          -3.9941964,
          -6.179933,
          -4.7668533,
          -4.299589,
          -4.807967,
          -6.9178166,
          -3.3723915,
          -7.1245365,
          -4.095907,
          -2.5499136,
          -2.3024228,
          -5.4015517,
          -3.6294234,
          -2.0864289,
          -4.837197,
          -6.561995,
          -4.213743,
          -4.3844566,
          -2.388564,
          -7.2140956,
          -4.8672132,
          -7.197654,
          -3.6898782,
          -4.5360513,
          -3.3528485,
          -4.412478,
          -2.9579797,
          -6.0768495,
          -5.331716,
          -5.389103,
          -3.738179,
          -2.8594654,
          -4.8840733,
          -4.6916933,
          -4.899075,
          -3.4067218,
          -4.5914664,
          -5.191979,
          -5.0509768,
          -4.47093,
          -4.9701557,
          -5.2804275,
          -4.6114464,
          -2.8283079,
          -3.8505156,
          -5.1261773,
          -3.9372835,
          -5.9652634,
          -1.9455886,
          -4.6895585,
          -4.0659423,
          -3.438381,
          -4.2860894,
          -4.6479034,
          -5.992489,
          -7.03303,
          -3.3938625,
          -6.5641503,
          -2.3901708,
          -5.4854927,
          -4.109262,
          -4.0631747,
          -3.3709233,
          -5.041257,
          -3.5419223,
          -4.2514725,
          -4.1429853,
          -5.065138,
          -2.2763681,
          -4.58281,
          -3.2996335,
          -6.098203,
          -4.171518,
          -4.240367,
          -6.9164996,
          9.207386,
          -5.560549,
          -5.116316,
          -2.1360784,
          -4.4938827,
          -2.2246404,
          -5.77231,
          -4.2895412,
          -5.939042,
          -5.4485216,
          -3.7190866,
          -4.111212,
          -6.8787303,
          -2.05021,
          -5.031305,
          -7.1721725,
          -3.4519095,
          -4.8150086,
          -4.8830256,
          -4.952226,
          -4.5112243,
          -4.7147813,
          -5.116929,
          -5.1359396,
          -4.8451886,
          -5.281805,
          -2.1404831,
          -5.145079,
          -4.6616764,
          -6.576326,
          -4.1407123,
          -3.1185808,
          -2.5162678,
          -4.5920563,
          -4.734323,
          -3.0118272,
          -3.3608575,
          -1.9267548,
          -4.6265798,
          -4.935486,
          -5.1250095,
          -6.813049,
          -2.962687,
          -4.908879,
          -5.056822,
          -4.041423,
          -3.8166406,
          -5.2733717,
          -5.192428,
          -4.653741,
          -2.1094542,
          -5.5803394,
          -2.8210852,
          -3.0144475,
          -5.4485745,
          -4.7756505,
          -5.442919,
          -4.6598363,
          -3.8421354,
          -4.838211,
          -5.6058183,
          -3.3186147,
          -4.3648357,
          -5.2138214,
          -2.1805947,
          -3.6828692,
          -6.951763,
          -6.1618533,
          -5.7025366,
          -4.1200004,
          -2.0771527,
          -4.384564,
          -7.1933594,
          -3.8945243,
          -4.5179687,
          -5.9632273,
          -4.6721163,
          -3.4435153,
          -4.938084,
          -3.4648278,
          -2.16678,
          -5.852656,
          -3.4844494,
          -5.307058,
          -4.11045,
          -4.851524,
          -3.8230941,
          -4.7050996,
          -4.7653913,
          9.259333,
          -3.849123,
          -2.934916,
          -2.0672722,
          -5.4111233,
          -1.9919099,
          -5.5320926,
          -4.5523276,
          -2.9159043,
          -5.1494145,
          -3.704827,
          -5.0406547,
          -3.1625156,
          -5.9388366,
          -4.213865,
          -4.869543,
          -6.404316,
          -5.353313,
          -5.171242,
          -2.1511056,
          -3.4784307,
          -6.8598514,
          -4.5543575,
          -3.346866,
          -4.0816474,
          -4.294011,
          -3.7357137,
          -5.4281225,
          -4.921579,
          -4.7379065,
          -4.514414,
          -3.8653467,
          -4.9817343,
          -4.9789834,
          -7.1345806,
          -4.968684,
          -5.470714,
          -5.8404875,
          -4.858757,
          -5.0593014,
          -2.0480866,
          -3.2936482,
          -5.24852,
          -4.6392093,
          -6.8858895,
          -5.3201723,
          -5.2419796,
          -5.05403,
          -3.772141,
          -6.602943,
          -5.4371285,
          -5.4302354,
          -3.8463006,
          -2.9245245,
          -4.6588664,
          -7.0617404,
          -3.2959769,
          -2.2627914,
          -7.1531386,
          -4.300827,
          -4.1148033,
          -2.1132915,
          -5.481365,
          -3.6193168,
          -4.70904,
          -4.463232,
          -5.074549,
          -4.5755324,
          -2.8429406,
          -3.4594486,
          -4.9604473,
          -5.4061427,
          -5.065278,
          -5.483192,
          -4.7717133,
          -5.447867,
          -3.5589068,
          -4.0585036,
          -3.986796,
          -5.3222356,
          -1.9486798,
          -2.9072711,
          10.106429,
          -2.7718127,
          -4.8485537,
          -4.968813,
          -7.049262,
          -4.6926184,
          -6.4039083,
          -5.1665325,
          -4.490509,
          -4.376172,
          -5.471708,
          -4.662961,
          -4.358453,
          -4.50886,
          -4.9608274,
          -2.0745864,
          -5.37819,
          -2.052734,
          -1.9155064,
          -2.2107346,
          -5.285465,
          -5.2459626,
          -3.3265035,
          -4.6751456,
          -2.9411469,
          -5.2543564,
          -3.3025732,
          -5.1064563,
          -7.1189084,
          -5.526781,
          -5.266297,
          10.743787,
          -6.8962326,
          -3.8552284,
          -2.0618894,
          -3.809106,
          -4.7690105,
          -3.3109975,
          -3.08436,
          -2.175583,
          -3.8949046,
          -3.9811668,
          -4.967936,
          -5.4705834,
          -5.2154465,
          -2.2516472,
          -4.725231,
          -3.0324607,
          -2.105562,
          -7.2104645,
          -4.5951633,
          -4.979026,
          -5.2917156,
          -5.197921,
          -6.0054326,
          -2.8470533,
          -1.9274163,
          -3.382076,
          -3.4300444,
          -5.5983887,
          -7.0659366,
          -4.3724866,
          -7.156827,
          -5.175152,
          -3.6661994,
          -5.729163,
          -7.119421,
          -2.9213142,
          -4.2363925,
          -2.9148161,
          -5.1435266,
          -4.606672,
          -4.518677,
          -4.8452945,
          -3.904777,
          -3.7305236,
          -5.3119755,
          -4.4299526,
          -2.8499317,
          -4.2055225,
          -2.9821649,
          -3.6742973,
          -4.6565366,
          -2.2271445,
          -4.30291,
          -2.342464,
          -2.1597912,
          -4.7845325,
          -3.840859,
          -5.0023656,
          -1.9663023,
          -3.7220936,
          -5.018252,
          -3.069791,
          -3.9134872,
          -3.5327497,
          -5.2381783,
          -3.7598681,
          -3.0918486,
          -6.6551285,
          -5.605321,
          -2.7908733,
          -4.0082307,
          -1.9896837,
          -2.9995952,
          -2.4531605,
          -3.8013785,
          -4.2696667,
          -4.860376,
          -4.8124495,
          -2.3448107,
          -3.5669096,
          -3.8901212,
          -2.9817734,
          -4.282314,
          -4.198357,
          -4.9878616,
          -5.400451,
          -3.1171844,
          -2.253055,
          -5.3594303,
          -4.8174267,
          -5.1496487,
          -5.3374867,
          -6.7180314,
          -7.231693,
          -2.0818892,
          -3.2907262,
          -2.804,
          -2.646504,
          -3.1235113,
          -5.028167,
          -5.1243196,
          -5.091433,
          -4.8495126,
          -4.2600846,
          -4.8318033,
          -5.2285995,
          -3.4336414,
          -2.3873706,
          -5.3631625,
          -5.2174554,
          -1.8789064,
          -3.4983132,
          -5.772631,
          -3.749053,
          -5.0688167,
          -5.604239,
          -4.5369124,
          -4.3282204,
          -4.6885667,
          -3.491529,
          -3.7751215,
          -2.3520155,
          -5.1635733,
          -4.406145,
          -3.5390785,
          -4.4648967,
          -2.907683,
          -1.9809625,
          -3.0063148,
          -5.0273156,
          -5.0932684,
          -5.017824,
          -6.924754,
          -6.7311726,
          -4.9259005,
          -4.7706842,
          -4.804886,
          -3.690343,
          -7.123041,
          -5.224298,
          -4.555662,
          -7.137721,
          -3.4852667,
          -4.2471323,
          -2.8461392,
          -4.818497,
          -4.7330947,
          -5.027431,
          -5.0855722,
          -5.6396513,
          -3.301876,
          -6.592112,
          -5.3095226,
          -4.3329015,
          -5.2628837,
          -3.433593,
          -6.888017,
          -5.4746814,
          -3.909079,
          -4.0898557,
          -5.0232744,
          -3.2903953,
          -5.1596327,
          -5.5732284,
          -2.6691566,
          -3.919534,
          -7.2402964,
          -4.2386694,
          -5.250412,
          -3.044049,
          -4.5958905,
          -3.1611216,
          -5.307423,
          -2.3642123,
          -2.9553015,
          -3.564931,
          -2.914877,
          -5.042352,
          -2.9951355,
          -4.6778274,
          -5.789931,
          -2.9396667,
          -2.6000323,
          -1.9246336,
          -5.241275,
          -3.3326945,
          -6.6047378,
          -3.9498453,
          -5.4144697,
          -4.642809,
          -4.7345834,
          -3.5520377,
          -5.271882,
          -4.3414407,
          -1.9912186,
          -5.3752117,
          -4.7901483,
          -7.11157,
          -4.1193304,
          -4.81538,
          -4.1585546,
          -6.4037504,
          -4.759167,
          -5.0167484,
          -5.0258207,
          -7.0487714,
          -5.6428475,
          -2.9622242,
          -5.4707885,
          -3.2904048,
          -5.28873,
          -2.9631314,
          -3.9911592,
          -4.619969,
          -2.905251,
          -5.202717,
          -4.4619026,
          -5.003396,
          -4.911906,
          -5.500468,
          -2.8219876,
          -5.443494,
          -4.877965,
          -4.7459583,
          -2.3672068,
          -3.8375938,
          -4.814993,
          -5.0148983,
          -4.9672847,
          -6.707262,
          -1.8246243,
          -2.9578059,
          -4.4088726,
          -2.870592,
          -3.6666877,
          -5.094666,
          -1.8460886,
          -7.108424,
          -5.0409646,
          -2.9540594,
          -5.1461177,
          -5.7486653,
          -2.299557,
          -7.171321,
          -4.4820213,
          -2.9487529,
          -6.0286894,
          -3.7272909,
          -4.3959556,
          -5.0437202,
          -4.3413258,
          -4.994219,
          -5.6203756,
          -4.2948027,
          -4.717457,
          -2.0163064,
          -3.3553727,
          -3.0404222,
          -3.7877102,
          -4.613442,
          -4.688072,
          -4.8487196,
          -2.8990047,
          -4.8071294,
          -1.8741151,
          -2.1712458,
          -3.8304195,
          -4.2134776,
          -3.9491065,
          -4.073475,
          -4.4818435,
          -5.188024,
          -4.7234435,
          -5.437122,
          -5.41318,
          -3.4272492,
          -4.8902516,
          -5.7217436,
          -6.096211,
          -4.397371,
          -3.2560773,
          -5.6216087,
          -5.0416613,
          -2.2213948,
          -5.039775,
          -4.4869256,
          -4.986737,
          -6.6048417,
          -2.8769717,
          -6.8657885,
          -7.169944,
          -6.160949,
          -7.15162,
          -4.1292706,
          -5.271038,
          -5.4126153,
          -3.2873592,
          -4.1159463,
          -4.5793834,
          -2.0822108,
          -2.954137,
          -4.4065948,
          -6.0610013,
          -3.2322795,
          -4.811325,
          -4.6487417,
          -4.8175063,
          -2.9934764,
          -3.6380548,
          -2.1022646,
          -5.441799,
          -4.003236,
          -3.0281463,
          -4.2154145,
          -4.8968115,
          -2.0251029,
          -4.7557993,
          -4.7153544,
          -3.667,
          -3.261741,
          -4.460775,
          -4.0209093,
          -4.886779,
          -2.2106967,
          -5.1345863,
          -5.021194,
          -3.0755734,
          -2.6254919,
          -5.8945174,
          -6.0888467,
          -4.7691226,
          -5.1191983,
          -4.6673217,
          -7.112642,
          -4.6543417,
          -4.757805,
          -4.4159226,
          -4.743061,
          -5.550283,
          -3.3718102,
          -1.848728,
          -3.7998176,
          -2.2367282,
          -2.759317,
          -3.8491478,
          -3.2418094,
          -6.53384,
          -5.2989645,
          -7.1013975,
          -6.19704,
          -2.5110652,
          -6.9642196,
          -5.419202,
          -3.7849298,
          -3.3131773,
          -3.3148677,
          -2.9459732,
          -3.5846045,
          -4.4711447,
          -5.0027065,
          -3.8465686,
          -5.481936,
          -4.845418,
          -3.7416234,
          -5.097657,
          -2.153242,
          -5.97975,
          -4.0748453,
          -5.4049826,
          -5.2832513,
          -3.3830054,
          -5.5194755,
          -5.2987027,
          -6.7123485,
          -5.1651607,
          -4.9293795,
          -4.836056,
          -4.9654937,
          -4.0963826,
          -3.9059045,
          -4.070033,
          -5.347436,
          -7.086124,
          -5.13992,
          -2.3399982,
          -3.0312188,
          -5.0258265,
          -4.0094037,
          -4.9597077,
          -5.7167025,
          -2.2600894,
          -6.1327286,
          -3.2959824,
          -6.464718,
          -3.7017798,
          -5.823563,
          -5.3876076,
          -5.2770042,
          -3.5293646,
          -3.1071732,
          -4.9797034,
          -4.554066,
          -5.4001436,
          -5.0019917,
          -5.1851497,
          -3.9200451,
          -4.039367,
          -5.4073124,
          -2.9133878,
          -4.588117,
          -6.7711377,
          -4.921787,
          -3.6088665,
          -2.2014854,
          -4.5762577,
          -3.2758894,
          -4.97255,
          -5.1969457,
          -4.710509,
          -5.1077414,
          -4.232396,
          -4.7791286,
          -4.7105083,
          -4.9363036,
          -5.484357,
          -4.723095,
          -4.7096553,
          -2.2371895,
          -5.35231,
          -3.7852328,
          -5.084049,
          -5.172151,
          -1.8983334,
          -6.8261285,
          -3.0814667,
          -3.7998877,
          -4.29061,
          -2.8357065,
          -5.0905285,
          -5.6398196,
          -7.1189437,
          -6.19763,
          -4.862465,
          -5.547053,
          -4.9214478,
          -4.5826616,
          -5.085321,
          -5.711994,
          -5.0829606,
          -6.668454,
          -4.612203,
          -7.166298,
          -4.847879,
          -3.52871,
          -2.868072,
          -3.1843119,
          -2.9409916,
          -2.8700736,
          -3.2362156,
          -4.114885,
          -2.014723,
          -5.6971416,
          -4.74352,
          -4.467494,
          -3.0094428,
          -6.787655,
          -3.445356,
          -4.8604016,
          -2.122169,
          -4.3171635,
          -2.4088964,
          -3.6544733,
          -4.521852,
          -5.6268907,
          -4.863056,
          -4.6769714,
          -5.176842,
          -3.7398236,
          -5.4711976,
          -6.9408083,
          -4.3074613,
          -5.7812123,
          -3.8599184,
          -3.5634987,
          -2.0822878,
          -3.8135448,
          -6.955888,
          -7.0512905,
          -5.49112,
          -3.9789667,
          -3.36694,
          -2.1546185,
          10.278226,
          -2.9435227,
          -6.884811,
          -2.8385983,
          -5.802241,
          -4.3955355,
          -4.47522,
          -7.118641,
          -4.0874066,
          -3.9467556,
          -4.8361983,
          -4.6667886,
          -5.38576,
          -5.0824623,
          -6.8043585,
          -2.051513,
          -3.3486662,
          -4.9977107,
          -4.388171,
          -2.375441,
          -2.3955662,
          -1.8632765,
          -6.8491187,
          -2.3457162,
          -5.5703173,
          -4.6613297,
          -5.211184,
          -2.7740574,
          -5.5733585,
          -5.3664923,
          -3.6501427,
          -4.816731,
          -4.776686,
          -3.3825085,
          -2.3153288,
          -3.8887253,
          -2.9097784,
          -2.164401,
          -4.1364293,
          -3.4423559,
          -6.8482394,
          -5.736516,
          -3.4072666,
          -2.3111734,
          -5.270714,
          -5.1612287,
          -1.8450477,
          -4.632154,
          -5.606758,
          -3.0874798,
          -4.381575,
          -2.9427547,
          -4.7208133,
          -6.9535007,
          -3.8056073,
          -5.0482016,
          -2.1388583,
          -4.638201,
          -4.4575043,
          -5.353742,
          -4.515148,
          -4.4776435,
          -1.8820543,
          -2.1845438,
          -4.3972836,
          -3.517448,
          -5.307758,
          -5.342336,
          -4.59366,
          -2.9375348,
          -2.9534583,
          -6.0166783,
          -4.368851,
          -6.7216263,
          -4.210165,
          -5.2721257,
          -2.7727568,
          -5.028821,
          -2.9517717,
          -3.9778738,
          -3.525257,
          -5.2229147,
          -2.1738253,
          -6.745749,
          -5.3110914,
          -3.4080937,
          -5.3028007,
          -4.830204,
          -5.0602717,
          -5.0214753,
          -3.2743208,
          -3.6548357,
          -4.64051,
          -5.338813,
          -4.905456,
          -3.532445,
          -3.8041115,
          -5.5365024,
          -4.7409124,
          -4.885217,
          -3.8081148,
          -4.9944515,
          -4.0051303,
          -4.8867755,
          -6.11498,
          -4.573523,
          -4.331296,
          -5.116671,
          -4.124329,
          -3.351314,
          -4.554051,
          -2.0758288,
          -5.623191,
          -5.1724124,
          -5.5719113,
          -4.989977,
          -4.488472,
          -5.113444,
          -2.2585042,
          -4.919069,
          -5.705534,
          -5.2014313,
          -2.0672522,
          -5.771225,
          11.063501,
          -4.7395287,
          -4.246136,
          -4.7668595,
          -5.6915936,
          -5.1849747,
          -5.3591633,
          -4.972049,
          -5.006384,
          -5.792134,
          -5.4854918,
          -4.46385,
          -5.673847,
          -5.0659714,
          -3.6427484,
          -5.834341,
          -3.4312637,
          -6.1358447,
          -5.341789,
          -4.796631,
          -4.888164,
          -3.0246396,
          -4.9980273,
          -4.433964,
          -5.9966855,
          -4.4873896,
          -4.51983,
          -5.0395274,
          -4.8263826,
          -3.8219197,
          -5.243021,
          -5.231044,
          -3.303008,
          -6.6497135,
          -3.4749465,
          -3.0055733,
          -3.9984066,
          -2.1198194,
          -3.5251997,
          -2.29944,
          -2.8442328,
          -5.0963573,
          -5.236983,
          -1.9746789,
          -5.9777412,
          -4.744801,
          -5.2951174,
          -5.5708623,
          -3.689614,
          -6.163301,
          -2.0567212,
          -4.5540786,
          -3.75168,
          -4.380724,
          -5.3683534,
          -5.4958005,
          -5.39568,
          -4.37497,
          -2.287415,
          -7.0791116,
          -6.7385354,
          -5.0598073,
          -4.9279466,
          -2.8711178,
          -3.2614315,
          -5.327008,
          -3.6706524,
          -2.0420558,
          -4.033343,
          -3.8441901,
          -4.6472826,
          -4.846583,
          -6.8816557,
          -2.9256546,
          -4.2867575,
          -4.1206403,
          -5.0844646,
          9.490174,
          -2.9334483,
          -4.472997,
          -4.324301,
          -5.4606104,
          -4.3745575,
          -4.9051423,
          -3.6797388,
          -6.665468,
          -4.5143495,
          -4.9094024,
          -4.1695676,
          -3.2909734,
          -3.3996062,
          -4.9034185,
          -4.767629,
          -4.9043317,
          -5.4729843,
          -5.353899,
          -4.6665244,
          -4.700514,
          -6.5363994,
          -4.176635,
          -6.1041436,
          -5.4083266,
          -3.823942,
          -5.844212,
          -5.269785,
          -3.0571127,
          -4.832287,
          -5.266621,
          -5.061006,
          -2.6256196,
          -5.1625047,
          -2.7616363,
          -4.9070363,
          -4.6241803,
          -5.2934823,
          -4.87263,
          -3.6845014,
          -3.7123263,
          -1.9936548,
          -1.8642519,
          -2.8946404,
          -4.111304,
          -3.2193115,
          -3.3662868,
          -4.803943,
          -2.7668054,
          -5.556092,
          -5.2941175,
          -4.6328335,
          -5.3412414,
          -3.485358,
          -6.7461,
          -7.0336905,
          -6.6234245,
          -4.997228,
          -4.685267,
          -4.6304655,
          -2.1654086,
          -3.6259859,
          -4.973661,
          -7.0853868,
          -4.744456,
          -4.5493665,
          -4.688148,
          -5.2035303,
          -5.1814065,
          -4.437947,
          -6.5463967,
          -1.8500296,
          -3.132291,
          -2.917432,
          -4.001071,
          10.103984,
          -4.5719795,
          -4.752639,
          -6.1125007,
          -2.299153,
          -5.4812865,
          -3.8020291,
          -4.9742737,
          -3.513843,
          -2.5523195,
          -5.096271,
          -5.12419,
          -6.7813005,
          -4.80036,
          -4.339786,
          -2.1466908,
          -4.877535,
          -5.440643,
          -5.2432194,
          -5.1481633,
          -5.483142,
          -4.418505,
          -7.114199,
          -2.0815961,
          -5.5039024,
          -5.3835373,
          -5.448531,
          -5.467347
         ],
         "xaxis": "x",
         "y": [
          -5.066168,
          -7.425559,
          -6.6255465,
          -7.8242793,
          -8.16233,
          -6.490408,
          -6.4570866,
          -6.595018,
          -6.418013,
          -5.9304695,
          -7.9001,
          -5.048302,
          -6.3975163,
          -6.6912026,
          -6.752311,
          -7.6127152,
          -8.4448,
          -5.1560535,
          -5.6755805,
          -5.9385705,
          -7.863943,
          -8.521423,
          -8.508487,
          -6.79375,
          -5.7266707,
          -7.3271017,
          -5.990974,
          -6.407369,
          -7.369916,
          -6.0051374,
          -8.662318,
          -5.191776,
          -6.0030704,
          -6.4264,
          -5.735995,
          -6.0318675,
          -5.9035215,
          -7.8342533,
          -6.817587,
          -6.29471,
          -5.867705,
          -5.9183106,
          -6.95247,
          -7.4252815,
          -5.911206,
          -6.771054,
          -5.239729,
          -7.2346444,
          -7.9519315,
          -8.280638,
          -5.9210653,
          -5.39373,
          -5.832534,
          -8.335884,
          -8.566958,
          -6.5355396,
          -8.159057,
          -8.667018,
          -5.3950653,
          -8.473643,
          -6.3513613,
          -5.631017,
          -8.159734,
          -6.53014,
          -5.29406,
          -3.2465315,
          -5.9754324,
          -5.014604,
          -8.740041,
          -5.7940526,
          -6.432628,
          -5.382669,
          -5.550618,
          -8.618619,
          -8.783278,
          -7.7665544,
          -6.0464687,
          -7.213651,
          -6.705096,
          -7.105963,
          -5.8908315,
          -6.073586,
          -6.131818,
          -7.191489,
          -5.6810436,
          -8.2460575,
          -8.3598995,
          -6.6749563,
          -6.434452,
          -5.5479302,
          -5.8594084,
          -6.639215,
          -5.6749606,
          -8.346477,
          -6.4980264,
          -7.4913673,
          -6.1030354,
          -8.776841,
          -7.425872,
          -7.63399,
          -7.461474,
          -7.3679743,
          -7.292222,
          -5.4495807,
          -8.653001,
          -5.587485,
          -6.084408,
          -6.3998814,
          -5.0983963,
          -8.08444,
          -7.26213,
          -5.339013,
          -6.459863,
          -6.768207,
          -6.529585,
          -7.531525,
          -6.575602,
          -5.8579583,
          -5.6302457,
          -5.9902897,
          -5.6396627,
          -6.0421305,
          -8.168792,
          -7.3957872,
          -8.428258,
          -6.388748,
          -8.399598,
          -7.583755,
          -6.8311143,
          -6.88953,
          -5.951239,
          -5.764901,
          -7.821544,
          -5.9583397,
          -5.9502616,
          -7.2606883,
          -8.442847,
          -3.2458127,
          -5.240564,
          -5.5688043,
          -7.949627,
          -5.268923,
          -5.7560153,
          -4.1293006,
          -7.779401,
          -7.554031,
          -7.920561,
          -4.990627,
          -7.2441854,
          -6.20003,
          -6.2151866,
          -4.522905,
          -7.675459,
          -5.954427,
          -7.8126326,
          -6.0548286,
          -8.223839,
          -8.8377905,
          -6.1359835,
          -7.042765,
          -5.2970157,
          -6.8255525,
          -5.9863496,
          -5.705029,
          -6.308915,
          -7.273063,
          -7.665226,
          -7.750009,
          -6.885686,
          -7.830871,
          -5.8156195,
          -5.957149,
          -6.6084986,
          -6.605832,
          -7.5111375,
          -8.177697,
          -6.2396293,
          -6.4008627,
          -5.9565187,
          -6.484411,
          -7.582532,
          -5.4380426,
          -5.9742284,
          -8.264878,
          -5.780749,
          -7.854243,
          -5.9001846,
          -6.1746726,
          -5.60841,
          -6.3070793,
          -7.707206,
          -5.8814797,
          -5.661279,
          -8.641584,
          -6.498945,
          -8.492876,
          -8.006104,
          -6.2700524,
          -6.1208067,
          -5.2883925,
          -6.1523013,
          -6.806813,
          -6.779132,
          -6.325495,
          -6.388258,
          -7.4493475,
          -7.4004097,
          -8.766605,
          -6.9188566,
          -7.10642,
          -7.5186863,
          -8.516081,
          -8.103692,
          -5.9477572,
          -5.8762083,
          -8.2992115,
          -5.78432,
          -6.5818686,
          -8.725007,
          -8.199369,
          -8.002093,
          -8.561903,
          -7.769822,
          -7.2987075,
          -5.9817643,
          -6.881795,
          -7.2538557,
          -6.011815,
          -7.8018484,
          -6.359166,
          -5.818876,
          -6.764785,
          -5.0966544,
          -7.557842,
          -5.879297,
          -6.976059,
          -6.640647,
          -6.2720265,
          -5.6121984,
          -5.8871145,
          -7.696829,
          -6.717178,
          -7.7335668,
          -6.3662186,
          -6.903328,
          -6.0632405,
          -8.445227,
          -6.0313888,
          -6.782799,
          -4.159925,
          -6.956449,
          -7.281728,
          -7.2693434,
          -6.0534816,
          4.4851747,
          -5.438897,
          -5.6960564,
          -6.200799,
          -8.588841,
          -6.1919823,
          -6.3535705,
          -5.4741797,
          -6.088911,
          -7.329435,
          -6.958989,
          -6.665741,
          -6.300487,
          -5.8308773,
          -6.102365,
          -6.0886106,
          -8.454821,
          -8.862031,
          -7.1181574,
          -8.538803,
          -8.392936,
          -6.913692,
          -7.9872437,
          -5.123147,
          -7.731542,
          -8.078196,
          -6.137878,
          -8.110417,
          -7.116309,
          -6.640161,
          -8.294394,
          -7.4964476,
          -6.3511143,
          -5.6752234,
          -5.4356155,
          -7.762753,
          -5.904649,
          -5.681928,
          -7.7947807,
          -5.1618133,
          -7.939166,
          -5.8185935,
          -3.3022878,
          -8.730951,
          -5.69346,
          -7.3978686,
          -5.6499047,
          -5.5434833,
          -7.5624337,
          -7.741388,
          -6.0944567,
          -5.5857964,
          -7.6379213,
          -8.149885,
          -6.6433463,
          -7.594952,
          -6.9288516,
          -6.211694,
          -5.220525,
          -7.570503,
          -7.640339,
          -8.36423,
          -6.4554806,
          -8.826523,
          -5.9597425,
          -6.7137914,
          -6.2110233,
          -7.583806,
          -5.969168,
          -7.64267,
          -5.957403,
          -8.13553,
          -6.065791,
          -6.957213,
          -8.5710745,
          -6.4749975,
          -5.010981,
          -5.8430066,
          -6.705241,
          -5.8724904,
          -6.5333824,
          -7.0952506,
          -5.8783145,
          -5.9660373,
          -7.539347,
          -7.742649,
          -5.218941,
          -6.8106136,
          -8.47338,
          4.6228657,
          -6.4673996,
          -3.24575,
          -5.7930446,
          -6.4492674,
          -5.814279,
          -5.561278,
          -6.9455204,
          -3.236129,
          -8.420921,
          -6.3645177,
          -8.523854,
          -3.7431052,
          -5.969414,
          -7.0601373,
          -6.2739315,
          -5.917568,
          -7.800488,
          -7.6036506,
          -5.58264,
          -6.4127884,
          -5.827814,
          -5.721232,
          -4.489929,
          -5.397942,
          -6.690937,
          -8.216703,
          -7.921071,
          -7.280691,
          -6.937273,
          -8.456084,
          -7.198796,
          -8.45566,
          -6.082566,
          -6.1301546,
          -7.6584,
          -5.662743,
          -6.055401,
          -6.572588,
          -7.9581714,
          -5.648311,
          -4.417692,
          -6.7099934,
          -6.6097636,
          -6.1989045,
          -7.236967,
          -7.0401764,
          -8.484749,
          -6.8878455,
          -6.4773283,
          -8.64873,
          -6.74074,
          -6.697515,
          -3.2455366,
          -7.2099566,
          -6.2134676,
          -4.430101,
          -5.7650533,
          -5.9927893,
          -7.9497256,
          -8.249144,
          -6.5415835,
          -5.797143,
          -6.70555,
          -8.306709,
          -7.897375,
          -7.6831355,
          -6.9828687,
          -7.5878778,
          -7.8722267,
          -8.15409,
          -6.742861,
          -6.7594543,
          -6.4010916,
          -8.603371,
          -8.577955,
          -6.8314,
          -7.9247465,
          -8.21158,
          -5.554781,
          -5.8673353,
          -3.234348,
          4.867383,
          -6.2251725,
          -5.053698,
          -8.693987,
          -5.8765874,
          -8.548741,
          -6.0021963,
          -6.522701,
          -5.734047,
          -7.679368,
          -6.5157933,
          -5.4120564,
          -6.703815,
          -5.9409714,
          -7.538325,
          -5.8206105,
          -5.6345515,
          -6.0392394,
          -5.892944,
          -6.0068073,
          -6.1084294,
          -7.736085,
          -5.368193,
          -5.5416617,
          -8.193734,
          -5.6823325,
          -4.424792,
          -8.825358,
          -6.08635,
          -5.394324,
          -6.7325687,
          2.2651374,
          -6.2376986,
          -7.2869296,
          -6.1169143,
          -6.827306,
          -7.4974923,
          -4.254216,
          -8.177524,
          -5.904629,
          -6.3495502,
          -6.7558484,
          -8.784268,
          -8.254383,
          -8.576067,
          -6.1479506,
          -7.616477,
          -8.232126,
          -5.611589,
          -5.9794145,
          -7.9000874,
          -5.9390903,
          -8.104473,
          -7.253498,
          -7.2899575,
          -3.2964022,
          -5.763371,
          -5.9045224,
          -7.6977916,
          -7.2287292,
          -5.9487557,
          -8.582298,
          -6.1365604,
          -8.408206,
          -6.848082,
          -7.487536,
          -5.876396,
          -7.6195617,
          -7.331743,
          -3.1934109,
          -5.9952035,
          -5.864813,
          -6.152243,
          -7.5549283,
          -5.4500384,
          -6.6600595,
          -5.7055182,
          -8.613016,
          -8.089923,
          -7.5070286,
          -8.153284,
          -6.5620227,
          -7.648921,
          -6.6712728,
          -7.4737334,
          -5.8211884,
          -6.440298,
          -5.016522,
          -6.887964,
          -8.873304,
          -5.7801414,
          -6.8933377,
          -6.3821115,
          -8.355445,
          -7.004948,
          -6.843085,
          -5.0090194,
          -6.8249846,
          -7.4868665,
          -6.275387,
          -6.759409,
          -6.144381,
          -6.475617,
          -6.0149107,
          -3.245344,
          -5.7571654,
          -6.831372,
          -7.4159155,
          -7.6885777,
          -7.828933,
          -5.71723,
          -6.478705,
          -7.106987,
          -3.335182,
          -7.3520613,
          -7.2042294,
          -5.370852,
          -6.7329226,
          -6.528396,
          -5.4756637,
          -5.4703245,
          -5.4965096,
          -5.4690394,
          -7.789342,
          -6.17833,
          -5.906729,
          -6.2419987,
          -4.411939,
          -7.6570916,
          -6.908534,
          -8.133984,
          -8.936205,
          -6.5782304,
          -8.8325205,
          -8.534553,
          -8.526233,
          -5.211344,
          -6.680411,
          -6.498201,
          -6.5070214,
          -6.717185,
          -8.4697485,
          -5.726358,
          -7.90332,
          -5.8148594,
          -6.764992,
          -8.802315,
          -6.8078327,
          -5.7437716,
          -6.7047563,
          -8.430993,
          -5.9571958,
          -6.8622136,
          -5.9356813,
          -5.0412917,
          -7.6716337,
          -7.8806653,
          -8.580336,
          -8.024285,
          -5.876666,
          -8.135569,
          -8.701486,
          -8.944537,
          -6.525121,
          -6.29665,
          -6.259364,
          -6.0994477,
          -7.7212095,
          -6.257473,
          -6.7157097,
          -6.104137,
          -7.516546,
          -8.743677,
          -6.0590134,
          -5.947913,
          -8.402584,
          -7.5881615,
          -7.6070333,
          -8.091416,
          -5.9611425,
          -7.082229,
          -5.6603994,
          -4.444222,
          -6.6917253,
          -5.5526476,
          -5.9938164,
          -7.8737,
          -8.046197,
          -5.792896,
          -6.9735203,
          -5.637972,
          -7.5427337,
          -5.997451,
          -8.426149,
          -8.55033,
          -5.8220043,
          -3.3852224,
          -7.7521825,
          -6.144703,
          -6.3313823,
          -8.700842,
          -8.319778,
          -7.593462,
          -8.361453,
          -7.7961216,
          -6.2525864,
          -3.2769814,
          -6.3233175,
          -5.796375,
          -8.927513,
          -8.069564,
          -6.9843545,
          -6.3828983,
          -8.130188,
          -6.051611,
          -5.6030955,
          -7.2008057,
          -8.296729,
          -6.5352783,
          -7.578562,
          -6.9293933,
          -7.6547246,
          -8.525345,
          -7.8839865,
          -8.617115,
          -8.425436,
          -6.044547,
          -7.1005836,
          -8.821632,
          -6.0051274,
          -5.818046,
          -8.682809,
          -8.383404,
          -6.508443,
          -5.050487,
          -6.536924,
          -6.5266438,
          -6.140654,
          -6.1997848,
          -8.322906,
          -6.368209,
          -8.402136,
          -6.350686,
          -8.283539,
          -5.8976517,
          -7.7229204,
          -3.2328203,
          -5.040995,
          -6.51976,
          -5.798734,
          -8.474422,
          -6.610583,
          -8.016004,
          -5.628435,
          -8.283643,
          -8.189818,
          -6.707336,
          -8.055542,
          -7.0110054,
          -6.5536675,
          -5.835394,
          -6.4894257,
          -5.8145547,
          -8.1232,
          -8.757565,
          -3.2466903,
          -6.9933767,
          -8.614848,
          -5.6531563,
          -6.2054496,
          -6.620987,
          -3.2291281,
          -6.592209,
          -7.562599,
          -6.597224,
          -6.0351453,
          -8.080783,
          -7.4635925,
          -7.272666,
          -8.168482,
          -7.311416,
          -6.1609135,
          -6.52505,
          -7.8477616,
          -6.3507986,
          -6.424579,
          -8.363307,
          -5.643789,
          -5.8533664,
          -8.315337,
          -7.142466,
          -6.865294,
          -7.4304595,
          -7.8676867,
          -3.2487185,
          -6.7345443,
          -5.9664335,
          -6.1333237,
          -7.1281343,
          -6.7363443,
          -6.9360056,
          -6.6983466,
          -5.672065,
          -6.1516824,
          -7.4326143,
          -6.581747,
          -6.9964595,
          -6.7190466,
          -6.4682355,
          -6.1763883,
          -6.6764827,
          -8.39734,
          -7.338343,
          -7.1439247,
          -5.1887918,
          -5.5538626,
          -7.691674,
          -8.785293,
          -5.7383776,
          -6.3774104,
          -8.16735,
          -5.8079796,
          -6.2038994,
          -7.505379,
          -6.089798,
          -8.155926,
          -5.1102533,
          -5.999965,
          -4.398151,
          -8.068077,
          -8.748466,
          -6.0021305,
          -7.9952765,
          -7.3280053,
          -7.310005,
          -7.634991,
          -6.9164896,
          -5.938785,
          -5.226261,
          -3.311708,
          -6.7163253,
          -5.738933,
          -5.743286,
          -6.655239,
          -8.170456,
          -8.0142765,
          -7.659764,
          -5.5797844,
          -5.9792523,
          -8.45103,
          -6.864447,
          -8.322642,
          -6.756205,
          -7.769684,
          -8.194724,
          -6.6185646,
          -7.5210567,
          -5.8755603,
          -6.4896917,
          -5.8667774,
          -7.2181025,
          -7.338376,
          -8.795892,
          -7.9596367,
          -5.5700107,
          -5.9631767,
          -6.9360695,
          -6.716787,
          -8.441194,
          -8.762852,
          -5.438755,
          -7.845251,
          -5.6634197,
          -5.2526174,
          -6.691113,
          -6.0902696,
          -6.89431,
          -8.051049,
          -5.9361014,
          -7.399029,
          -6.0762296,
          -5.958999,
          -6.4005933,
          -5.890038,
          -7.8116117,
          -6.4549184,
          -4.5157356,
          -6.7920265,
          -7.8711376,
          -6.550838,
          -8.745517,
          -5.1526213,
          -7.2095823,
          -6.312655,
          -7.212231,
          -6.2961993,
          -6.1681485,
          -6.3299155,
          -7.384196,
          -6.5180964,
          -5.39064,
          -8.320809,
          -5.898197,
          -7.028263,
          -7.919134,
          -5.679445,
          -8.71102,
          -8.227789,
          -4.9892507,
          -8.376662,
          -8.347731,
          -5.698525,
          -7.5715823,
          -7.63589,
          -5.941868,
          -8.656977,
          -5.72049,
          -8.350748,
          -5.285695,
          -5.541163,
          -7.632122,
          -6.154769,
          -6.4642835,
          -7.468743,
          -4.440461,
          -6.559262,
          -8.510186,
          -5.733976,
          -5.603043,
          -8.450928,
          -6.9747005,
          -8.298615,
          -6.5876045,
          -7.5514874,
          -7.8565645,
          -5.9058814,
          -5.1719675,
          -7.2482843,
          -6.038353,
          -8.171317,
          -6.2253165,
          -8.21092,
          -6.198878,
          -5.1639533,
          -6.522096,
          -5.9416094,
          -8.500152,
          -4.06825,
          -8.092374,
          -8.265555,
          -5.6679654,
          -8.337638,
          -7.4781923,
          -5.616734,
          -7.728982,
          -5.407188,
          -6.23776,
          -8.188184,
          -8.658967,
          -6.5934396,
          -8.168264,
          -7.158208,
          -6.0445304,
          -5.1798224,
          -5.764777,
          -6.100091,
          -3.4329371,
          -8.419135,
          -6.532336,
          -7.648922,
          -5.504075,
          -6.8519025,
          -5.9851556,
          -7.5899677,
          -8.64299,
          -5.809286,
          -8.590355,
          -8.684547,
          -5.7573223,
          -6.819624,
          -7.8426275,
          -6.174863,
          -8.02491,
          -6.034856,
          -8.718183,
          -7.4997034,
          -7.383129,
          -8.246297,
          -3.2596433,
          -8.16562,
          -8.063232,
          -7.567087,
          -5.7176905,
          -6.484049,
          -6.075084,
          -6.382504,
          -8.231647,
          -6.414771,
          -5.850307,
          -5.721609,
          -5.8519397,
          -8.711745,
          -6.63653,
          -6.2795324,
          -8.56458,
          -6.519728,
          -5.471959,
          -8.832269,
          -5.6703815,
          -6.740706,
          -5.5703483,
          -5.8047876,
          -6.3927608,
          -6.4546003,
          -7.3963394,
          -6.821224,
          -5.6463866,
          -7.1474614,
          -6.197611,
          -5.8137565,
          -5.409336,
          -8.105519,
          -5.8864045,
          -6.0353065,
          2.895253,
          -7.6782584,
          -6.5103807,
          -3.292728,
          -5.5911803,
          -8.663851,
          -8.839398,
          -5.895112,
          -7.0121326,
          -7.051801,
          -8.686566,
          -6.90091,
          -6.7969656,
          -5.267151,
          -5.777652,
          -6.286455,
          -7.682383,
          -7.6517334,
          -8.713075,
          -5.813682,
          -5.8380184,
          -5.7761874,
          -6.3499866,
          -5.8553357,
          -7.558342,
          -6.546819,
          -5.635961,
          -6.770572,
          -7.1167693,
          -6.3133717,
          -5.555491,
          -6.2941456,
          -8.481578,
          -8.078834,
          -6.1804423,
          -5.235626,
          -3.2316885,
          -6.64503,
          -6.873795,
          -5.4553676,
          -6.1592884,
          -6.646264,
          -4.7778726,
          -5.84214,
          -6.6343627,
          -8.047709,
          -5.682549,
          -8.912646,
          -5.5327725,
          -8.2568865,
          -7.0472164,
          -3.45416,
          -5.566415,
          -5.9820476,
          -6.4534655,
          -7.7253275,
          -6.028323,
          -6.9746323,
          -6.5540805,
          -7.307879,
          -8.607672,
          -7.808096,
          -5.942679,
          -6.519929,
          -6.2370143,
          -6.7248607,
          -5.826634,
          -6.438718,
          -6.846832,
          -3.219199,
          -3.2250109,
          -7.2026734,
          -8.576988,
          -6.7244787,
          -7.4584346,
          -7.3031197,
          -6.270197,
          -6.654632,
          -8.148741,
          -7.6141543,
          -7.9082932,
          -7.0394335,
          -6.74582,
          -5.728171,
          -7.447803,
          -5.8163733,
          -8.793492,
          -7.0883346,
          -6.0412645,
          -7.902682,
          -3.9913816,
          -5.654491,
          -8.480729,
          -6.165622,
          -6.7316008,
          -6.351404,
          -6.4870987,
          -7.2729597,
          -7.5973806,
          -8.58172,
          -5.240732,
          -8.313145,
          -8.106296,
          -6.722443,
          -6.670643,
          -6.4957066,
          -7.6444945,
          -8.809104,
          -7.201212,
          -8.453014,
          -8.863108,
          -6.2320633,
          -5.782099,
          -6.7404313,
          -6.770347,
          -6.5645776,
          -8.805167,
          -8.523586,
          -6.5119586,
          -6.328665,
          -6.5212884,
          -7.3153424,
          -5.9606056,
          -6.813541,
          4.294516,
          -5.354516,
          -6.1847305,
          -7.2919164,
          -6.980779,
          -6.52818,
          -5.035593,
          -5.022353,
          -5.0571227,
          -6.0517983,
          -5.491948,
          -6.2818804,
          -6.765919,
          -8.310541,
          -5.3569183,
          -6.443593,
          -5.6792393,
          -7.6214004,
          -6.3961344,
          -7.604652,
          -8.207903,
          -8.3517685,
          -5.9346848,
          -6.3487854,
          -5.817286,
          -8.726482,
          -5.420907,
          -8.108689,
          -5.369376,
          -7.366472,
          -8.875411,
          -5.780589,
          -4.430051,
          -6.5641427,
          -5.9116154,
          -8.318376,
          -8.270847,
          -5.7040787,
          -6.2063217,
          -5.7703333,
          -7.5417986,
          -7.573941,
          -8.728048,
          -5.6377172,
          -6.6230497,
          -8.363069,
          -8.255974,
          -7.8692675,
          -6.9646053,
          -7.615427,
          -6.0088983,
          -8.496738,
          -5.218153,
          -8.430858,
          -6.7673936,
          -5.537317,
          -7.936495,
          -5.6039805,
          -5.720814,
          -6.161326,
          -6.4799175,
          -5.5239654,
          -6.176888,
          -3.242246,
          -8.385276,
          -6.258302,
          -6.672183,
          -6.151156,
          -7.3229995,
          -7.1127715,
          -8.686878,
          -8.75801,
          -6.1616936,
          -3.2367616,
          -6.482067,
          -8.307559,
          -7.584165,
          3.3441877,
          -3.2647767,
          -5.8299685,
          -6.3597174,
          -7.943858,
          -8.363673,
          -6.1128764,
          -6.848534,
          -6.4950824,
          -8.668204,
          -8.609547,
          -8.51001,
          -4.0133576,
          -5.902079,
          -5.281285,
          -7.3485622,
          -7.3734946,
          -6.4842324,
          -5.5642796,
          -7.6506896,
          -6.1851807,
          -6.847113,
          -7.4219184,
          -7.2915535,
          -5.549058,
          -8.554314,
          -6.6714463,
          -6.4091477,
          -8.328613,
          -7.9654493,
          -8.231269,
          -8.434385,
          -6.2846007,
          -5.7932506,
          -6.0862455,
          -8.80286,
          -5.541619,
          -6.3416004,
          -8.821262,
          -5.982616,
          -6.954521,
          -6.306274,
          -5.702675,
          -7.455701,
          -6.4399605,
          -4.3608427,
          -6.8875623,
          -7.3481703,
          -6.3671694,
          -5.430292,
          -7.2090425,
          -7.453818,
          -6.3073483,
          -7.6986866,
          -6.429943,
          -6.3971143,
          -6.210687,
          -5.753291,
          -9.006564,
          -8.533512,
          -6.353689,
          -6.5931044,
          -8.234611,
          -5.731911,
          -8.943701,
          -7.3843384,
          -8.411809,
          -6.2854667,
          -5.037596,
          -5.7859936,
          -6.4354076,
          -5.8657618,
          -8.180224,
          -7.762443,
          -6.9804344,
          5.27704,
          -6.104372,
          -7.852067,
          -7.568015,
          -5.8630095,
          -7.563479,
          -6.3284435,
          -6.1336584,
          -6.79055,
          -6.708508,
          -5.1592255,
          -5.403773,
          -6.365136,
          -8.341417,
          -7.4107,
          -5.6584496,
          -6.313336,
          -6.8986526,
          -8.283947,
          -8.640774,
          -5.5866075,
          -7.414299,
          -5.981658,
          -5.5144854,
          -8.202095,
          -6.8417964,
          -7.368914,
          -6.083288
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "label"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "UMAP Projection of Valid Features"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c7d6150f-bfa0-4ad1-8cf5-dcff56d77ce3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c7d6150f-bfa0-4ad1-8cf5-dcff56d77ce3\")) {                    Plotly.newPlot(                        \"c7d6150f-bfa0-4ad1-8cf5-dcff56d77ce3\",                        [{\"hovertemplate\":\"label=Dog\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Dog\",\"marker\":{\"color\":\"#636efa\",\"opacity\":0.7,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"Dog\",\"showlegend\":true,\"x\":[9.374576,11.664153,9.865637,10.312103,12.499628,9.058861,-4.8311133,9.602362,8.810123,9.967304,10.665594,12.547597,10.759385,11.2544565,11.456573,11.39665,10.201269,9.809888,9.024736,9.60324,8.658779,11.638123,9.649982,11.3455305,10.428519,10.39384,9.852422,9.5251045,11.888587,12.648829,10.604397,12.689399,8.5019865,10.7321825,12.591793,12.607427,12.222824,10.155426,12.006969,10.603145,10.155885,11.225825,12.739087,11.058471,11.008299,12.114425,9.673509,12.651362,11.407324,10.6845665,11.203926,11.81604,8.774018,8.810974,10.604383,10.221313,9.606398,9.225284,10.142987,10.350686,8.436897,11.543985,11.303921,10.604458,10.618195,9.755954,12.501968,13.217555,9.752474,8.655605,11.848745,12.57742,10.231975,9.376074,10.775823,8.878108,10.109458,10.218341,9.539921,8.340905,11.724513,10.56739,9.461502,9.466263,10.277877,9.751771,11.107198,12.708456,13.0169115,11.925215,9.788591,12.637917,12.482611,10.392359,10.536123,11.0907,11.131508,9.237232,9.7646055,8.774872,11.198737,8.798836,9.442295,10.467665,11.954395,9.144053,11.948794,10.003415,9.5735855,12.076704,10.587363,9.334867,10.287985,11.1750965,11.962684,9.569915,9.47941,10.593983,9.675892,11.259455,11.38174,10.528152,11.386179,9.45438,11.577645,10.452601,9.213205,12.415732,8.466796,11.061401,9.46121,9.577496,11.391385,12.082205,8.590366,9.577045,9.527692,11.373986,11.449158,12.531991,11.636703,11.843799,11.063353,8.809567,10.704194,12.565459,8.893027,12.45754,9.646777,11.214811,12.707256,9.401738,12.481858,10.749526,11.504996,11.640162,11.129208,11.190167,8.971674,8.677972,11.604097,9.447301,9.388123,9.93747,7.3308344,10.717713,12.327223,10.446527,9.58409,11.12174,10.611118,10.1520605,10.059169,12.254628,9.563511,11.319999,11.39681,10.298494,9.489368,12.491463,9.405704,10.111393,8.832863,8.4106455,12.644372,9.44291,9.582951,9.816214,9.544248,11.220314,9.205283,10.314472,12.402173,9.706954,9.645242,8.746789,12.562019,12.63091,9.768603,9.020623,9.456043,11.876625,8.747432,11.390949,9.504687,8.407416,11.032316,10.928408,12.530276,9.982475,9.070616,8.443099,12.201566,9.6086645,13.196266,10.67003,10.785904,9.93119,11.202248,11.309414,10.903404,8.893323,11.173387,9.254805,12.721126,11.416752,11.700916,12.4702425,9.868975,10.822079,11.9263,11.136352,9.462731,8.803405,11.448971,10.690933,10.281317,9.430084,13.172813,9.715711,10.640638,9.920751,10.401981,8.424079,10.439503,8.6870365,12.622719,12.624333,9.645836,10.387962,12.487219,9.378176,12.601833,9.206104,9.856611,12.454953,10.101888,9.302824,9.487972,11.382568,9.345036,10.034905,9.251897,9.451137,10.149068,10.250022,8.490629,9.688293,10.954486,8.926434,9.549215,12.106051,12.000337,12.7029505,9.530317,9.658587,9.192922,11.563167,12.555785,12.222757,9.943809,10.045787,9.723103,10.165974,10.175188,12.62138,11.679813,11.241094,11.918924,13.179307,11.621355,10.642039,10.518301,12.154979,12.706792,9.617324,10.129586,11.97546,9.893812,10.362724,12.51103,12.005003,9.369878,11.601947,12.283611,10.426425,10.918849,7.3322372,9.353343,8.857627,10.132133,9.748957,11.34723,10.732167,12.203293,8.704468,9.518755,11.563194,8.633683,8.929024,9.516285,12.629174,8.618742,9.796865,10.32994,10.132915,9.437654,11.648362,9.529486,12.806064,12.118117,8.442268,11.104362,12.569568,9.864938,8.469657,9.82514,9.397669,10.145265,9.854187,12.033269,11.172558,9.354893,12.287763,9.590646,11.083768,10.160413,12.197133,11.02478,11.339694,12.654846,10.110463,11.660306,9.696242,8.723731,10.960397,13.111755,10.584322,12.033653,12.619227,8.428701,11.659854,10.172877,11.245335,10.224099,10.516684,10.303133,11.066364,8.821754,8.615385,9.85521,11.07374,10.396665,11.546469,12.498086,10.004447,8.768287,9.492311,11.1629715,12.522201,11.085359,12.563017,10.191258,9.439049,8.851086,9.121806,12.811753,9.338949,12.628545,9.866261,11.788298,11.253642,12.623959,10.897499,13.08763,12.261191,12.62798,13.13653,10.787224,9.758489,11.460318,10.975831,9.572503,10.3544855,12.236637,9.249052,9.5081215,9.769501,8.467142,9.341997,12.513409,9.749426,9.633983,11.05203,10.97796,12.418348,8.591653,12.611717,9.161408,11.049745,10.547229,7.3350782,11.743366,8.634676,12.40333,9.463832,11.848377,9.310811,9.731793,8.709019,12.3542185,11.32471,9.631568,11.648265,11.714146,10.784294,11.045459,10.799309,12.5745325,12.355822,10.113781,11.444938,10.5644245,10.385964,9.310634,9.688365,10.033969,11.162275,10.525592,10.030502,9.699486,11.509383,11.22546,9.169395,11.222782,11.630824,11.261352,9.987841,8.787816,12.675235,11.9297285,9.723648,8.83135,9.800223,11.173219,9.718976,10.241125,9.654693,12.423866,9.593757,10.921527,9.828367,9.994635,-2.5157974,12.583332,10.097241,12.513271,10.337933,9.697414,11.551312,9.422224,10.066885,11.123847,11.868304,11.378221,10.374976,10.851866,9.700091,9.946508,9.731539,8.7405615,12.445545,8.690085,11.603016,8.551123,9.724075,12.697399,9.863047,12.566675,9.669548,10.607836,9.679455,8.725993,10.801033,12.418048,12.337239,10.06007,10.490417,8.819282,12.3817425,10.421014,8.809311,11.238326,8.404288,9.7364435,10.83741,9.105713,8.5111685,9.555266,10.011656,11.573754,8.807974,8.894805,11.790231,8.441028,12.004434,7.330279,11.822685,12.366872,10.542514,11.634298,9.631145,9.496539,9.572807,8.536809,10.32749,12.200634,9.713158,9.521029,12.020345,9.811754,11.822147,12.461233,10.512418,11.554931,12.651429,9.44977,9.731125,11.911994,8.903211,11.734308,9.489183,9.602552,11.171191,9.641614,12.517627,9.872352,10.495942,12.593652,8.920707,8.597936,11.201576,11.940777,9.690756,8.357936,11.183126,9.675819,12.269674,8.921039,11.511774,12.610909,11.688418,9.641046,12.020366,8.714286,10.305671,12.349709,10.8872595,9.456396,8.786659,10.46465,11.365016,9.729256,9.995266,9.636697,9.927789,11.443369,12.338174,11.044418,11.935249,8.342067,10.090785,10.990205,12.725809,9.495427,11.631767,11.042414,10.822533,9.480975,10.88689,9.745502,9.03303,13.230339,11.527782,10.604124,12.579321,12.517178,13.149801,9.792838,11.913633,9.326177,9.808183,11.580033,11.532475,11.4036875,12.079481,10.1457615,12.56175,10.017414,9.928715,11.650298,8.40668,8.873001,11.63665,12.255007,8.859183,12.589363,9.248873,10.158745,9.849391,12.385522,10.162347,9.738053,11.335523,9.056868,8.886076,9.681782,10.552034,9.56315,12.550327,12.509792,9.111655,9.686536,9.852517,11.323027,10.197621,8.990612,8.404414,12.382553,12.483025,12.014877,10.60116,9.69996,9.411061,11.050987,10.908483,11.117481,9.061687,9.097182,9.417508,8.482739,9.6321535,9.832768,10.016044,11.05144,13.198101,10.306378,10.920571,11.651,12.663597,11.963513,10.267946,9.632223,11.483129,11.46672,12.629626,9.699118,9.262125,10.204467,10.896692,9.555058,10.755048,9.886787,11.187696,9.193026,9.705302,9.729812,11.115586,9.715353,9.493419,8.882227,12.605871,12.644402,11.060264,8.698155,9.900158,9.810261,10.888851,11.818396,12.690219,9.685143,10.554162,7.339557,8.781495,10.966574,12.628352,8.892281,8.872952,10.42714,9.646745,12.549585,12.16515,9.748546,11.21984,9.544617,11.435158,12.550738,9.8277,9.672107,12.639499,8.60245,10.60165,12.179479,8.342354,11.029229,9.69809,8.81016,11.286309,11.777842,12.549371,8.409929,12.340428,12.166239,9.562025,10.541522,12.231582,8.337123,10.243832,10.316271,9.754547,9.365959,11.735329,12.581899,10.366994,8.594576,-4.1271563,9.334357,9.120896,11.333738,9.790381,8.746347,9.764854,9.827506,11.143387,10.567613,12.283661,8.3166485,11.773213,9.74192,12.213676,12.20813,9.679566,10.861885,11.589964,11.321096,11.562243,11.462612,8.904942,11.405072,8.823824,12.586005,11.77937,11.454112,8.851317,12.436345,11.326265,9.88615,9.662546,11.56203,9.653656,11.316468,9.367652,10.314657,11.196573,9.790577,8.361253,10.578239,10.013381,9.75242,12.44394,10.666247,11.213975,9.610043,12.706018,11.270196,10.195907,9.630135,12.578959,9.445203,10.9514885,9.544724,10.639922,9.574605,9.893145,10.649062,11.002584,10.82778,9.425792,9.888187,9.461346,8.616568,9.472414,12.21297,11.353279,12.653455,8.707595,12.24903,8.703381,10.968306,12.638453,7.3225613,9.432111,12.355397,11.275695,9.6090145,10.902552,9.704591,9.535183,9.809523,9.891502,12.229651,12.655541,9.249021,11.423009,11.62523,10.695369,10.053779,10.065279,11.641225,9.640688,9.356091,8.698618,10.469066,9.711876,9.463376,9.644092,9.462475,10.583717,13.226088,8.6557455,10.0884075,11.485598,9.793558,10.525295,11.27782,11.65625,12.613426,8.586162,9.571754,8.8046255,10.309156,10.078216,9.462521,9.427304,12.551581,11.773685,12.546025,9.15773,9.528886,10.017067,11.701146,10.316229,11.925958,10.825023,12.8334255,12.11526,9.188293,12.302395,9.635688,11.292468,9.66917,9.047851,12.36979,12.406905,10.802133,11.154042,9.742421,10.89817,8.586788,9.868141,12.619745,9.736232,10.631478,11.643726,10.863099,11.064101,8.582397,10.042648,9.6105585,10.07639,8.57735,10.648601,10.244759,11.863196,11.256187,11.171177,8.441794,11.020486,12.0084305,12.4324,12.234765,11.32641,11.071283,8.3093195,9.583841,9.794449,12.013033,8.854529,9.457831,9.928024,10.999536,12.54632,10.198223,10.648273,10.8567,11.567789,10.820138,11.617586,9.494962,12.5344925,12.624791,9.475651,9.135806,9.404983,12.193863,11.730827,9.856768,11.465685,11.478601,9.35426,11.232186,10.18053,9.587327,12.665882,11.724187,10.183266,10.21539,8.785691,9.989639,12.620364,10.689774,10.145037,11.12834,13.208141,9.164773,9.714349,12.0106945,8.825206,11.5611105,9.439907,13.019282,8.772695,11.704325,8.407055,10.420546,9.53279,11.11346,11.52182,9.658547,9.374635,9.735807,11.276842,9.310962,10.17392,9.937276,9.677627,10.94594,10.736837,12.599823,11.544026,8.980317,9.836704,9.713696,12.314048,9.227415,10.612823,9.570947,10.6711855,11.448643,9.322512,10.532073,11.414982,9.720326,11.904881,9.690711,11.696005,9.560999,12.483718,8.671908,8.778636,10.872536,8.440441,8.7906685,12.369392,9.851591,11.906624,10.539718,10.472,10.456074,9.341838,12.487066,12.735611,9.738638,11.570753,12.620041,12.70558,8.859716,11.267213,10.299598,9.802544,12.577744,10.524857,12.552597,11.04809,11.675251,9.413491,10.507807,9.570891,10.033926,12.329143,9.24171,11.424995,10.366324,8.604091,11.3511095,9.514219,9.745359,10.628145,11.594145,11.018046,10.194104,11.571978,10.091616,10.791032,8.360254,12.552612,9.139725,12.616854,9.580676,10.673939,9.783007,12.534656,9.456701,10.727883,9.617877,11.660155,11.361048,8.748131,12.681841,11.578616,10.020125,12.646796,9.737776,10.012029,12.589057,9.218868,10.065574,11.623716,11.490578,8.825355,10.624568,10.260048,9.626395,8.602388,9.835636,8.373354,9.31172,12.019965,12.394322,8.904009,11.560618,9.714322,9.597749,10.679038,8.692872,10.981461,9.443546,12.1623745,11.440008,9.018753,10.980949,10.806662,9.664831,11.683664,11.700374,11.71915,9.532426,10.36497,8.879545,10.1332855,11.230991,11.924522,12.307625,9.935554,11.129208,8.848253,9.915061,10.008135,12.695096,10.813338,12.658735,12.528286,9.886799,8.910892,10.218699,9.869023,10.814978,11.862564,9.57461,10.402138,12.2959175,9.992428,8.750475,12.184827,11.082478,11.779184,12.654517,11.246702,10.249315,12.608197,12.456038,10.576485,9.438554,12.554587,9.645674,8.707652,10.067439,11.8799095,9.770102,10.204338,12.032227,13.209645,9.0421095,12.168849,10.734196,10.501201,9.991072,9.963108,8.76966,10.076856,12.430699,9.829982,9.6658125,9.895287,8.516064,9.961023,9.823311,9.9726,9.640277,11.463377,10.437648,9.741984,11.356534,9.362675,12.620665,8.702565,10.819038,12.3523245,9.994608,9.407546,10.382807,8.417149,12.52087,11.77921,9.498095,9.515274,8.576187,12.053669,11.826385,11.32397,11.864534,12.622843,9.607962,12.293678,9.765524,9.79865,11.919916,9.891884,8.825301,8.781287,10.037982,9.852238,12.6853,9.668116,12.017534,9.719557,9.454256,10.4973955,8.513109,9.4341345,9.8172455,12.523524,9.419821,9.90131,11.619283,8.966636,9.504717,11.395886,9.382107,8.4273615,11.739544,9.094825,8.958591,11.883331,9.641087,9.866294,11.7669,9.431921,9.6025095,9.316937,8.979705,12.459291,10.029397,11.89031,9.573305,11.759638,9.356553,10.478313,10.137822,10.546406,9.52427,9.553232,12.696017,10.528886,9.283734,12.352286,9.512371,11.922989,10.2615595],\"xaxis\":\"x\",\"y\":[6.604148,3.7584264,3.7379966,3.561493,5.415899,3.4310014,-8.788725,4.9201646,3.5335398,3.1873255,2.9256525,6.0087934,5.020589,6.134492,5.6126256,4.823803,3.8908916,3.573445,3.54191,4.877772,4.4883876,2.3950183,5.8906593,5.519247,3.8595066,4.977317,0.9421182,5.1697946,3.5593247,6.133783,3.2285888,6.276565,0.73999953,2.1901417,5.5838118,6.3033605,4.7545056,4.3865094,4.2924795,2.2060995,4.758752,6.2185526,6.2069798,1.8968306,5.635702,6.37202,4.6281652,3.1842947,3.7335374,5.4500613,3.9247408,3.7554245,2.9341853,5.619495,4.6430798,6.429401,3.0611362,4.16793,2.9947598,5.2864947,1.1607786,3.811701,2.3941123,3.5817838,2.281284,5.796883,4.554188,6.160136,6.05363,5.6183505,5.1474357,4.517663,6.4573736,5.047465,2.2434208,4.760891,4.246091,3.796867,5.4897985,4.4862766,3.4318917,3.0135267,5.2641506,0.27812368,5.873924,5.7969913,3.2620523,3.1534352,6.086455,4.040778,3.1809778,2.1280468,2.585319,5.2592053,3.528213,1.9452734,4.4113393,3.8383315,2.9757326,5.485673,5.9318542,0.57707244,0.18998486,3.2033179,3.5140765,3.3153596,5.3053465,5.9412804,4.760695,5.2097044,3.1183846,6.6350718,2.047688,5.219728,5.0948195,0.56445885,2.9084873,2.647033,0.6569267,6.281078,6.4539146,3.1168883,5.8487973,5.122657,2.4048407,6.041587,5.2474084,3.0873492,4.4588,5.2159314,0.3029459,6.2126813,3.7468162,6.03697,4.556199,2.821859,4.2338257,5.5342236,3.632803,6.170242,6.196747,3.9634624,2.0639732,0.5656971,4.5952225,6.148228,5.5284605,4.465677,4.5334945,6.3813224,3.2683132,4.9545836,3.2782161,4.306421,2.3229635,2.29801,4.0799875,3.8133314,4.886725,4.360272,2.2809582,0.2897031,6.594021,4.6653514,5.094324,5.957203,4.622855,4.4804387,4.5941505,3.5442598,5.2081585,1.5787234,5.688315,3.4942517,4.384784,3.5945206,6.515419,3.7899652,2.961128,4.596109,0.38077828,5.187216,3.760024,1.4042387,3.2091327,3.9425693,0.59322816,5.8271155,4.5188737,2.5141113,3.5709863,3.224435,5.425323,3.067814,4.5407104,5.6180058,6.3783774,6.3013697,0.8519677,3.8118832,0.28866518,3.2690694,0.7135855,5.590847,6.1476135,1.4094417,5.050795,4.4276686,5.4994774,5.514032,3.8440957,4.633367,4.7843995,2.1337214,6.1492553,3.2576513,2.1761916,0.71565866,2.4316957,3.5853922,3.660832,3.769031,5.790232,3.8033836,6.2280703,6.5424676,5.6035976,6.0358195,1.3205045,3.8908005,6.0170684,5.6730843,5.443033,3.856427,5.670126,4.3343754,4.3428636,5.1178823,6.169802,5.8473353,2.2948565,5.9201016,3.1035168,4.4329863,3.2304156,2.9396405,2.1661267,2.1516097,5.6360106,4.3417387,2.2821577,6.63337,4.509016,5.7159557,5.9289145,5.9034586,3.6917949,6.646602,0.32525614,3.3069806,6.5931544,5.914056,3.6994832,4.7588024,5.2601676,6.4551387,1.2979345,5.1430025,5.3053966,4.797596,4.369285,6.1817384,5.573326,3.2141087,0.967722,0.7204985,3.7638965,3.3229792,6.2201777,5.081516,4.528146,6.2057667,3.2004004,3.9631064,5.2707467,2.1368272,2.5333703,2.8787167,5.6122413,6.13526,2.4947076,4.8469334,5.9143524,5.449043,3.2454894,4.587794,5.884636,5.668067,5.4352393,3.869542,4.58792,3.5558212,6.5819116,2.2683644,5.474406,5.161968,4.7303243,5.092221,5.6258116,3.746745,4.074782,3.8721824,6.4137583,4.2646375,2.8572092,0.5764957,2.0382636,2.9564357,5.628111,4.8252306,6.4172425,2.1498253,5.614516,4.608249,2.5014572,5.0475693,6.4595594,2.5211012,6.2186813,6.201145,4.9748616,1.2966278,5.673503,4.5301137,0.8921979,4.34577,5.664344,4.3323126,5.587461,0.91394633,4.2989936,2.8660197,5.5674496,3.3372622,3.7897954,4.371957,4.7557306,4.8887596,1.8952882,4.5649185,6.3359323,3.0481834,2.390356,2.8805141,5.6429896,5.931113,6.106798,4.8204656,5.6927676,3.248136,1.3789903,2.5526943,5.833437,6.302609,6.420648,3.3615196,3.5221918,3.218669,3.7172315,5.606695,5.897626,4.2414303,4.572597,3.8506513,4.4699097,0.5739138,3.9645658,5.7932525,5.831921,2.2567666,2.9261172,3.274857,4.0644584,0.4425373,3.8424509,3.307427,6.131739,5.5080404,3.1794283,3.260968,5.6109695,6.3243437,3.1688337,3.0206654,6.151981,3.5477333,6.279627,6.089873,4.315778,5.982975,3.709099,3.5220945,3.0950005,3.441006,5.299135,3.7182055,5.8504496,3.229094,0.75982034,6.646772,3.249394,5.7526813,4.8007374,4.947099,2.745548,6.096889,5.6171494,3.5310168,5.068702,5.24986,3.1049075,5.092557,3.492365,0.79396516,6.4281874,5.724568,4.073411,3.6309788,5.2009673,0.67973226,5.476004,6.1678247,0.89493203,2.4284825,4.433518,4.5365186,1.8748262,2.327064,6.18816,2.3301802,1.2978435,5.6623516,3.6100962,3.5442274,5.500236,5.9115233,5.126092,4.6227,4.51005,0.5395143,1.4663365,2.8824754,5.836973,3.2674468,5.4535375,2.546942,5.718965,3.5073342,5.691149,3.2616248,4.8249187,4.747171,0.599317,1.410562,5.6445284,3.1656983,6.467919,6.09972,6.4353323,2.9542298,3.7358754,1.2802671,2.6936486,-6.4248652,5.612578,5.612287,6.426236,3.8326046,3.6535537,5.5391674,5.6233377,2.7409997,3.2676766,4.214315,6.4938974,2.5059085,5.4067187,4.652653,5.3075995,3.3050582,3.086193,3.278502,0.6498405,2.5784407,0.7172527,2.9917283,6.2380233,0.9515443,4.52648,3.0795388,3.8050897,3.0911634,5.2083473,3.4355674,6.4560966,3.384163,3.928683,3.181237,4.650889,6.4604135,5.6972857,3.8856263,6.1458883,1.3885833,2.9453053,3.5241365,3.8463683,0.794721,3.0479484,0.55798846,2.3511946,0.5578274,3.723876,3.8844132,1.4302791,5.662487,5.093434,3.7428584,4.671376,3.3324518,2.3061438,3.9755564,6.4150367,4.871915,0.7696691,3.331455,4.2103868,0.99908054,1.0123445,6.024977,4.3375216,4.9058003,2.281134,2.9843729,3.3841925,2.1033394,4.0876827,5.9821057,3.959818,5.4515004,3.6938286,4.333485,4.628186,5.4086833,3.0134819,6.446114,5.9421997,3.095019,6.1117377,4.832521,0.80727774,3.50838,4.24019,3.247303,4.3972692,5.798934,4.5276036,4.906316,4.825094,5.6000323,6.2373176,3.876471,3.2635117,3.782769,5.6559873,3.8684747,3.4348001,2.3868551,1.0459634,4.097273,5.39399,3.7901862,2.9670253,3.681482,4.660302,1.1085306,5.6828337,5.2012596,1.8861535,4.239048,4.4238944,6.2586856,5.0228677,6.0823474,4.858594,2.5636144,1.8800176,2.3381937,6.3784723,3.453059,5.969264,3.741525,6.162283,6.549039,2.979989,3.2099516,2.178129,6.1194186,0.82454467,6.390649,5.1893463,0.8229044,2.644816,3.507361,3.2089422,4.929989,4.786346,3.15423,3.2933788,0.7645578,2.621409,1.3908571,4.867844,2.2949514,3.5217721,3.7206435,2.361815,5.1099668,5.636533,5.671661,6.467506,1.406383,4.6695395,2.961201,5.5194197,3.6805956,5.7643113,3.3741248,3.2161946,3.1920025,3.2681203,3.7082057,5.9579144,4.9898725,4.086826,4.2521777,3.9224231,1.3974187,6.3741045,3.2915485,3.413072,5.6885085,3.6735299,3.7383041,4.0805826,3.5740082,5.599263,3.6642365,3.8818884,0.23736963,0.75614053,5.6430016,5.2471595,0.7362846,3.0133767,6.1428866,1.8270944,5.4632983,2.5469391,3.2522283,4.5789437,4.5784774,5.8713055,3.452411,4.083147,2.1375163,5.9246926,4.5795946,3.0900705,4.635383,5.890742,3.4572709,4.3868084,4.311745,3.4013624,2.8232799,5.726486,3.3187997,3.683252,0.9376696,3.8013978,2.239069,2.1139088,5.7967415,2.8792422,6.02791,5.8444343,5.4915285,6.3414145,3.25164,2.7501292,3.006528,5.093427,0.5603907,5.737069,2.1704032,0.5755944,4.8154917,5.0213647,0.5772379,3.0308723,4.8589487,5.428106,6.2564497,4.5574603,3.7828481,5.37461,0.9495527,5.8326087,2.1259952,0.6762862,3.5593798,4.6522675,4.404561,4.077976,5.837124,4.973191,6.3729434,4.5322347,2.1611385,1.4221134,5.981557,5.882208,4.723898,4.852406,5.4274187,4.4631147,4.3980236,4.6776624,5.8274646,6.6096387,3.3918848,4.507929,1.4755476,0.7586153,-6.4798474,6.614194,5.379183,5.905374,1.0832955,2.8519127,0.7898636,6.1033363,2.9393144,2.9886346,3.4908524,4.428035,5.5036654,5.7331038,5.527042,5.4365387,5.763666,4.8633356,2.2854664,2.59193,3.3667464,2.5579283,3.5682309,3.8294218,3.8273354,4.5061836,3.348911,3.3890564,3.7797656,5.2850437,3.1879025,6.007589,0.9525614,2.3985367,5.2904725,6.4047837,3.5915384,2.7633655,3.6081665,4.128355,4.415246,4.777494,5.606279,5.311608,6.4596114,4.009887,5.886098,2.9875453,3.1957474,6.3916216,3.7530859,3.2832677,4.5111732,0.24995135,4.6865554,0.9222992,3.7739573,6.239126,0.99876034,4.3412843,3.6536927,2.3748865,0.20184718,5.454969,5.0042067,5.570359,0.26708478,4.8399715,3.2716272,2.1189137,2.879753,3.3325746,2.8841407,4.9492655,2.126837,5.0964913,0.1985112,6.5176053,6.274579,3.1021729,6.047479,3.543363,3.1380708,1.061097,3.5966892,4.560767,3.1911347,3.507611,5.6415706,6.5752945,4.768375,5.2903957,3.9350023,3.6177926,3.155677,6.5661674,2.8843908,4.7578216,0.9877411,5.7734566,3.0109344,6.3979754,5.623189,6.1735573,0.6174892,3.6266932,6.559209,0.6774047,3.6201978,3.0693653,2.3418245,6.1811204,5.5984273,2.7512844,4.448102,4.298867,0.4730681,6.0234327,2.5653856,3.4206862,4.0371685,3.2140584,4.0480433,4.457652,5.2086563,2.5685902,3.857397,5.585172,5.0408664,6.0774517,5.43,5.367643,5.4080696,2.8967507,3.4735932,4.552093,3.3794596,6.0162463,3.3753998,4.0573864,5.6364126,3.0767012,5.444575,0.805229,5.677493,3.182373,0.753598,2.2214034,2.8435614,2.1344955,1.8802218,0.7113886,4.1015034,2.9971778,3.3712206,0.71990556,4.688889,6.438521,4.834324,5.2891107,3.3016188,1.4570056,3.057821,3.2590628,5.1983747,5.267379,3.9152381,3.2025063,4.41733,5.5749464,3.8713875,3.4970791,0.57019466,5.9586906,3.5949826,5.2181807,4.4933486,2.992993,3.338251,3.3875198,5.166154,5.444867,2.5551,0.29951164,5.389702,3.2049367,0.23399693,5.5239816,6.3702283,5.7927012,6.593457,2.7866855,3.3846312,2.3794897,6.559527,3.2048616,4.76656,4.762953,2.1016724,6.5835648,5.682191,5.085424,4.1789947,0.61410934,3.2564273,4.713986,3.7867992,5.82581,6.1566005,3.2868514,4.6736097,5.6464124,3.6958308,4.318688,0.95840394,6.075792,4.583788,4.488346,1.4102743,6.0826173,5.778154,5.7936163,2.3488708,0.9708869,3.6184268,5.006812,6.41604,6.6816397,3.1231332,3.6547992,5.5274715,5.3446,4.4987845,3.2298434,2.3372145,4.7509265,4.580332,2.917828,4.6006584,3.5884907,3.1197371,4.3518124,2.2612705,5.508855,3.8262699,3.0828989,5.6985035,3.1543963,3.7712889,4.3904095,3.1861625,3.1689665,6.2727404,5.612139,0.61344814,5.3907833,1.4859922,0.5361406,6.4744606,0.62582296,4.1312003,2.9588363,3.0582247,5.699334,3.7358723,5.511168,6.1714854,3.1792226,2.350222,6.2034206,3.2568824,3.7126749,6.2980866,2.7633169,4.4219384,2.2710917,5.353339,6.382513,5.0342026,2.5654726,6.4783125,2.9928896,4.5382757,3.752119,4.6334476,5.3974853,6.449173,2.4271631,5.6499686,6.3280697,2.9393277,5.6804886,4.2804885,6.5624785,4.7828608,4.830035,2.8887658,4.3407445,2.1744525,4.450121,4.539104,3.3155172,2.125496,5.979862,3.0854678,5.886137,6.3128896,5.765786,4.7361064,4.4431925,5.957486,3.305125,4.974565,2.0760558,5.5350404,5.2474136,6.3514705,0.7459893,2.7462966,5.8709064,5.75911,5.9520917,2.7898865,3.3560283,4.206782,3.1001375,4.9113507,5.8020153,5.6441073,1.216799,1.360654,4.521839,5.6166024,6.473286,3.5937548,2.6985323,1.225773,4.4447403,2.1567645,0.59522194,5.803297,6.6032147,5.2371206,5.0192165,3.772301,5.081695,2.225534,3.149051,5.3788257,2.7322795,5.41949,6.024886,4.6767354,3.6607304,5.439157,4.69922,4.0811815,3.211021,5.4840016,5.191817,0.50909156,5.8152018,5.270708,6.143155,2.3003824,2.1271222,4.5183053,5.4993668,4.2904043,3.9954405,4.216397,2.8697915,3.837389,5.728325,4.3121023,3.5291255,0.5384637,4.98284,5.3502574,6.3324,6.6098986,2.1254714,6.293412,4.6876445,5.6985455,4.650152,2.3410044,3.2417564,6.422785,6.086643,5.677309,0.4734649,3.784652,4.8230214,5.143947,4.9423747,6.147633,3.3689892,4.8532553,4.518334,3.3502543,3.4296281,4.7357492,5.649017,3.776565,5.426312,6.1195636,0.72811615,3.0034802,0.7286468,4.8158216,3.1198318,2.8193848,3.1065364,3.4286416,3.0426447,5.96508,3.7120092,1.0854622,6.2770157,2.8822317,2.161454,6.4768004,2.7906806,3.7073767,4.456703,1.3708851,5.3468065,3.6746714,0.9131581,0.9928846,0.6750487,3.4685764,6.433328,4.1740627,4.2192254,6.4132504,5.998631,6.510157,3.9621017,6.017107,4.499083,0.7790445,4.961645,3.720339,0.50612795,4.0872617,6.29492,5.617026,4.0916467,6.0081882,4.200694,3.6808534,0.8138795,5.575452,1.4597005,4.797979,3.1571193,0.8839767,5.4360266,3.7055979,4.2221956,6.008835,6.6018605,1.3546036,2.8229227,3.6525939,3.9293697,3.8145845,6.029241,4.4083953,2.701502,5.0345216,0.9737638,4.9962773,0.5669358,6.256547,5.718006,5.6699686,4.868014,5.976898,0.39007008,2.9663224,3.9800427,6.2792754,0.56606275,5.734677,3.2247486,5.8332148,6.3882594,5.1879554,4.490272,6.631586,5.7899857],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"hovertemplate\":\"label=Cat\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Cat\",\"marker\":{\"color\":\"#EF553B\",\"opacity\":0.7,\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"Cat\",\"showlegend\":true,\"x\":[-5.053914,-4.1924887,-4.144672,-5.085069,-4.8372555,-2.2907465,-5.293836,-4.0055947,-4.766608,-2.013208,-3.9936278,-5.2097416,-5.4139104,-5.791187,-5.3124647,-6.150733,-3.2575233,-4.708004,-5.2208385,-7.02423,-4.5873933,-4.8509665,-4.549844,-3.7008333,-2.5538695,-5.174922,-6.556973,-5.287911,-4.803562,-3.6537488,-4.8209157,-4.8059535,-2.0852785,-5.081088,-5.5824113,-7.158357,-3.4613135,-4.642931,-3.8218138,-4.7486525,-2.1603484,-2.65854,-3.7579405,-5.155369,-7.1114063,-3.8328087,-5.1519933,-5.5787573,-5.2686305,-4.140438,-2.5361953,-3.8707337,-5.3253875,-4.660884,-4.5786905,-2.1705904,-4.235619,-4.5614758,-5.5228558,-5.008288,-5.7351975,-4.499389,-4.68788,-5.8428926,-3.7891395,-2.9150033,-7.167877,-4.793401,-4.877652,-2.7336307,-6.819147,-4.887045,-5.168584,-5.2176604,-5.1375713,-4.88876,-2.507762,-5.5817213,-2.6520674,-3.6770582,-4.5658474,-6.919939,-5.0574417,-5.2363987,-2.0389543,-4.10917,-4.81834,-3.8846529,-4.733402,-4.210498,-4.1077495,-5.686602,-2.7530189,-4.569791,-5.9997215,-6.083587,-2.1497815,-4.4944444,-5.1624856,-6.1011386,-4.856674,-4.0693784,-5.116525,-4.989719,-4.480902,-1.9864148,-4.952461,-6.7527723,-5.2554903,-4.7535834,-3.9728727,-5.5093136,-4.3724537,-4.720222,-3.9241416,-4.7724714,-6.562945,-3.7525065,-5.5412254,-4.58727,-4.9150357,-7.17789,-3.7690408,-5.648027,-4.6643276,-5.1495943,-3.4438634,-4.5552945,-3.4355166,-5.179761,-3.49542,-2.13776,-2.8699498,-1.9695377,-4.9924417,-5.559659,-4.965447,-2.9344153,-3.8435,-5.454635,-2.9079447,-3.8094294,-5.0720487,-3.3406658,-2.8315227,-3.0013595,-5.455775,-4.834991,-4.0499487,-4.669195,-6.4715786,-3.3414972,-6.0289226,-2.4159384,-4.830134,-7.1414986,-4.4530272,-5.352101,-6.586973,-5.133542,-3.768645,-5.4703116,-5.3309135,-5.358112,-3.7590048,-3.9581301,-4.1620474,-5.97199,-3.4521616,-3.4193826,-2.8250973,-5.7111278,-2.3148293,-3.7389028,-5.1090074,-3.076484,-3.514095,-4.494913,-5.2009025,-3.9941964,-6.179933,-4.7668533,-4.299589,-4.807967,-6.9178166,-3.3723915,-7.1245365,-4.095907,-2.5499136,-2.3024228,-5.4015517,-3.6294234,-2.0864289,-4.837197,-6.561995,-4.213743,-4.3844566,-2.388564,-7.2140956,-4.8672132,-7.197654,-3.6898782,-4.5360513,-3.3528485,-4.412478,-2.9579797,-6.0768495,-5.331716,-5.389103,-3.738179,-2.8594654,-4.8840733,-4.6916933,-4.899075,-3.4067218,-4.5914664,-5.191979,-5.0509768,-4.47093,-4.9701557,-5.2804275,-4.6114464,-2.8283079,-3.8505156,-5.1261773,-3.9372835,-5.9652634,-1.9455886,-4.6895585,-4.0659423,-3.438381,-4.2860894,-4.6479034,-5.992489,-7.03303,-3.3938625,-6.5641503,-2.3901708,-5.4854927,-4.109262,-4.0631747,-3.3709233,-5.041257,-3.5419223,-4.2514725,-4.1429853,-5.065138,-2.2763681,-4.58281,-3.2996335,-6.098203,-4.171518,-4.240367,-6.9164996,9.207386,-5.560549,-5.116316,-2.1360784,-4.4938827,-2.2246404,-5.77231,-4.2895412,-5.939042,-5.4485216,-3.7190866,-4.111212,-6.8787303,-2.05021,-5.031305,-7.1721725,-3.4519095,-4.8150086,-4.8830256,-4.952226,-4.5112243,-4.7147813,-5.116929,-5.1359396,-4.8451886,-5.281805,-2.1404831,-5.145079,-4.6616764,-6.576326,-4.1407123,-3.1185808,-2.5162678,-4.5920563,-4.734323,-3.0118272,-3.3608575,-1.9267548,-4.6265798,-4.935486,-5.1250095,-6.813049,-2.962687,-4.908879,-5.056822,-4.041423,-3.8166406,-5.2733717,-5.192428,-4.653741,-2.1094542,-5.5803394,-2.8210852,-3.0144475,-5.4485745,-4.7756505,-5.442919,-4.6598363,-3.8421354,-4.838211,-5.6058183,-3.3186147,-4.3648357,-5.2138214,-2.1805947,-3.6828692,-6.951763,-6.1618533,-5.7025366,-4.1200004,-2.0771527,-4.384564,-7.1933594,-3.8945243,-4.5179687,-5.9632273,-4.6721163,-3.4435153,-4.938084,-3.4648278,-2.16678,-5.852656,-3.4844494,-5.307058,-4.11045,-4.851524,-3.8230941,-4.7050996,-4.7653913,9.259333,-3.849123,-2.934916,-2.0672722,-5.4111233,-1.9919099,-5.5320926,-4.5523276,-2.9159043,-5.1494145,-3.704827,-5.0406547,-3.1625156,-5.9388366,-4.213865,-4.869543,-6.404316,-5.353313,-5.171242,-2.1511056,-3.4784307,-6.8598514,-4.5543575,-3.346866,-4.0816474,-4.294011,-3.7357137,-5.4281225,-4.921579,-4.7379065,-4.514414,-3.8653467,-4.9817343,-4.9789834,-7.1345806,-4.968684,-5.470714,-5.8404875,-4.858757,-5.0593014,-2.0480866,-3.2936482,-5.24852,-4.6392093,-6.8858895,-5.3201723,-5.2419796,-5.05403,-3.772141,-6.602943,-5.4371285,-5.4302354,-3.8463006,-2.9245245,-4.6588664,-7.0617404,-3.2959769,-2.2627914,-7.1531386,-4.300827,-4.1148033,-2.1132915,-5.481365,-3.6193168,-4.70904,-4.463232,-5.074549,-4.5755324,-2.8429406,-3.4594486,-4.9604473,-5.4061427,-5.065278,-5.483192,-4.7717133,-5.447867,-3.5589068,-4.0585036,-3.986796,-5.3222356,-1.9486798,-2.9072711,10.106429,-2.7718127,-4.8485537,-4.968813,-7.049262,-4.6926184,-6.4039083,-5.1665325,-4.490509,-4.376172,-5.471708,-4.662961,-4.358453,-4.50886,-4.9608274,-2.0745864,-5.37819,-2.052734,-1.9155064,-2.2107346,-5.285465,-5.2459626,-3.3265035,-4.6751456,-2.9411469,-5.2543564,-3.3025732,-5.1064563,-7.1189084,-5.526781,-5.266297,10.743787,-6.8962326,-3.8552284,-2.0618894,-3.809106,-4.7690105,-3.3109975,-3.08436,-2.175583,-3.8949046,-3.9811668,-4.967936,-5.4705834,-5.2154465,-2.2516472,-4.725231,-3.0324607,-2.105562,-7.2104645,-4.5951633,-4.979026,-5.2917156,-5.197921,-6.0054326,-2.8470533,-1.9274163,-3.382076,-3.4300444,-5.5983887,-7.0659366,-4.3724866,-7.156827,-5.175152,-3.6661994,-5.729163,-7.119421,-2.9213142,-4.2363925,-2.9148161,-5.1435266,-4.606672,-4.518677,-4.8452945,-3.904777,-3.7305236,-5.3119755,-4.4299526,-2.8499317,-4.2055225,-2.9821649,-3.6742973,-4.6565366,-2.2271445,-4.30291,-2.342464,-2.1597912,-4.7845325,-3.840859,-5.0023656,-1.9663023,-3.7220936,-5.018252,-3.069791,-3.9134872,-3.5327497,-5.2381783,-3.7598681,-3.0918486,-6.6551285,-5.605321,-2.7908733,-4.0082307,-1.9896837,-2.9995952,-2.4531605,-3.8013785,-4.2696667,-4.860376,-4.8124495,-2.3448107,-3.5669096,-3.8901212,-2.9817734,-4.282314,-4.198357,-4.9878616,-5.400451,-3.1171844,-2.253055,-5.3594303,-4.8174267,-5.1496487,-5.3374867,-6.7180314,-7.231693,-2.0818892,-3.2907262,-2.804,-2.646504,-3.1235113,-5.028167,-5.1243196,-5.091433,-4.8495126,-4.2600846,-4.8318033,-5.2285995,-3.4336414,-2.3873706,-5.3631625,-5.2174554,-1.8789064,-3.4983132,-5.772631,-3.749053,-5.0688167,-5.604239,-4.5369124,-4.3282204,-4.6885667,-3.491529,-3.7751215,-2.3520155,-5.1635733,-4.406145,-3.5390785,-4.4648967,-2.907683,-1.9809625,-3.0063148,-5.0273156,-5.0932684,-5.017824,-6.924754,-6.7311726,-4.9259005,-4.7706842,-4.804886,-3.690343,-7.123041,-5.224298,-4.555662,-7.137721,-3.4852667,-4.2471323,-2.8461392,-4.818497,-4.7330947,-5.027431,-5.0855722,-5.6396513,-3.301876,-6.592112,-5.3095226,-4.3329015,-5.2628837,-3.433593,-6.888017,-5.4746814,-3.909079,-4.0898557,-5.0232744,-3.2903953,-5.1596327,-5.5732284,-2.6691566,-3.919534,-7.2402964,-4.2386694,-5.250412,-3.044049,-4.5958905,-3.1611216,-5.307423,-2.3642123,-2.9553015,-3.564931,-2.914877,-5.042352,-2.9951355,-4.6778274,-5.789931,-2.9396667,-2.6000323,-1.9246336,-5.241275,-3.3326945,-6.6047378,-3.9498453,-5.4144697,-4.642809,-4.7345834,-3.5520377,-5.271882,-4.3414407,-1.9912186,-5.3752117,-4.7901483,-7.11157,-4.1193304,-4.81538,-4.1585546,-6.4037504,-4.759167,-5.0167484,-5.0258207,-7.0487714,-5.6428475,-2.9622242,-5.4707885,-3.2904048,-5.28873,-2.9631314,-3.9911592,-4.619969,-2.905251,-5.202717,-4.4619026,-5.003396,-4.911906,-5.500468,-2.8219876,-5.443494,-4.877965,-4.7459583,-2.3672068,-3.8375938,-4.814993,-5.0148983,-4.9672847,-6.707262,-1.8246243,-2.9578059,-4.4088726,-2.870592,-3.6666877,-5.094666,-1.8460886,-7.108424,-5.0409646,-2.9540594,-5.1461177,-5.7486653,-2.299557,-7.171321,-4.4820213,-2.9487529,-6.0286894,-3.7272909,-4.3959556,-5.0437202,-4.3413258,-4.994219,-5.6203756,-4.2948027,-4.717457,-2.0163064,-3.3553727,-3.0404222,-3.7877102,-4.613442,-4.688072,-4.8487196,-2.8990047,-4.8071294,-1.8741151,-2.1712458,-3.8304195,-4.2134776,-3.9491065,-4.073475,-4.4818435,-5.188024,-4.7234435,-5.437122,-5.41318,-3.4272492,-4.8902516,-5.7217436,-6.096211,-4.397371,-3.2560773,-5.6216087,-5.0416613,-2.2213948,-5.039775,-4.4869256,-4.986737,-6.6048417,-2.8769717,-6.8657885,-7.169944,-6.160949,-7.15162,-4.1292706,-5.271038,-5.4126153,-3.2873592,-4.1159463,-4.5793834,-2.0822108,-2.954137,-4.4065948,-6.0610013,-3.2322795,-4.811325,-4.6487417,-4.8175063,-2.9934764,-3.6380548,-2.1022646,-5.441799,-4.003236,-3.0281463,-4.2154145,-4.8968115,-2.0251029,-4.7557993,-4.7153544,-3.667,-3.261741,-4.460775,-4.0209093,-4.886779,-2.2106967,-5.1345863,-5.021194,-3.0755734,-2.6254919,-5.8945174,-6.0888467,-4.7691226,-5.1191983,-4.6673217,-7.112642,-4.6543417,-4.757805,-4.4159226,-4.743061,-5.550283,-3.3718102,-1.848728,-3.7998176,-2.2367282,-2.759317,-3.8491478,-3.2418094,-6.53384,-5.2989645,-7.1013975,-6.19704,-2.5110652,-6.9642196,-5.419202,-3.7849298,-3.3131773,-3.3148677,-2.9459732,-3.5846045,-4.4711447,-5.0027065,-3.8465686,-5.481936,-4.845418,-3.7416234,-5.097657,-2.153242,-5.97975,-4.0748453,-5.4049826,-5.2832513,-3.3830054,-5.5194755,-5.2987027,-6.7123485,-5.1651607,-4.9293795,-4.836056,-4.9654937,-4.0963826,-3.9059045,-4.070033,-5.347436,-7.086124,-5.13992,-2.3399982,-3.0312188,-5.0258265,-4.0094037,-4.9597077,-5.7167025,-2.2600894,-6.1327286,-3.2959824,-6.464718,-3.7017798,-5.823563,-5.3876076,-5.2770042,-3.5293646,-3.1071732,-4.9797034,-4.554066,-5.4001436,-5.0019917,-5.1851497,-3.9200451,-4.039367,-5.4073124,-2.9133878,-4.588117,-6.7711377,-4.921787,-3.6088665,-2.2014854,-4.5762577,-3.2758894,-4.97255,-5.1969457,-4.710509,-5.1077414,-4.232396,-4.7791286,-4.7105083,-4.9363036,-5.484357,-4.723095,-4.7096553,-2.2371895,-5.35231,-3.7852328,-5.084049,-5.172151,-1.8983334,-6.8261285,-3.0814667,-3.7998877,-4.29061,-2.8357065,-5.0905285,-5.6398196,-7.1189437,-6.19763,-4.862465,-5.547053,-4.9214478,-4.5826616,-5.085321,-5.711994,-5.0829606,-6.668454,-4.612203,-7.166298,-4.847879,-3.52871,-2.868072,-3.1843119,-2.9409916,-2.8700736,-3.2362156,-4.114885,-2.014723,-5.6971416,-4.74352,-4.467494,-3.0094428,-6.787655,-3.445356,-4.8604016,-2.122169,-4.3171635,-2.4088964,-3.6544733,-4.521852,-5.6268907,-4.863056,-4.6769714,-5.176842,-3.7398236,-5.4711976,-6.9408083,-4.3074613,-5.7812123,-3.8599184,-3.5634987,-2.0822878,-3.8135448,-6.955888,-7.0512905,-5.49112,-3.9789667,-3.36694,-2.1546185,10.278226,-2.9435227,-6.884811,-2.8385983,-5.802241,-4.3955355,-4.47522,-7.118641,-4.0874066,-3.9467556,-4.8361983,-4.6667886,-5.38576,-5.0824623,-6.8043585,-2.051513,-3.3486662,-4.9977107,-4.388171,-2.375441,-2.3955662,-1.8632765,-6.8491187,-2.3457162,-5.5703173,-4.6613297,-5.211184,-2.7740574,-5.5733585,-5.3664923,-3.6501427,-4.816731,-4.776686,-3.3825085,-2.3153288,-3.8887253,-2.9097784,-2.164401,-4.1364293,-3.4423559,-6.8482394,-5.736516,-3.4072666,-2.3111734,-5.270714,-5.1612287,-1.8450477,-4.632154,-5.606758,-3.0874798,-4.381575,-2.9427547,-4.7208133,-6.9535007,-3.8056073,-5.0482016,-2.1388583,-4.638201,-4.4575043,-5.353742,-4.515148,-4.4776435,-1.8820543,-2.1845438,-4.3972836,-3.517448,-5.307758,-5.342336,-4.59366,-2.9375348,-2.9534583,-6.0166783,-4.368851,-6.7216263,-4.210165,-5.2721257,-2.7727568,-5.028821,-2.9517717,-3.9778738,-3.525257,-5.2229147,-2.1738253,-6.745749,-5.3110914,-3.4080937,-5.3028007,-4.830204,-5.0602717,-5.0214753,-3.2743208,-3.6548357,-4.64051,-5.338813,-4.905456,-3.532445,-3.8041115,-5.5365024,-4.7409124,-4.885217,-3.8081148,-4.9944515,-4.0051303,-4.8867755,-6.11498,-4.573523,-4.331296,-5.116671,-4.124329,-3.351314,-4.554051,-2.0758288,-5.623191,-5.1724124,-5.5719113,-4.989977,-4.488472,-5.113444,-2.2585042,-4.919069,-5.705534,-5.2014313,-2.0672522,-5.771225,11.063501,-4.7395287,-4.246136,-4.7668595,-5.6915936,-5.1849747,-5.3591633,-4.972049,-5.006384,-5.792134,-5.4854918,-4.46385,-5.673847,-5.0659714,-3.6427484,-5.834341,-3.4312637,-6.1358447,-5.341789,-4.796631,-4.888164,-3.0246396,-4.9980273,-4.433964,-5.9966855,-4.4873896,-4.51983,-5.0395274,-4.8263826,-3.8219197,-5.243021,-5.231044,-3.303008,-6.6497135,-3.4749465,-3.0055733,-3.9984066,-2.1198194,-3.5251997,-2.29944,-2.8442328,-5.0963573,-5.236983,-1.9746789,-5.9777412,-4.744801,-5.2951174,-5.5708623,-3.689614,-6.163301,-2.0567212,-4.5540786,-3.75168,-4.380724,-5.3683534,-5.4958005,-5.39568,-4.37497,-2.287415,-7.0791116,-6.7385354,-5.0598073,-4.9279466,-2.8711178,-3.2614315,-5.327008,-3.6706524,-2.0420558,-4.033343,-3.8441901,-4.6472826,-4.846583,-6.8816557,-2.9256546,-4.2867575,-4.1206403,-5.0844646,9.490174,-2.9334483,-4.472997,-4.324301,-5.4606104,-4.3745575,-4.9051423,-3.6797388,-6.665468,-4.5143495,-4.9094024,-4.1695676,-3.2909734,-3.3996062,-4.9034185,-4.767629,-4.9043317,-5.4729843,-5.353899,-4.6665244,-4.700514,-6.5363994,-4.176635,-6.1041436,-5.4083266,-3.823942,-5.844212,-5.269785,-3.0571127,-4.832287,-5.266621,-5.061006,-2.6256196,-5.1625047,-2.7616363,-4.9070363,-4.6241803,-5.2934823,-4.87263,-3.6845014,-3.7123263,-1.9936548,-1.8642519,-2.8946404,-4.111304,-3.2193115,-3.3662868,-4.803943,-2.7668054,-5.556092,-5.2941175,-4.6328335,-5.3412414,-3.485358,-6.7461,-7.0336905,-6.6234245,-4.997228,-4.685267,-4.6304655,-2.1654086,-3.6259859,-4.973661,-7.0853868,-4.744456,-4.5493665,-4.688148,-5.2035303,-5.1814065,-4.437947,-6.5463967,-1.8500296,-3.132291,-2.917432,-4.001071,10.103984,-4.5719795,-4.752639,-6.1125007,-2.299153,-5.4812865,-3.8020291,-4.9742737,-3.513843,-2.5523195,-5.096271,-5.12419,-6.7813005,-4.80036,-4.339786,-2.1466908,-4.877535,-5.440643,-5.2432194,-5.1481633,-5.483142,-4.418505,-7.114199,-2.0815961,-5.5039024,-5.3835373,-5.448531,-5.467347],\"xaxis\":\"x\",\"y\":[-5.066168,-7.425559,-6.6255465,-7.8242793,-8.16233,-6.490408,-6.4570866,-6.595018,-6.418013,-5.9304695,-7.9001,-5.048302,-6.3975163,-6.6912026,-6.752311,-7.6127152,-8.4448,-5.1560535,-5.6755805,-5.9385705,-7.863943,-8.521423,-8.508487,-6.79375,-5.7266707,-7.3271017,-5.990974,-6.407369,-7.369916,-6.0051374,-8.662318,-5.191776,-6.0030704,-6.4264,-5.735995,-6.0318675,-5.9035215,-7.8342533,-6.817587,-6.29471,-5.867705,-5.9183106,-6.95247,-7.4252815,-5.911206,-6.771054,-5.239729,-7.2346444,-7.9519315,-8.280638,-5.9210653,-5.39373,-5.832534,-8.335884,-8.566958,-6.5355396,-8.159057,-8.667018,-5.3950653,-8.473643,-6.3513613,-5.631017,-8.159734,-6.53014,-5.29406,-3.2465315,-5.9754324,-5.014604,-8.740041,-5.7940526,-6.432628,-5.382669,-5.550618,-8.618619,-8.783278,-7.7665544,-6.0464687,-7.213651,-6.705096,-7.105963,-5.8908315,-6.073586,-6.131818,-7.191489,-5.6810436,-8.2460575,-8.3598995,-6.6749563,-6.434452,-5.5479302,-5.8594084,-6.639215,-5.6749606,-8.346477,-6.4980264,-7.4913673,-6.1030354,-8.776841,-7.425872,-7.63399,-7.461474,-7.3679743,-7.292222,-5.4495807,-8.653001,-5.587485,-6.084408,-6.3998814,-5.0983963,-8.08444,-7.26213,-5.339013,-6.459863,-6.768207,-6.529585,-7.531525,-6.575602,-5.8579583,-5.6302457,-5.9902897,-5.6396627,-6.0421305,-8.168792,-7.3957872,-8.428258,-6.388748,-8.399598,-7.583755,-6.8311143,-6.88953,-5.951239,-5.764901,-7.821544,-5.9583397,-5.9502616,-7.2606883,-8.442847,-3.2458127,-5.240564,-5.5688043,-7.949627,-5.268923,-5.7560153,-4.1293006,-7.779401,-7.554031,-7.920561,-4.990627,-7.2441854,-6.20003,-6.2151866,-4.522905,-7.675459,-5.954427,-7.8126326,-6.0548286,-8.223839,-8.8377905,-6.1359835,-7.042765,-5.2970157,-6.8255525,-5.9863496,-5.705029,-6.308915,-7.273063,-7.665226,-7.750009,-6.885686,-7.830871,-5.8156195,-5.957149,-6.6084986,-6.605832,-7.5111375,-8.177697,-6.2396293,-6.4008627,-5.9565187,-6.484411,-7.582532,-5.4380426,-5.9742284,-8.264878,-5.780749,-7.854243,-5.9001846,-6.1746726,-5.60841,-6.3070793,-7.707206,-5.8814797,-5.661279,-8.641584,-6.498945,-8.492876,-8.006104,-6.2700524,-6.1208067,-5.2883925,-6.1523013,-6.806813,-6.779132,-6.325495,-6.388258,-7.4493475,-7.4004097,-8.766605,-6.9188566,-7.10642,-7.5186863,-8.516081,-8.103692,-5.9477572,-5.8762083,-8.2992115,-5.78432,-6.5818686,-8.725007,-8.199369,-8.002093,-8.561903,-7.769822,-7.2987075,-5.9817643,-6.881795,-7.2538557,-6.011815,-7.8018484,-6.359166,-5.818876,-6.764785,-5.0966544,-7.557842,-5.879297,-6.976059,-6.640647,-6.2720265,-5.6121984,-5.8871145,-7.696829,-6.717178,-7.7335668,-6.3662186,-6.903328,-6.0632405,-8.445227,-6.0313888,-6.782799,-4.159925,-6.956449,-7.281728,-7.2693434,-6.0534816,4.4851747,-5.438897,-5.6960564,-6.200799,-8.588841,-6.1919823,-6.3535705,-5.4741797,-6.088911,-7.329435,-6.958989,-6.665741,-6.300487,-5.8308773,-6.102365,-6.0886106,-8.454821,-8.862031,-7.1181574,-8.538803,-8.392936,-6.913692,-7.9872437,-5.123147,-7.731542,-8.078196,-6.137878,-8.110417,-7.116309,-6.640161,-8.294394,-7.4964476,-6.3511143,-5.6752234,-5.4356155,-7.762753,-5.904649,-5.681928,-7.7947807,-5.1618133,-7.939166,-5.8185935,-3.3022878,-8.730951,-5.69346,-7.3978686,-5.6499047,-5.5434833,-7.5624337,-7.741388,-6.0944567,-5.5857964,-7.6379213,-8.149885,-6.6433463,-7.594952,-6.9288516,-6.211694,-5.220525,-7.570503,-7.640339,-8.36423,-6.4554806,-8.826523,-5.9597425,-6.7137914,-6.2110233,-7.583806,-5.969168,-7.64267,-5.957403,-8.13553,-6.065791,-6.957213,-8.5710745,-6.4749975,-5.010981,-5.8430066,-6.705241,-5.8724904,-6.5333824,-7.0952506,-5.8783145,-5.9660373,-7.539347,-7.742649,-5.218941,-6.8106136,-8.47338,4.6228657,-6.4673996,-3.24575,-5.7930446,-6.4492674,-5.814279,-5.561278,-6.9455204,-3.236129,-8.420921,-6.3645177,-8.523854,-3.7431052,-5.969414,-7.0601373,-6.2739315,-5.917568,-7.800488,-7.6036506,-5.58264,-6.4127884,-5.827814,-5.721232,-4.489929,-5.397942,-6.690937,-8.216703,-7.921071,-7.280691,-6.937273,-8.456084,-7.198796,-8.45566,-6.082566,-6.1301546,-7.6584,-5.662743,-6.055401,-6.572588,-7.9581714,-5.648311,-4.417692,-6.7099934,-6.6097636,-6.1989045,-7.236967,-7.0401764,-8.484749,-6.8878455,-6.4773283,-8.64873,-6.74074,-6.697515,-3.2455366,-7.2099566,-6.2134676,-4.430101,-5.7650533,-5.9927893,-7.9497256,-8.249144,-6.5415835,-5.797143,-6.70555,-8.306709,-7.897375,-7.6831355,-6.9828687,-7.5878778,-7.8722267,-8.15409,-6.742861,-6.7594543,-6.4010916,-8.603371,-8.577955,-6.8314,-7.9247465,-8.21158,-5.554781,-5.8673353,-3.234348,4.867383,-6.2251725,-5.053698,-8.693987,-5.8765874,-8.548741,-6.0021963,-6.522701,-5.734047,-7.679368,-6.5157933,-5.4120564,-6.703815,-5.9409714,-7.538325,-5.8206105,-5.6345515,-6.0392394,-5.892944,-6.0068073,-6.1084294,-7.736085,-5.368193,-5.5416617,-8.193734,-5.6823325,-4.424792,-8.825358,-6.08635,-5.394324,-6.7325687,2.2651374,-6.2376986,-7.2869296,-6.1169143,-6.827306,-7.4974923,-4.254216,-8.177524,-5.904629,-6.3495502,-6.7558484,-8.784268,-8.254383,-8.576067,-6.1479506,-7.616477,-8.232126,-5.611589,-5.9794145,-7.9000874,-5.9390903,-8.104473,-7.253498,-7.2899575,-3.2964022,-5.763371,-5.9045224,-7.6977916,-7.2287292,-5.9487557,-8.582298,-6.1365604,-8.408206,-6.848082,-7.487536,-5.876396,-7.6195617,-7.331743,-3.1934109,-5.9952035,-5.864813,-6.152243,-7.5549283,-5.4500384,-6.6600595,-5.7055182,-8.613016,-8.089923,-7.5070286,-8.153284,-6.5620227,-7.648921,-6.6712728,-7.4737334,-5.8211884,-6.440298,-5.016522,-6.887964,-8.873304,-5.7801414,-6.8933377,-6.3821115,-8.355445,-7.004948,-6.843085,-5.0090194,-6.8249846,-7.4868665,-6.275387,-6.759409,-6.144381,-6.475617,-6.0149107,-3.245344,-5.7571654,-6.831372,-7.4159155,-7.6885777,-7.828933,-5.71723,-6.478705,-7.106987,-3.335182,-7.3520613,-7.2042294,-5.370852,-6.7329226,-6.528396,-5.4756637,-5.4703245,-5.4965096,-5.4690394,-7.789342,-6.17833,-5.906729,-6.2419987,-4.411939,-7.6570916,-6.908534,-8.133984,-8.936205,-6.5782304,-8.8325205,-8.534553,-8.526233,-5.211344,-6.680411,-6.498201,-6.5070214,-6.717185,-8.4697485,-5.726358,-7.90332,-5.8148594,-6.764992,-8.802315,-6.8078327,-5.7437716,-6.7047563,-8.430993,-5.9571958,-6.8622136,-5.9356813,-5.0412917,-7.6716337,-7.8806653,-8.580336,-8.024285,-5.876666,-8.135569,-8.701486,-8.944537,-6.525121,-6.29665,-6.259364,-6.0994477,-7.7212095,-6.257473,-6.7157097,-6.104137,-7.516546,-8.743677,-6.0590134,-5.947913,-8.402584,-7.5881615,-7.6070333,-8.091416,-5.9611425,-7.082229,-5.6603994,-4.444222,-6.6917253,-5.5526476,-5.9938164,-7.8737,-8.046197,-5.792896,-6.9735203,-5.637972,-7.5427337,-5.997451,-8.426149,-8.55033,-5.8220043,-3.3852224,-7.7521825,-6.144703,-6.3313823,-8.700842,-8.319778,-7.593462,-8.361453,-7.7961216,-6.2525864,-3.2769814,-6.3233175,-5.796375,-8.927513,-8.069564,-6.9843545,-6.3828983,-8.130188,-6.051611,-5.6030955,-7.2008057,-8.296729,-6.5352783,-7.578562,-6.9293933,-7.6547246,-8.525345,-7.8839865,-8.617115,-8.425436,-6.044547,-7.1005836,-8.821632,-6.0051274,-5.818046,-8.682809,-8.383404,-6.508443,-5.050487,-6.536924,-6.5266438,-6.140654,-6.1997848,-8.322906,-6.368209,-8.402136,-6.350686,-8.283539,-5.8976517,-7.7229204,-3.2328203,-5.040995,-6.51976,-5.798734,-8.474422,-6.610583,-8.016004,-5.628435,-8.283643,-8.189818,-6.707336,-8.055542,-7.0110054,-6.5536675,-5.835394,-6.4894257,-5.8145547,-8.1232,-8.757565,-3.2466903,-6.9933767,-8.614848,-5.6531563,-6.2054496,-6.620987,-3.2291281,-6.592209,-7.562599,-6.597224,-6.0351453,-8.080783,-7.4635925,-7.272666,-8.168482,-7.311416,-6.1609135,-6.52505,-7.8477616,-6.3507986,-6.424579,-8.363307,-5.643789,-5.8533664,-8.315337,-7.142466,-6.865294,-7.4304595,-7.8676867,-3.2487185,-6.7345443,-5.9664335,-6.1333237,-7.1281343,-6.7363443,-6.9360056,-6.6983466,-5.672065,-6.1516824,-7.4326143,-6.581747,-6.9964595,-6.7190466,-6.4682355,-6.1763883,-6.6764827,-8.39734,-7.338343,-7.1439247,-5.1887918,-5.5538626,-7.691674,-8.785293,-5.7383776,-6.3774104,-8.16735,-5.8079796,-6.2038994,-7.505379,-6.089798,-8.155926,-5.1102533,-5.999965,-4.398151,-8.068077,-8.748466,-6.0021305,-7.9952765,-7.3280053,-7.310005,-7.634991,-6.9164896,-5.938785,-5.226261,-3.311708,-6.7163253,-5.738933,-5.743286,-6.655239,-8.170456,-8.0142765,-7.659764,-5.5797844,-5.9792523,-8.45103,-6.864447,-8.322642,-6.756205,-7.769684,-8.194724,-6.6185646,-7.5210567,-5.8755603,-6.4896917,-5.8667774,-7.2181025,-7.338376,-8.795892,-7.9596367,-5.5700107,-5.9631767,-6.9360695,-6.716787,-8.441194,-8.762852,-5.438755,-7.845251,-5.6634197,-5.2526174,-6.691113,-6.0902696,-6.89431,-8.051049,-5.9361014,-7.399029,-6.0762296,-5.958999,-6.4005933,-5.890038,-7.8116117,-6.4549184,-4.5157356,-6.7920265,-7.8711376,-6.550838,-8.745517,-5.1526213,-7.2095823,-6.312655,-7.212231,-6.2961993,-6.1681485,-6.3299155,-7.384196,-6.5180964,-5.39064,-8.320809,-5.898197,-7.028263,-7.919134,-5.679445,-8.71102,-8.227789,-4.9892507,-8.376662,-8.347731,-5.698525,-7.5715823,-7.63589,-5.941868,-8.656977,-5.72049,-8.350748,-5.285695,-5.541163,-7.632122,-6.154769,-6.4642835,-7.468743,-4.440461,-6.559262,-8.510186,-5.733976,-5.603043,-8.450928,-6.9747005,-8.298615,-6.5876045,-7.5514874,-7.8565645,-5.9058814,-5.1719675,-7.2482843,-6.038353,-8.171317,-6.2253165,-8.21092,-6.198878,-5.1639533,-6.522096,-5.9416094,-8.500152,-4.06825,-8.092374,-8.265555,-5.6679654,-8.337638,-7.4781923,-5.616734,-7.728982,-5.407188,-6.23776,-8.188184,-8.658967,-6.5934396,-8.168264,-7.158208,-6.0445304,-5.1798224,-5.764777,-6.100091,-3.4329371,-8.419135,-6.532336,-7.648922,-5.504075,-6.8519025,-5.9851556,-7.5899677,-8.64299,-5.809286,-8.590355,-8.684547,-5.7573223,-6.819624,-7.8426275,-6.174863,-8.02491,-6.034856,-8.718183,-7.4997034,-7.383129,-8.246297,-3.2596433,-8.16562,-8.063232,-7.567087,-5.7176905,-6.484049,-6.075084,-6.382504,-8.231647,-6.414771,-5.850307,-5.721609,-5.8519397,-8.711745,-6.63653,-6.2795324,-8.56458,-6.519728,-5.471959,-8.832269,-5.6703815,-6.740706,-5.5703483,-5.8047876,-6.3927608,-6.4546003,-7.3963394,-6.821224,-5.6463866,-7.1474614,-6.197611,-5.8137565,-5.409336,-8.105519,-5.8864045,-6.0353065,2.895253,-7.6782584,-6.5103807,-3.292728,-5.5911803,-8.663851,-8.839398,-5.895112,-7.0121326,-7.051801,-8.686566,-6.90091,-6.7969656,-5.267151,-5.777652,-6.286455,-7.682383,-7.6517334,-8.713075,-5.813682,-5.8380184,-5.7761874,-6.3499866,-5.8553357,-7.558342,-6.546819,-5.635961,-6.770572,-7.1167693,-6.3133717,-5.555491,-6.2941456,-8.481578,-8.078834,-6.1804423,-5.235626,-3.2316885,-6.64503,-6.873795,-5.4553676,-6.1592884,-6.646264,-4.7778726,-5.84214,-6.6343627,-8.047709,-5.682549,-8.912646,-5.5327725,-8.2568865,-7.0472164,-3.45416,-5.566415,-5.9820476,-6.4534655,-7.7253275,-6.028323,-6.9746323,-6.5540805,-7.307879,-8.607672,-7.808096,-5.942679,-6.519929,-6.2370143,-6.7248607,-5.826634,-6.438718,-6.846832,-3.219199,-3.2250109,-7.2026734,-8.576988,-6.7244787,-7.4584346,-7.3031197,-6.270197,-6.654632,-8.148741,-7.6141543,-7.9082932,-7.0394335,-6.74582,-5.728171,-7.447803,-5.8163733,-8.793492,-7.0883346,-6.0412645,-7.902682,-3.9913816,-5.654491,-8.480729,-6.165622,-6.7316008,-6.351404,-6.4870987,-7.2729597,-7.5973806,-8.58172,-5.240732,-8.313145,-8.106296,-6.722443,-6.670643,-6.4957066,-7.6444945,-8.809104,-7.201212,-8.453014,-8.863108,-6.2320633,-5.782099,-6.7404313,-6.770347,-6.5645776,-8.805167,-8.523586,-6.5119586,-6.328665,-6.5212884,-7.3153424,-5.9606056,-6.813541,4.294516,-5.354516,-6.1847305,-7.2919164,-6.980779,-6.52818,-5.035593,-5.022353,-5.0571227,-6.0517983,-5.491948,-6.2818804,-6.765919,-8.310541,-5.3569183,-6.443593,-5.6792393,-7.6214004,-6.3961344,-7.604652,-8.207903,-8.3517685,-5.9346848,-6.3487854,-5.817286,-8.726482,-5.420907,-8.108689,-5.369376,-7.366472,-8.875411,-5.780589,-4.430051,-6.5641427,-5.9116154,-8.318376,-8.270847,-5.7040787,-6.2063217,-5.7703333,-7.5417986,-7.573941,-8.728048,-5.6377172,-6.6230497,-8.363069,-8.255974,-7.8692675,-6.9646053,-7.615427,-6.0088983,-8.496738,-5.218153,-8.430858,-6.7673936,-5.537317,-7.936495,-5.6039805,-5.720814,-6.161326,-6.4799175,-5.5239654,-6.176888,-3.242246,-8.385276,-6.258302,-6.672183,-6.151156,-7.3229995,-7.1127715,-8.686878,-8.75801,-6.1616936,-3.2367616,-6.482067,-8.307559,-7.584165,3.3441877,-3.2647767,-5.8299685,-6.3597174,-7.943858,-8.363673,-6.1128764,-6.848534,-6.4950824,-8.668204,-8.609547,-8.51001,-4.0133576,-5.902079,-5.281285,-7.3485622,-7.3734946,-6.4842324,-5.5642796,-7.6506896,-6.1851807,-6.847113,-7.4219184,-7.2915535,-5.549058,-8.554314,-6.6714463,-6.4091477,-8.328613,-7.9654493,-8.231269,-8.434385,-6.2846007,-5.7932506,-6.0862455,-8.80286,-5.541619,-6.3416004,-8.821262,-5.982616,-6.954521,-6.306274,-5.702675,-7.455701,-6.4399605,-4.3608427,-6.8875623,-7.3481703,-6.3671694,-5.430292,-7.2090425,-7.453818,-6.3073483,-7.6986866,-6.429943,-6.3971143,-6.210687,-5.753291,-9.006564,-8.533512,-6.353689,-6.5931044,-8.234611,-5.731911,-8.943701,-7.3843384,-8.411809,-6.2854667,-5.037596,-5.7859936,-6.4354076,-5.8657618,-8.180224,-7.762443,-6.9804344,5.27704,-6.104372,-7.852067,-7.568015,-5.8630095,-7.563479,-6.3284435,-6.1336584,-6.79055,-6.708508,-5.1592255,-5.403773,-6.365136,-8.341417,-7.4107,-5.6584496,-6.313336,-6.8986526,-8.283947,-8.640774,-5.5866075,-7.414299,-5.981658,-5.5144854,-8.202095,-6.8417964,-7.368914,-6.083288],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"label\"},\"tracegroupgap\":0},\"title\":{\"text\":\"UMAP Projection of Valid Features\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c7d6150f-bfa0-4ad1-8cf5-dcff56d77ce3');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Create a UMAP projection of the valid features\n",
    "# UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique\n",
    "# that is particularly well-suited for visualizing high-dimensional data.\n",
    "\n",
    "# Assuming valid_features is a list of tuples where each tuple contains (label, file_path, feature_vector)\n",
    "# Extract the feature vectors from valid_features and convert them to a numpy array\n",
    "feature_vectors = np.array([x[2] for x in valid_features])\n",
    "\n",
    "# Initialize the UMAP model with desired parameters\n",
    "# n_neighbors: The size of the local neighborhood (in terms of number of neighboring sample points) used for manifold approximation\n",
    "# min_dist: The minimum distance between points in the low-dimensional space\n",
    "# n_components: The number of dimensions of the low-dimensional space (2D in this case)\n",
    "# random_state: Seed for the random number generator to ensure reproducibility\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=271828)\n",
    "\n",
    "# Fit the UMAP model to the feature vectors and transform them into a 2D projection\n",
    "valid_features_umap = umap_model.fit_transform(feature_vectors)\n",
    "\n",
    "# valid_features_umap now contains the 2D coordinates of the valid features in the UMAP projection\n",
    "\n",
    "# Create a DataFrame from the UMAP projection\n",
    "# The DataFrame will have columns \"x\" and \"y\" for the 2D coordinates\n",
    "df = pd.DataFrame(valid_features_umap, columns=[\"x\", \"y\"])\n",
    "\n",
    "# Add the labels to the DataFrame\n",
    "# Extract the labels from valid_features and map them to \"Cat\" and \"Dog\"\n",
    "df[\"label\"] = [x[0] for x in valid_features]\n",
    "df[\"label\"] = df[\"label\"].map({0: \"Cat\", 1: \"Dog\"})\n",
    "\n",
    "# Plot the UMAP projection with uniform opacity\n",
    "# Use Plotly Express to create a scatter plot of the UMAP projection\n",
    "# Color the points by their labels (\"Cat\" or \"Dog\")\n",
    "fig = px.scatter(df, x=\"x\", y=\"y\", color=\"label\", title=\"UMAP Projection of Valid Features\", opacity=0.7)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Training\n",
    "\n",
    "Self-training is a **semi-supervised learning technique** that leverages both a small labeled dataset and a larger unlabeled dataset to improve model performance. This approach is particularly beneficial when labeled data is scarce and expensive to obtain. The core idea is to iteratively refine the model by generating pseudo-labels for the unlabeled data and incorporating these pseudo-labels into the training process.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/self_training.png\"  width=\"80%\" height=\"80%\" />\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "The self-training algorithm can be broken down into the following steps:\n",
    "\n",
    "1. **Initialize Model**: Train an initial model on the small labeled dataset.\n",
    "2. **Generate Pseudo-Labels**: Use the trained model to predict labels for the unlabeled dataset, creating pseudo-labels.\n",
    "3. **Combine Data**: Merge the original labeled data with the pseudo-labeled data.\n",
    "4. **Retrain Model**: Retrain the model on this combined dataset.\n",
    "5. **Repeat**: Iterate steps 2-4 until the model's performance converges or a predetermined number of iterations is reached.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "#### Initialize Model\n",
    "- **Objective**: Create a baseline model using the limited labeled data available.\n",
    "- **Approach**: Train the model using standard supervised learning techniques on this small dataset.\n",
    "\n",
    "#### Generate Pseudo-Labels\n",
    "- **Objective**: Utilize the trained model to predict labels for the unlabeled data.\n",
    "- **Approach**: The model assigns the most likely class label to each unlabeled instance, treating these predictions as pseudo-labels.\n",
    "\n",
    "#### Combine Data\n",
    "- **Objective**: Create an expanded training dataset by merging the original labeled data with the newly pseudo-labeled data.\n",
    "- **Approach**: The combined dataset now includes both the actual labeled examples and the pseudo-labeled examples, increasing the training data size.\n",
    "\n",
    "#### Retrain Model\n",
    "- **Objective**: Improve the model's performance by training it on the expanded dataset.\n",
    "- **Approach**: Retrain the model, allowing it to learn from both true labels and pseudo-labels, enhancing its generalization capabilities.\n",
    "\n",
    "#### Repeat\n",
    "- **Objective**: Refine the model iteratively for better accuracy.\n",
    "- **Approach**: Continue generating pseudo-labels and retraining the model until performance stabilizes or a set number of iterations is completed.\n",
    "\n",
    "### Pseudo-Labeling\n",
    "\n",
    "**Pseudo-labeling** is the process of assigning labels to unlabeled data based on predictions from a model trained on labeled data. These pseudo-labels are then treated as ground truth during subsequent training phases, effectively incorporating the unlabeled data into the training process.\n",
    "\n",
    "- **Purpose**: To utilize the vast amount of unlabeled data by turning it into a form that can assist in model training.\n",
    "- **Process**: \n",
    "  - Predict labels for the unlabeled data using the current model.\n",
    "  - Treat these predictions as if they were actual labels.\n",
    "  - Incorporate these pseudo-labeled data points into the training set.\n",
    "\n",
    "### Key Points to Consider\n",
    "\n",
    "- **Model Confidence**: Pseudo-labeling relies on the assumption that the model's predictions are reasonably accurate. Low-confidence predictions can introduce noise.\n",
    "- **Iteration Control**: Monitor performance metrics to decide when to stop iterations. Too many iterations can lead to overfitting.\n",
    "- **Data Balance**: Ensure that the pseudo-labeled data does not overwhelm the original labeled data, maintaining a balance to avoid biasing the model.\n",
    "\n",
    "### Implementation Example\n",
    "\n",
    "To illustrate the self-training process, consider the \"Dogs vs Cats\" image classification task. Suppose we have:\n",
    "- A small labeled dataset with 100 images from the `train` set.\n",
    "- A large unlabeled dataset with the rest of the `train` set.\n",
    "- A large unlabeled dataset.\n",
    "\n",
    "Let's try it now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-extracted features from disk\n",
    "# These features were previously saved using joblib\n",
    "train_features = joblib.load('outputs/cats-vs-dogs/train_features.joblib')\n",
    "valid_features = joblib.load('outputs/cats-vs-dogs/valid_features.joblib')\n",
    "# test_features = joblib.load('outputs/cats-vs-dogs/test_features.joblib')\n",
    "\n",
    "# Ensure train_features is a numpy array with a consistent shape\n",
    "# dtype=object is used because the array contains tuples of different types\n",
    "train_features = np.array(train_features, dtype=object)\n",
    "\n",
    "# Create a copy of train_features for self-learning\n",
    "self_learning_train_features = train_features.copy()\n",
    "\n",
    "# Randomly select 100 elements from self_learning_train_features\n",
    "# Set the labels of the other elements to -1 to indicate they are unlabeled\n",
    "indices = rng.choice(len(self_learning_train_features), 100, replace=False)\n",
    "self_learning_train_features[:, 0][~np.isin(np.arange(len(self_learning_train_features)), indices)] = -1\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from self_learning_train_features\n",
    "X_train = np.array([x for _, _, x in self_learning_train_features])\n",
    "y_train = np.array([y for y, _, _ in self_learning_train_features])\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from valid_features\n",
    "X_valid = np.array([x for _, _, x in valid_features])\n",
    "y_valid = np.array([y for y, _, _ in valid_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19947, 768), (19947,), (2492, 768), (2492,))"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    19847\n",
       " 1       54\n",
       " 0       46\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the y_train array into a pandas Series\n",
    "# This allows us to use pandas' built-in functions for data analysis\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "# Count the occurrences of each unique value in y_train_series\n",
    "# This provides a summary of the distribution of labels in the training data\n",
    "label_counts = y_train_series.value_counts()\n",
    "\n",
    "# Display the counts of each label\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1253\n",
       "1    1239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_valid).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      1253\n",
      "           1       0.98      1.00      0.99      1239\n",
      "\n",
      "    accuracy                           0.99      2492\n",
      "   macro avg       0.99      0.99      0.99      2492\n",
      "weighted avg       0.99      0.99      0.99      2492\n",
      "\n",
      "Accuracy: 0.9915730337078652\n",
      "Matthews Correlation Coefficient: 0.9832383379379628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import SelfTrainingClassifier # Sklearn provides a SelfTrainingClassifier class that can be used to use self-training with any classifier.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "\n",
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "\n",
    "# Create a self-training classifier\n",
    "self_training_rf = SelfTrainingClassifier(rf)\n",
    "\n",
    "# Fit the self-training classifier\n",
    "self_training_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_valid_pred = self_training_rf.predict(X_valid)\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Show Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show Matthews Correlation Coefficient\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termination condition met: max_iter\n"
     ]
    }
   ],
   "source": [
    "# Check if the termination condition of the self-training random forest model is met\n",
    "# The termination condition indicates whether the model has reached a stopping criterion\n",
    "# such as a maximum number of iterations or a convergence threshold.\n",
    "\n",
    "# Access the termination condition attribute of the self-training random forest model\n",
    "termination_condition_met = self_training_rf.termination_condition_\n",
    "\n",
    "# Print the status of the termination condition\n",
    "if termination_condition_met:\n",
    "    print(f\"Termination condition met: {self_training_rf.termination_condition_}\")\n",
    "else:\n",
    "    print(\"Termination condition not met\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    10000\n",
       " 0     9671\n",
       "-1      276\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the transduction labels from the self-training random forest model into a pandas Series\n",
    "# The transduction labels represent the predicted labels for the unlabeled data points\n",
    "transduction_labels_series = pd.Series(self_training_rf.transduction_)\n",
    "\n",
    "# Count the occurrences of each unique label in the transduction labels\n",
    "# This provides a summary of how many data points were assigned to each class\n",
    "label_counts = transduction_labels_series.value_counts()\n",
    "\n",
    "# Display the counts of each label\n",
    "label_counts\n",
    "\n",
    "# Note that our final model trained with much more (pseudo-labeled) data than our initially labeled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Propagation\n",
    "\n",
    "**Label propagation** is a semi-supervised learning technique that leverages both labeled and unlabeled data to assign labels to previously unclassified instances. Imagine a social network where some users have declared their political affiliation, while others haven't. Label propagation would attempt to predict the political leanings of these undeclared users based on their connections and the affiliations of their friends.\n",
    "\n",
    "This method relies on constructing a graph representation of the data, where nodes represent data points (e.g., users in the network) and edges represent relationships or similarities between them (e.g., friendships). The labeled data points act as \"anchors,\" and the algorithm propagates these labels through the graph, influencing the labels of connected nodes.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/label_propagation.png\"  width=\"80%\" height=\"80%\" />\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "### How Label Propagation Works\n",
    "\n",
    "1. **Graph Construction:** The algorithm begins by creating a graph where each data point, labeled or unlabeled, is represented by a node. The edges connecting these nodes are weighted based on the similarity between the data points. This similarity can be determined using various measures like Euclidean distance or kernel functions.\n",
    "\n",
    "2. **Label Initialization:** Initially, the labeled data points retain their known labels, while the unlabeled points are assigned a uniform distribution over the possible labels. For instance, in a binary classification problem, unlabeled points might start with a 50/50 probability for each class.\n",
    "\n",
    "3. **Label Propagation:** The algorithm iteratively updates the label probabilities of unlabeled nodes based on the labels of their neighbors.  This propagation can be envisioned as a \"diffusion\" process, where the label information flows from the labeled \"anchors\" to the unlabeled nodes through the edges of the graph. The strength of this influence is determined by the edge weights, with stronger connections carrying more weight.\n",
    "\n",
    "4. **Convergence:** The process continues until the label probabilities for the unlabeled nodes stabilize, meaning further iterations result in minimal changes. At this point, the algorithm assigns the label with the highest probability to each unlabeled node.\n",
    "\n",
    "### Model Features\n",
    "\n",
    "* **Label Clamping:**  During the propagation process, the algorithm can handle the initial labeled data in two primary ways:\n",
    "    * **Hard Clamping:**  The labels of the initially labeled data points remain fixed throughout the iterations, ensuring they don't change.\n",
    "    * **Soft Clamping:** Allows for some flexibility in the initial labels. This means the assigned labels can change slightly during each iteration, controlled by a parameter (alpha). This flexibility can be beneficial if there's a chance of noise or errors in the initial labeling.\n",
    "\n",
    "* **Kernel:** The choice of kernel function influences how the similarity between data points is measured, which in turn affects the edge weights in the graph.\n",
    "    * **RBF Kernel:**  Creates a dense matrix, potentially leading to higher computational costs, especially with large datasets.\n",
    "    * **KNN Kernel:** Constructs a sparse matrix by connecting each data point only to its 'k' nearest neighbors. This results in faster computation, particularly for large datasets. \n",
    "\n",
    "### Advantages and Limitations\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "* **Utilizes Unlabeled Data:** Label propagation effectively leverages the information present in unlabeled data, which is often abundant and cheaper to obtain than labeled data.\n",
    "* **Simple and Intuitive:** The core concept of propagating labels based on graph connectivity is relatively straightforward to grasp.\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "* **Computational Cost:** Constructing and manipulating the graph, especially with dense matrices, can be computationally expensive for large datasets.\n",
    "* **Sensitivity to Graph Structure:** The performance of label propagation heavily relies on the quality of the graph representation. Poorly constructed graphs with inaccurate similarity measures can lead to inaccurate label assignments. \n",
    "\n",
    "\n",
    "Let's try it now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-extracted features from disk using joblib\n",
    "# These features were previously saved and contain tuples of (label, file_path, feature_vector)\n",
    "train_features = joblib.load('outputs/cats-vs-dogs/train_features.joblib')\n",
    "valid_features = joblib.load('outputs/cats-vs-dogs/valid_features.joblib')\n",
    "\n",
    "# Ensure train_features is a numpy array with a consistent shape\n",
    "# dtype=object is used because the array contains tuples of different types\n",
    "train_features = np.array(train_features, dtype=object)\n",
    "\n",
    "# Create a copy of train_features for label propagation purposes\n",
    "label_propagation_train_features = train_features.copy()\n",
    "\n",
    "# Randomly select 100 elements from label_propagation_train_features to be labeled\n",
    "# Set the labels of the other elements to -1 to indicate they are unlabeled\n",
    "labeled_indices = rng.choice(len(label_propagation_train_features), 100, replace=False)\n",
    "label_propagation_train_features[:, 0][~np.isin(np.arange(len(label_propagation_train_features)), labeled_indices)] = -1\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from label_propagation_train_features\n",
    "X_train = np.array([x for _, _, x in label_propagation_train_features])\n",
    "y_train = np.array([y for y, _, _ in label_propagation_train_features])\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from valid_features\n",
    "X_valid = np.array([x for _, _, x in valid_features])\n",
    "y_valid = np.array([y for y, _, _ in valid_features])\n",
    "\n",
    "# Identify the indices of the unlabeled data points in the training set\n",
    "unlabeled_indices = np.where(y_train == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19947, 768), (19947,), (2492, 768), (2492,))"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19847"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1253\n",
      "           1       0.99      0.98      0.98      1239\n",
      "\n",
      "    accuracy                           0.98      2492\n",
      "   macro avg       0.98      0.98      0.98      2492\n",
      "weighted avg       0.98      0.98      0.98      2492\n",
      "\n",
      "Accuracy: 0.9839486356340289\n",
      "Matthews Correlation Coefficient: 0.9679553454117247\n"
     ]
    }
   ],
   "source": [
    "from sklearn.semi_supervised import LabelPropagation\n",
    "\n",
    "# Create a label propagation model\n",
    "label_propagation = LabelPropagation(kernel='knn', n_jobs=-1, max_iter=2000)\n",
    "\n",
    "# Fit the label propagation model   \n",
    "label_propagation.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training unlabeled data\n",
    "y_train_label_propagation = label_propagation.transduction_\n",
    "\n",
    "# Train a random forest classifier on the updated training data\n",
    "rf = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "rf.fit(X_train, y_train_label_propagation)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_valid_pred = rf.predict(X_valid)\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Show Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show Matthews Correlation Coefficient\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10435,  9512])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train_label_propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      1253\n",
      "           1       1.00      0.00      0.00      1239\n",
      "\n",
      "    accuracy                           0.50      2492\n",
      "   macro avg       0.75      0.50      0.34      2492\n",
      "weighted avg       0.75      0.50      0.34      2492\n",
      "\n",
      "Accuracy: 0.5040128410914928\n",
      "Matthews Correlation Coefficient: 0.034913071783961136\n"
     ]
    }
   ],
   "source": [
    "# Now the same thing, but with a different kernel\n",
    "\n",
    "# Create a label propagation model\n",
    "label_propagation = LabelPropagation(kernel='rbf', n_jobs=-1, max_iter=2000)\n",
    "\n",
    "# Fit the label propagation model   \n",
    "label_propagation.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training unlabeled data\n",
    "y_train_label_propagation = label_propagation.transduction_\n",
    "\n",
    "# Train a random forest classifier on the updated training data\n",
    "rf = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "rf.fit(X_train, y_train_label_propagation)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_valid_pred = rf.predict(X_valid)\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Show Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show Matthews Correlation Coefficient\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19802,   145])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train_label_propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96      1253\n",
      "           1       0.99      0.92      0.95      1239\n",
      "\n",
      "    accuracy                           0.96      2492\n",
      "   macro avg       0.96      0.95      0.95      2492\n",
      "weighted avg       0.96      0.96      0.95      2492\n",
      "\n",
      "Accuracy: 0.9550561797752809\n",
      "Matthews Correlation Coefficient: 0.9127863817792571\n"
     ]
    }
   ],
   "source": [
    "# Now the same thing, but with a slighly different approach called Label Spreading\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "\n",
    "# Create a label spreading model\n",
    "label_spreading = LabelSpreading(kernel='knn', n_jobs=-1, max_iter=2000)\n",
    "\n",
    "# Fit the label spreading model   \n",
    "label_spreading.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the training unlabeled data\n",
    "y_train_label_spreading = label_spreading.transduction_\n",
    "\n",
    "# Train a random forest classifier on the updated training data\n",
    "rf = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "rf.fit(X_train, y_train_label_spreading)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_valid_pred = rf.predict(X_valid)\n",
    "\n",
    "# Show the classification report\n",
    "print(classification_report(y_valid, y_valid_pred))\n",
    "\n",
    "# Show Accuracy\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show Matthews Correlation Coefficient\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Training\n",
    "\n",
    "**Co-training** is a technique in semi-supervised learning that leverages multiple models trained on different views of the data. This method uses the predictions of one model to improve the training of another, making it particularly effective when data can be naturally divided into distinct feature sets or views. Each view should capture different aspects of the underlying structure of the data.\n",
    "\n",
    "<br>\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/cotraining.png\"  width=\"80%\" height=\"80%\" />\n",
    "  <br>\n",
    "</p>\n",
    " <br>\n",
    " \n",
    "### Key Concepts\n",
    "\n",
    "- **Different Views of Data**: In co-training, the data is split into different subsets of features or representations, referred to as *views*. For example, in a dogs-cats dataset, consider an image embedding with 768 dimensions:\n",
    "  - One view could be 384 dimensions of the embedding.\n",
    "  - Another view could be the remaining 384 dimensions of the embedding.\n",
    "- **Model Training and Interaction**: Two models are trained separately on each view. These models then exchange predictions on the unlabeled data, allowing each model to learn from the other's predictions.\n",
    "\n",
    "### Traditional Co-Training Process\n",
    "\n",
    "1. **Initial Training**:\n",
    "   - The method starts by checking and validating the input data.\n",
    "   - It then randomly splits the features into two views\n",
    "   - Two base classifiers are initialized and trained on the labeled data, each using its respective view:\n",
    "\n",
    "2. **Co-training Loop**:\n",
    "   - The main loop runs for `n_iter` iterations or until there's no more unlabeled data.\n",
    "   - In each iteration:\n",
    "      - A pool of unlabeled data is randomly selected\n",
    "      - Both classifiers make predictions on this pool\n",
    "      - The most confident positive and negative predictions from each classifier are selected\n",
    "      - These selected examples are added to the labeled dataset\n",
    "      - The selected examples are removed from the unlabeled dataset\n",
    "      - Both classifiers are retrained on the updated labeled dataset\n",
    "\n",
    "3. **Final Prediction**:\n",
    "   - For making predictions, the classifier:\n",
    "      - Gets predictions from both views. \n",
    "      - Averages these predictions. \n",
    "      - Returns either the class label (predict) or the probability (predict_proba).\n",
    "\n",
    "The key idea behind co-training is that each view of the data can provide different, complementary information. By allowing each classifier to \"teach\" the other with its most confident predictions, the algorithm can leverage unlabeled data to improve overall performance. The random split of features into views adds an element of diversity, potentially capturing different aspects of the data in each view.\n",
    "\n",
    "### Advantages of Co-Training\n",
    "\n",
    "- **Improved Learning**: By leveraging multiple views, co-training can improve learning performance, especially when labeled data is scarce.\n",
    "- **Complementary Information**: Different views can provide complementary information, making the combined model more robust and accurate.\n",
    "\n",
    "### Multi-View Learning\n",
    "\n",
    "**Multi-view learning** generalizes the concept of co-training by combining multiple views of data. This approach offers more flexibility and can capture more complex data structures.\n",
    "\n",
    "#### Differences from Traditional Co-Training\n",
    "\n",
    "- **Flexibility**: Unlike traditional co-training, which typically involves two models, multi-view learning can integrate more than two views.\n",
    "- **Complex Structures**: It is capable of modeling more complex relationships and dependencies among the views.\n",
    "\n",
    "### Potential Questions and Misconceptions\n",
    "\n",
    "- **What if the views are not truly independent?**\n",
    "  - The effectiveness of co-training relies on the assumption that the views are conditionally independent given the class label. If this assumption is violated, the performance might degrade.\n",
    "- **Can co-training be applied to any type of data?**\n",
    "  - Co-training is particularly useful when data can be naturally split into distinct views. If such views are not available, other semi-supervised learning techniques might be more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-extracted features from disk using joblib\n",
    "# These features were previously saved and contain tuples of (label, file_path, feature_vector)\n",
    "train_features = joblib.load('outputs/cats-vs-dogs/train_features.joblib')\n",
    "valid_features = joblib.load('outputs/cats-vs-dogs/valid_features.joblib')\n",
    "\n",
    "# Ensure train_features is a numpy array with a consistent shape\n",
    "# dtype=object is used because the array contains tuples of different types\n",
    "train_features = np.array(train_features, dtype=object)\n",
    "\n",
    "# Create a copy of train_features for co-training purposes\n",
    "co_training_train_features = train_features.copy()\n",
    "\n",
    "# Randomly select 100 elements from co_training_train_features to be labeled\n",
    "# Set the labels of the other elements to -1 to indicate they are unlabeled\n",
    "labeled_indices = rng.choice(len(co_training_train_features), 100, replace=False)\n",
    "co_training_train_features[:, 0][~np.isin(np.arange(len(co_training_train_features)), labeled_indices)] = -1\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from co_training_train_features\n",
    "X_train = np.array([x for _, _, x in co_training_train_features])\n",
    "y_train = np.array([y for y, _, _ in co_training_train_features])\n",
    "\n",
    "# Extract feature vectors (X) and labels (y) from valid_features\n",
    "X_valid = np.array([x for _, _, x in valid_features])\n",
    "y_valid = np.array([y for y, _, _ in valid_features])\n",
    "\n",
    "# Identify the indices of the unlabeled data points in the training set\n",
    "unlabeled_indices = np.where(y_train == -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19947, 768), (19947,), (2492, 768), (2492,))"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 768), (100,), (19847, 768))"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the labeled and unlabeled training data\n",
    "# Labeled data has labels not equal to -1\n",
    "# Unlabeled data has labels equal to -1\n",
    "\n",
    "# Extract feature vectors and labels for labeled training data\n",
    "X_train_labeled = X_train[y_train != -1]\n",
    "y_train_labeled = y_train[y_train != -1]\n",
    "\n",
    "# Extract feature vectors for unlabeled training data\n",
    "X_train_unlabeled = X_train[y_train == -1]\n",
    "\n",
    "# Display the shapes of the labeled and unlabeled datasets\n",
    "# This helps in understanding the distribution of labeled and unlabeled data\n",
    "X_train_labeled.shape, y_train_labeled.shape, X_train_unlabeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, matthews_corrcoef\n",
    "from helpers.semisupervised import MultiViewCoTrainingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9923756019261637 - 2 views\n",
      "Matthews Correlation Coefficient: 0.9848052156854211 - 2 views\n",
      "Accuracy: 0.992776886035313 - 4 views\n",
      "Matthews Correlation Coefficient: 0.985616441444488 - 4 views\n",
      "Accuracy: 0.9915730337078652 - 6 views\n",
      "Matthews Correlation Coefficient: 0.9832383379379628 - 6 views\n",
      "Accuracy: 0.9891653290529695 - 8 views\n",
      "Matthews Correlation Coefficient: 0.9784989168942184 - 8 views\n"
     ]
    }
   ],
   "source": [
    "for n_views in [2, 4, 6, 8]:\n",
    "\n",
    "    # Initialize and train the co-training classifier\n",
    "    co_clf = MultiViewCoTrainingClassifier(n_views=n_views, n_iter=20)\n",
    "    co_clf.fit(X_train_labeled, y_train_labeled, X_train_unlabeled)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_valid_pred = co_clf.predict(X_valid)\n",
    "\n",
    "    # Show Accuracy\n",
    "    print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)} - {n_views} views\")\n",
    "\n",
    "    # Show Matthews Correlation Coefficient\n",
    "    print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)} - {n_views} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we can see, semi-supervised learning techniques like self-training and label propagation offer powerful tools for leveraging unlabeled data to improve model performance. By incorporating the information present in the unlabeled data, these methods can enhance the generalization capabilities of models trained on limited labeled data.\n",
    ">\n",
    "> Understanding the underlying principles and implementation details of these techniques is essential for effectively applying them to real-world problems.\n",
    "> \n",
    "> You must be aware of the assumptions and limitations of these methods to ensure their successful application in practice. Always experiment with different hyperparameters, monitor performance metrics, and validate the results to determine the effectiveness of semi-supervised learning techniques in your specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PU Learning\n",
    "\n",
    "PU learning, or Positive-Unlabeled learning, is a specialized form of semi-supervised learning designed for scenarios where we only have access to **positive (P)** and **unlabeled (U)** data, lacking explicitly labeled negative instances. This approach proves particularly valuable in data-centric AI applications where:\n",
    "\n",
    "- **Labeled data is scarce or expensive:** Obtaining labeled data can be resource-intensive, making PU learning an attractive alternative.\n",
    "- **Positive instances are easily identifiable:** In some domains, identifying positive cases is straightforward, while labeling negatives might be ambiguous or costly.\n",
    "- **Unlabeled data is abundant:** PU learning leverages the readily available unlabeled data to enhance model performance.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/pulearn.webp\"  width=\"80%\" height=\"80%\" />\n",
    "  <br>\n",
    "</p>\n",
    "\n",
    "PU learning offers a powerful framework for leveraging partially labeled datasets in data-centric AI applications. By transforming the problem and relying on carefully considered assumptions, we can train effective models even when traditional fully-labeled datasets are unavailable or impractical to obtain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Concepts and Notation\n",
    "\n",
    "Let's define our dataset as $(x, y^*, \\tilde{y})$, where:\n",
    "\n",
    "* $x$ represents the input features.\n",
    "* $y^*$ represents the true target variable (unobserved), which we aim to predict.\n",
    "* $\\tilde{y}$ represents the observed label (positive or unlabeled).\n",
    "\n",
    "In PU learning, our goal is to estimate the probability of an instance being positive given its features:\n",
    "\n",
    "$$f(x) = P(y^* = 1 | x)$$\n",
    "\n",
    "However, due to the absence of labeled negative instances, we can only directly estimate:\n",
    "\n",
    "$$\\tilde{f}(x) = P(\\tilde{y} = 1 | x)$$\n",
    "\n",
    "where $\\tilde{f}(x)$ represents our model's prediction for the positive class based on the available data, and $\\tilde{y}$ denotes the observed label (positive or unlabeled).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Assumptions\n",
    "\n",
    "PU learning relies on several crucial assumptions to function effectively:\n",
    "\n",
    "1. **Positive Label Reliability:** All instances labeled as positive are indeed positive. This assumption implies:\n",
    "\n",
    "   $$P(\\tilde{y} = 1 | x, y^* = 0) = 0$$\n",
    "\n",
    "2. **Unlabeled Data Composition:** Unlabeled instances can belong to either the positive or negative class.\n",
    "\n",
    "3. **Label Flipping Independence:** The probability of a positive instance being mislabeled as unlabeled is independent of its features. This assumption, while strong, is necessary for tractability and can be expressed as:\n",
    "\n",
    "   $$P(\\tilde{y} = 0 | x, y^* = 1) = P(\\tilde{y} = 0 | y^* = 1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The PU Learning Transformation\n",
    "\n",
    "The fundamental idea behind PU learning is to transform the problem from predicting the unknown true target variable $y^*$ to predicting the observed positive class label $\\tilde{y} = 1$. This transformation allows us to effectively use the information contained within the unlabeled data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Key Lemma\n",
    "\n",
    "This transformation hinges on a key lemma that connects the true positive probability to our observable probabilities:\n",
    "\n",
    "$$P(y^* = 1 | x) = \\frac{P(\\tilde{y} = 1 | x)}{c}$$\n",
    "\n",
    "where $c = P(\\tilde{y} = 1 | y^* = 1)$ represents the **class prior** or **label frequency**, indicating the probability of a positive instance being observed as positive in our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof Sketch\n",
    "\n",
    "1. We start by marginalizing the joint probability:\n",
    "\n",
    "   $$P(\\tilde{y} = 1 | x) = P(y^* = 1, \\tilde{y} = 1 | x) + P(y^* = 0, \\tilde{y} = 1 | x)$$\n",
    "\n",
    "2. Applying the positive label reliability assumption ($P(\\tilde{y} = 1 | x, y^* = 0) = 0$), the second term vanishes:\n",
    "\n",
    "   $$P(\\tilde{y} = 1 | x) = P(y^* = 1, \\tilde{y} = 1 | x)$$\n",
    "\n",
    "3. Using the definition of conditional probability, we can rewrite this as:\n",
    "\n",
    "   $$P(\\tilde{y} = 1 | x) = P(y^* = 1 | x) * P(\\tilde{y} = 1 | y^* = 1, x)$$\n",
    "\n",
    "4. Applying the label flipping independence assumption:\n",
    "\n",
    "   $$P(\\tilde{y} = 1 | x) = P(y^* = 1 | x) * P(\\tilde{y} = 1 | y^* = 1)$$\n",
    "\n",
    "5. Rearranging the terms leads us to the key lemma:\n",
    "\n",
    "   $$P(y^* = 1 | x) = \\frac{P(\\tilde{y} = 1 | x)}{P(\\tilde{y} = 1 | y^* = 1)} = \\frac{P(\\tilde{y} = 1 | x)}{c}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the Class Prior (c)\n",
    "\n",
    "To bridge the gap between our model's predictions $\\tilde{f}(x)$ and the true positive probabilities $f(x)$, we need to estimate the class prior $c$. There are several approaches to estimating the class prior:\n",
    "\n",
    "1. **Proportion-based Estimation:**\n",
    "   This simple method involves training a classifier on the combined set of positive and unlabeled instances and estimating $c$ as the proportion of positive predictions among the unlabeled instances:\n",
    "\n",
    "   $$c \\approx \\frac{1}{|P|} \\sum_{x \\in U} P(\\tilde{y} = 1 | x)$$\n",
    "\n",
    "   where $|P|$ represents the number of positive instances in the dataset, and the summation iterates over all instances in the unlabeled set $U$.\n",
    "\n",
    "2. **Spy Technique:**\n",
    "   This method involves \"spying\" on the unlabeled data by mixing a small subset of positive examples with the unlabeled set. By observing how these \"spy\" instances are classified, we can better estimate the class prior.\n",
    "\n",
    "3. **Expectation-Maximization (EM) Algorithm:**\n",
    "   The EM algorithm can be adapted for PU learning to iteratively estimate the class prior and refine the classifier. This approach alternates between:\n",
    "   - E-step: Estimating the probability of each unlabeled instance being positive.\n",
    "   - M-step: Updating the classifier parameters and the class prior estimate.\n",
    "\n",
    "4. **Kernel Mean Matching (KMM):**\n",
    "   This non-parametric method estimates the class prior by matching the means of the positive and unlabeled data distributions in a high-dimensional feature space.\n",
    "\n",
    "The choice of method depends on factors such as dataset size, computational resources, and the specific characteristics of the problem at hand. We'll stick to the proportion-based estimation for simplicity in this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Application in Binary Classification\n",
    "\n",
    "PU learning finds frequent application in binary classification tasks, where the target variable $y$ takes on values of 0 or 1. In this context:\n",
    "\n",
    "* Positive instances: $y = 1$ (known)\n",
    "* Unlabeled instances: $y \\in \\{0, 1\\}$ (unknown)\n",
    "\n",
    "Our dataset can be represented as $(x, \\tilde{y}) \\in \\{0, 1\\}$, where $\\tilde{y}$ represents the observed labels (1 for positive, 0 for unlabeled).\n",
    "\n",
    "\n",
    "#### PU Learning Workflow\n",
    "\n",
    "The typical workflow for PU learning in binary classification involves the following steps:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Separate the dataset into positive instances ($P$) and unlabeled instances ($U$).\n",
    "   - Combine the positive and unlabeled instances into a single dataset $(X, \\tilde{y})$.\n",
    "\n",
    "2. **Class Prior Estimation:**\n",
    "    - Estimate the class prior $c$ using one of the methods discussed earlier.\n",
    "\n",
    "3. **Model Training:**\n",
    "    - Train a classifier on the combined positive and unlabeled dataset $(X, \\tilde{y})$.\n",
    "    - Use the estimated class prior $c$ to adjust the model's predictions.\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "    - Evaluate the model's performance on a separate test set or through cross-validation.\n",
    "    - Assess the model's ability to generalize to new data and make accurate predictions.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2200, 768), (2200,), (2492, 768), (2492,))"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-extracted features from disk using joblib\n",
    "# These features were previously saved and contain tuples of (label, file_path, feature_vector)\n",
    "train_features = joblib.load('outputs/cats-vs-dogs/train_features.joblib')\n",
    "valid_features = joblib.load('outputs/cats-vs-dogs/valid_features.joblib')\n",
    "\n",
    "# Ensure train_features is a numpy array with a consistent shape\n",
    "# dtype=object is used because the array contains tuples of different types\n",
    "train_features = np.array(train_features, dtype=object)\n",
    "\n",
    "# Create a copy of train_features for PU learning purposes\n",
    "pu_learn_train_features = train_features.copy()\n",
    "\n",
    "# Identify indices of positive samples (label == 1)\n",
    "positive_indices = np.where(pu_learn_train_features[:, 0] == 1)[0]\n",
    "\n",
    "# Randomly select 200 positive samples\n",
    "positive_indices_sample = rng.choice(positive_indices, 200, replace=False)\n",
    "\n",
    "# Randomly select 2000 samples to be treated as unlabeled\n",
    "unlabeled_indices = rng.choice(len(pu_learn_train_features), 2000, replace=False)\n",
    "\n",
    "# Combine the selected positive samples and unlabeled samples\n",
    "indices_to_keep = np.concatenate([positive_indices_sample, unlabeled_indices])\n",
    "\n",
    "# Extract feature vectors and labels for the selected positive samples\n",
    "X_train_positive = np.array([x for _, _, x in pu_learn_train_features[positive_indices_sample]])\n",
    "y_train_positive = np.array([y for y, _, _ in pu_learn_train_features[positive_indices_sample]])\n",
    "\n",
    "# Extract feature vectors for the selected unlabeled samples\n",
    "# Unlabeled samples are assigned a label of 0\n",
    "X_train_unlabeled = np.array([x for _, _, x in pu_learn_train_features[unlabeled_indices]])\n",
    "y_train_unlabeled = np.zeros(len(unlabeled_indices))\n",
    "\n",
    "# Combine the positive and unlabeled samples to form the training set\n",
    "X_train = np.concatenate([X_train_positive, X_train_unlabeled])\n",
    "y_train = np.concatenate([y_train_positive, y_train_unlabeled]).astype(int)\n",
    "\n",
    "# Extract feature vectors and labels for the entire training set\n",
    "X_train_full = np.array([x for _, _, x in pu_learn_train_features])\n",
    "y_train_full = np.array([y for y, _, _ in pu_learn_train_features]).astype(int)\n",
    "\n",
    "# Extract feature vectors and labels for the validation set\n",
    "X_valid = np.array([x for _, _, x in valid_features])\n",
    "y_valid = np.array([y for y, _, _ in valid_features]).astype(int)\n",
    "\n",
    "# Display the shapes of the training and validation datasets\n",
    "# This helps in understanding the distribution of data\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAT       1.00      0.99      0.99      1253\n",
      "         DOG       0.99      1.00      0.99      1239\n",
      "\n",
      "    accuracy                           0.99      2492\n",
      "   macro avg       0.99      0.99      0.99      2492\n",
      "weighted avg       0.99      0.99      0.99      2492\n",
      "\n",
      "Accuracy: 0.9939807383627608\n",
      "Matthews Correlation Coefficient: 0.9880001945828281\n"
     ]
    }
   ],
   "source": [
    "# Establish our baseline - if the model was trained on a full supervised dataset\n",
    "# Note: This is for demonstration purposes; in practice, you wouldn't train on the full dataset for baseline comparison.\n",
    "\n",
    "# Import necessary libraries from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the RandomForestClassifier model with specified parameters\n",
    "# random_state=271828: Seed for random number generator to ensure reproducibility\n",
    "# n_jobs=-1: Use all available CPU cores for parallel processing\n",
    "model = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "\n",
    "# Train the model on the full supervised dataset\n",
    "model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Predict the target values for the validation set using the trained model\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Print the classification report to evaluate the model's performance\n",
    "# target_names=['CAT', 'DOG']: Specify the names of the target classes\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=['CAT', 'DOG']))\n",
    "\n",
    "# Show the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show the Matthews Correlation Coefficient (MCC) of the model\n",
    "# MCC is a balanced measure that can be used even if the classes are of very different sizes\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to PU Learning\n",
    "\n",
    "#### Naive Approach\n",
    "\n",
    "A straightforward approach to binary classification is to treat the unlabeled data as samples of the negative class. However, this approach has limited performance when dealing with imbalanced datasets, i.e., datasets with a large number of unknown positive samples. It relies on the assumption that the proportion of positive samples in the unlabeled data is small enough not to significantly impact the model’s performance. Despite being simple to implement, the naive approach often results in suboptimal performance in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def fit_positive_unlabeled_estimator(X: np.ndarray, y: np.ndarray, hold_out_ratio: float, estimator: LogisticRegression) -> Tuple[LogisticRegression, float, float]:\n",
    "    \"\"\"Fits a positive-unlabeled (PU) estimator and estimates P(s=1|y=1).\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Labels array.\n",
    "        hold_out_ratio (float): Ratio of positive samples to hold out for estimating P(s=1|y=1).\n",
    "        estimator (LogisticRegression): The logistic regression estimator.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[LogisticRegression, float, float]: The fitted estimator, the estimated P(s=1|y=1), and the estimated c (the proportion of positive samples in the unlabeled set).\n",
    "    \"\"\"\n",
    "    assert isinstance(y, np.ndarray), \"Must pass np.ndarray rather than list as y\"\n",
    "    \n",
    "    # Find indices of positive/labeled elements\n",
    "    positive_indices = np.where(y == 1.0)[0]\n",
    "    \n",
    "    # Calculate the number of positive samples to hold out\n",
    "    hold_out_size = int(np.ceil(len(positive_indices) * hold_out_ratio))\n",
    "    \n",
    "    # Shuffle the positive indices to ensure randomness\n",
    "    np.random.shuffle(positive_indices)\n",
    "    \n",
    "    # Select hold-out indices and corresponding samples\n",
    "    hold_out_indices = positive_indices[:hold_out_size]\n",
    "    X_hold_out = X[hold_out_indices]\n",
    "    \n",
    "    # Remove hold-out samples from X and y to create the training set\n",
    "    X = np.delete(X, hold_out_indices, axis=0)\n",
    "    y = np.delete(y, hold_out_indices)\n",
    "    \n",
    "    # Fit the estimator on the remaining samples\n",
    "    estimator.fit(X, y)\n",
    "    \n",
    "    # Predict probabilities for the hold-out set\n",
    "    hold_out_predictions = estimator.predict_proba(X_hold_out)[:, 1]\n",
    "    \n",
    "    # Estimate P(s=1|y=1) as the mean probability of the hold-out set. P(s=1|y=1) is the probability of being labeled given a positive sample.\n",
    "    prob_s1_given_y1 = np.mean(hold_out_predictions)\n",
    "\n",
    "    # Estimate c as the mean probability of the entire training set\n",
    "    c = np.mean(estimator.predict_proba(X)[:, 1])\n",
    "\n",
    "    # Adjust c to account for the estimated P(s=1|y=1)\n",
    "    c = c / prob_s1_given_y1\n",
    "    \n",
    "    return estimator, prob_s1_given_y1, c\n",
    "\n",
    "def predict_positive_unlabeled_prob(X: np.ndarray, estimator: LogisticRegression, prob_s1_given_y1: float) -> np.ndarray:\n",
    "    \"\"\"Predicts the probability of being labeled for positive-unlabeled data.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        estimator (LogisticRegression): The fitted logistic regression estimator.\n",
    "        prob_s1_given_y1 (float): The estimated P(s=1|y=1), that is, the probability of being labeled given a positive sample.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted probabilities.\n",
    "    \"\"\"\n",
    "    # Predict probabilities for the input data\n",
    "    predicted_probabilities = estimator.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Adjust the predicted probabilities by dividing by P(s=1|y=1)\n",
    "    return predicted_probabilities / prob_s1_given_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2000,  200])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Iteration::0/100 => P(s=1|y=1)=0.1 c=0.73\n",
      "Learning Iteration::10/100 => P(s=1|y=1)=0.11 c=0.65\n",
      "Learning Iteration::20/100 => P(s=1|y=1)=0.12 c=0.59\n",
      "Learning Iteration::30/100 => P(s=1|y=1)=0.11 c=0.66\n",
      "Learning Iteration::40/100 => P(s=1|y=1)=0.09 c=0.81\n",
      "Learning Iteration::50/100 => P(s=1|y=1)=0.12 c=0.61\n",
      "Learning Iteration::60/100 => P(s=1|y=1)=0.12 c=0.57\n",
      "Learning Iteration::70/100 => P(s=1|y=1)=0.12 c=0.59\n",
      "Learning Iteration::80/100 => P(s=1|y=1)=0.12 c=0.58\n",
      "Learning Iteration::90/100 => P(s=1|y=1)=0.12 c=0.59\n"
     ]
    }
   ],
   "source": [
    "# Initialize predictions array to accumulate predicted probabilities\n",
    "predicted_probabilities = np.zeros(len(X_train))\n",
    "\n",
    "# Initialize list to store c values from each iteration\n",
    "c_values = []\n",
    "\n",
    "# Number of learning iterations\n",
    "num_iterations = 100\n",
    "\n",
    "# Perform learning iterations\n",
    "for iteration in range(num_iterations):\n",
    "    # Fit the PU estimator and get the estimated probabilities and c value\n",
    "    # hold_out_ratio=0.25: 25% of positive samples are held out for estimating P(s=1|y=1)\n",
    "    pu_estimator, prob_s1_given_y1, c = fit_positive_unlabeled_estimator(\n",
    "        X_train, y_train, 0.25, LogisticRegression(max_iter=5000, n_jobs=-1)\n",
    "    )\n",
    "    \n",
    "    # Store the c value\n",
    "    c_values.append(c)\n",
    "    \n",
    "    # Predict probabilities for the current iteration\n",
    "    predicted_probabilities_unscaled = predict_positive_unlabeled_prob(\n",
    "        X_train, pu_estimator, prob_s1_given_y1\n",
    "    )\n",
    "    \n",
    "    # Accumulate the predicted probabilities\n",
    "    predicted_probabilities += predicted_probabilities_unscaled\n",
    "    \n",
    "    # Print progress every 10 iterations\n",
    "    if iteration % 10 == 0:\n",
    "        print(f'Learning Iteration::{iteration}/{num_iterations} => P(s=1|y=1)={round(prob_s1_given_y1, 2)} c={round(c, 2)}')\n",
    "\n",
    "# Normalize the accumulated probabilities by the number of iterations\n",
    "predicted_probabilities /= num_iterations\n",
    "\n",
    "# Scale the normalized probabilities to the range [0, 1]\n",
    "predicted_probabilities_scaled = MinMaxScaler().fit_transform(\n",
    "    predicted_probabilities.reshape(-1, 1)\n",
    ").flatten()\n",
    "\n",
    "# Calculate the mean c value from all iterations\n",
    "mean_c = np.mean(c_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated positive class prior probability (c) is: 0.6053\n",
      "The estimated probability of a truly positive sample being labeled is: 0.0848\n"
     ]
    }
   ],
   "source": [
    "print(f\"The estimated positive class prior probability (c) is: {mean_c:.4f}\")\n",
    "print(f\"The estimated probability of a truly positive sample being labeled is: {predicted_probabilities_scaled.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The real positive class prior probability (c) is: 0.5001\n",
      "The real probability of a truly positive sample being labeled is: 0.0909\n"
     ]
    }
   ],
   "source": [
    "# We would not have this data in real life, but..... the real probabilities are\n",
    "\n",
    "# Calculate the proportion of positive samples in the training data\n",
    "# [1] selects the count of positive samples \n",
    "real_c = (np.bincount(y_train_full) / len(y_train_full))[1]\n",
    "\n",
    "# Calculate the probability that a truly positive sample is labeled as positive\n",
    "# The ratio gives the probability of a positive sample being labeled\n",
    "real_p = len(y_train_positive) / len(y_train)\n",
    "\n",
    "# Print the calculated probabilities\n",
    "# real_c is the proportion of positive samples in the training data\n",
    "# real_p is the probability that a truly positive sample is labeled as positive\n",
    "print(f\"The real positive class prior probability (c) is: {real_c:.4f}\")\n",
    "print(f'The real probability of a truly positive sample being labeled is: {real_p:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predicted probabilities to binary labels\n",
    "# If the predicted probability is greater than 0.5, classify as positive (1, 'DOG')\n",
    "# Otherwise, classify as negative (0, 'CAT')\n",
    "predicted_labels = predicted_probabilities > 0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the estimated labels to train another classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAT       0.87      1.00      0.93      1253\n",
      "         DOG       1.00      0.85      0.92      1239\n",
      "\n",
      "    accuracy                           0.92      2492\n",
      "   macro avg       0.93      0.92      0.92      2492\n",
      "weighted avg       0.93      0.92      0.92      2492\n",
      "\n",
      "Accuracy: 0.9229534510433387\n",
      "Matthews Correlation Coefficient: 0.8560109094778148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "# random_state=271828: Seed for random number generator to ensure reproducibility\n",
    "# n_jobs=-1: Use all available CPU cores for parallel processing\n",
    "classifier = RandomForestClassifier(random_state=271828, n_jobs=-1)\n",
    "\n",
    "# Fit the classifier on the combined dataset\n",
    "# X_train: Feature matrix containing both positive and unlabeled samples\n",
    "# predicted_labels: Labels predicted in the previous step\n",
    "classifier.fit(X_train, predicted_labels)\n",
    "\n",
    "# Predict labels for the validation set using the trained classifier\n",
    "y_valid_pred = classifier.predict(X_valid)\n",
    "\n",
    "# Print the classification report to evaluate the model's performance\n",
    "# target_names=['CAT', 'DOG']: Specify the names of the target classes\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=['CAT', 'DOG']))\n",
    "\n",
    "# Show the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show the Matthews Correlation Coefficient (MCC) of the model\n",
    "# MCC is a balanced measure that can be used even if the classes are of very different sizes\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elkan and Noto’s Approach\n",
    "\n",
    "A more sophisticated method for positive-unlabeled classification is [Elkan and Noto’s (E&N) approach](https://dl.acm.org/doi/10.1145/1401890.1401920). This method typically involves:\n",
    "\n",
    "1. **Training a Classifier:** Predict the probability that a sample is labeled.\n",
    "2. **Estimating Probabilities:** Use the model to estimate the probability that a positive sample is labeled.\n",
    "3. **Adjusting Probabilities:** Divide the probability that an unlabeled sample is labeled by the probability that a positive sample is labeled to get the actual probability that the sample is positive.\n",
    "\n",
    "While the E&N approach is effective in practice, it requires a significant amount of labeled data to accurately estimate the likelihood of a sample being positive, which may be impractical in real-world scenarios where labeled data is scarce.\n",
    "\n",
    "> **Note:** The E&N approach is advantageous because it directly handles the labeling issue, but its reliance on sufficient labeled data can be a limitation in certain applications.\n",
    "\n",
    "### Positive-Unlabeled Learning with [pulearn](https://pulearn.github.io/pulearn/doc/pulearn/)\n",
    " \n",
    "The pulearn Python package provides a collection of scikit-learn wrappers for various Positive-Unlabeled (PU) learning methods. PU learning is a type of semi-supervised learning where the training data consists of positive samples and unlabeled samples, with the latter potentially containing both positive and negative samples.\n",
    "\n",
    "This library expects the input data to be in the form of a feature matrix X and a target vector y, where the target vector `y contains the labels for the positive samples (1) and unlabeled samples (-1)`. The pulearn library provides a range of PU learning algorithms, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of -1 (unlabeled): 2000\n",
      "Count of 1 (positive): 200\n"
     ]
    }
   ],
   "source": [
    "# Convert the combined labels to a new format\n",
    "# If the label is 0 (unlabeled), convert it to -1\n",
    "# If the label is 1 (positive), keep it as 1\n",
    "y_train_formatted = np.array([-1 if x == 0 else 1 for x in y_train])\n",
    "\n",
    "# Count the occurrences of each label (-1 and 1) in the formatted labels\n",
    "# np.bincount counts the number of occurrences of each value in the array\n",
    "# Since np.bincount expects non-negative integers, it will not work directly with -1\n",
    "# To handle this, we can use a workaround by adding 1 to each element before counting\n",
    "# This shifts the range to non-negative integers\n",
    "counts = np.bincount(y_train_formatted + 1)\n",
    "\n",
    "# Print the counts of -1 and 1\n",
    "# counts[0] corresponds to the count of -1 (originally 0 in the shifted range)\n",
    "# counts[2] corresponds to the count of 1 (originally 2 in the shifted range)\n",
    "print(f\"Count of -1 (unlabeled): {counts[0]}\")\n",
    "print(f\"Count of 1 (positive): {counts[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CAT       0.94      0.96      0.95      1253\n",
      "         DOG       0.96      0.94      0.95      1239\n",
      "\n",
      "    accuracy                           0.95      2492\n",
      "   macro avg       0.95      0.95      0.95      2492\n",
      "weighted avg       0.95      0.95      0.95      2492\n",
      "\n",
      "Accuracy: 0.9514446227929374\n",
      "Matthews Correlation Coefficient: 0.9030864730146777\n"
     ]
    }
   ],
   "source": [
    "from pulearn import ElkanotoPuClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "# n_jobs=-1: Use all available CPU cores for parallel processing\n",
    "# random_state=271828: Seed for random number generator to ensure reproducibility\n",
    "model = RandomForestClassifier(n_jobs=-1, random_state=271828)\n",
    "\n",
    "# Initialize the ElkanotoPuClassifier with the RandomForestClassifier as the base estimator\n",
    "# hold_out_ratio=0.30: Ratio of positive samples to hold out for estimating P(s=1|y=1)\n",
    "pu_estimator = ElkanotoPuClassifier(estimator=model, hold_out_ratio=0.30)\n",
    "\n",
    "# Fit the PU classifier on the combined dataset\n",
    "# X_train: Feature matrix containing both positive and unlabeled samples\n",
    "# y_train_formatted: Labels formatted to -1 for unlabeled and 1 for positive samples\n",
    "pu_estimator.fit(X_train, y_train_formatted)\n",
    "\n",
    "# Predict labels for the validation set using the trained PU classifier\n",
    "y_valid_pred = pu_estimator.predict(X_valid)\n",
    "\n",
    "# Print the classification report to evaluate the model's performance\n",
    "# target_names=['CAT', 'DOG']: Specify the names of the target classes\n",
    "print(classification_report(y_valid, y_valid_pred, target_names=['CAT', 'DOG']))\n",
    "\n",
    "# Show the accuracy of the model\n",
    "print(f\"Accuracy: {accuracy_score(y_valid, y_valid_pred)}\")\n",
    "\n",
    "# Show the Matthews Correlation Coefficient (MCC) of the model\n",
    "# MCC is a balanced measure that can be used even if the classes are of very different sizes\n",
    "print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(y_valid, y_valid_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the primary motivation for using semi-supervised learning?\n",
    "\n",
    "2. Describe the self-training algorithm in semi-supervised learning.\n",
    "\n",
    "3. What are the key assumptions in semi-supervised learning?\n",
    "\n",
    "4. Explain the concept of label propagation in semi-supervised learning.\n",
    "\n",
    "5. How does co-training leverage multiple views of the data?\n",
    "\n",
    "6. What is the core idea behind PU learning?\n",
    "\n",
    "7. What are the key assumptions of PU learning?\n",
    "\n",
    "8. Describe Elkan and Noto’s approach to PU learning.\n",
    "\n",
    "9. What is the purpose of estimating the class prior in PU learning?\n",
    "\n",
    "10. How does the naive approach to PU learning treat the unlabeled data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell.`\n",
    "\n",
    "<!-- 1. The primary motivation for using semi-supervised learning is to leverage the abundant unlabeled data available in many practical scenarios to improve model performance when labeled data is limited or expensive to obtain.\n",
    "\n",
    "2. The self-training algorithm involves initializing a model with a small labeled dataset, generating pseudo-labels for the unlabeled data using the model, combining the labeled and pseudo-labeled data, retraining the model, and repeating the process until the model's performance converges.\n",
    "\n",
    "3. The key assumptions in semi-supervised learning include the cluster assumption (homogeneity), the continuity assumption (smoothness), and the manifold assumption. These assumptions suggest that similar data points are likely to have the same label and that the data lies on a lower-dimensional manifold.\n",
    "\n",
    "4. Label propagation constructs a graph where nodes represent data points and edges represent similarities. It spreads labels from known instances to unlabeled ones through the graph, iteratively updating label probabilities until they stabilize.\n",
    "\n",
    "5. Co-training leverages multiple views of the data by training separate models on each view and allowing them to teach each other through their most confident predictions. This method is effective when data can be naturally divided into distinct feature sets or views.\n",
    "\n",
    "6. The core idea behind PU learning is to transform the problem from predicting the unknown true target variable to predicting the observed positive class label, effectively using the information contained within the unlabeled data.\n",
    "\n",
    "7. The key assumptions of PU learning are positive label reliability (all instances labeled as positive are indeed positive), unlabeled data composition (unlabeled instances can belong to either the positive or negative class), and label flipping independence (the probability of a positive instance being mislabeled as unlabeled is independent of its features).\n",
    "\n",
    "8. Elkan and Noto’s approach to PU learning involves training a classifier to predict the probability that a sample is labeled, estimating the probability that a positive sample is labeled, and adjusting the probabilities to get the actual probability that the sample is positive.\n",
    "\n",
    "9. Estimating the class prior in PU learning is crucial for adjusting the model's predictions to reflect the true positive probabilities. It helps bridge the gap between the observed data (positive and unlabeled) and the true underlying distribution.\n",
    "\n",
    "10. The naive approach to PU learning treats the unlabeled data as samples of the negative class, assuming that the proportion of positive samples in the unlabeled data is small enough not to significantly impact the model’s performance. -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacentric_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
