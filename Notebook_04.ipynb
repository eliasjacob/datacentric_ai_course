{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Topics in Weak Supervision - Name Entity Recognition\n",
    "## Learning with Limited Labels: Weak Supervision and Uncertainty-Aware Training\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Keypoints\n",
    "- **Named Entity Recognition (NER)**: Task in NLP to identify and classify named entities in text into predefined categories, transforming unstructured text into structured data.\n",
    "\n",
    "- **Weak Supervision**: Techniques to efficiently annotate large datasets for NER tasks, reducing the reliance on labor-intensive manual labeling.\n",
    "\n",
    "- **Labeling Functions**: Use of multiple methods such as gazetteers, pre-trained transformer models, regex patterns, and zero-shot learning (GLiNER, NuNER, LangChain).\n",
    "\n",
    "- **Skweak Framework**: Framework for combining weak labels using generative models (e.g., Hidden Markov Models) and majority voting to enhance the quality of annotations.\n",
    "\n",
    "- **Document-Level Labeling**: Application of document-level labeling functions to ensure label consistency within documents, improving annotation accuracy.\n",
    "\n",
    "- **Transfer Learning**: Using pre-trained models like BERT, fine-tuning them on weakly labeled datasets for tasks like drug entity recognition in legal documents.\n",
    "\n",
    "- **Iterative Refinement**: Continuous improvement of labeling functions and model performance through iterative refinement.\n",
    "\n",
    "- **Time Efficiency**: Highlighting significant time savings achieved through weak supervision compared to traditional manual labeling efforts.\n",
    "\n",
    "## Takeaways\n",
    "- **Efficiency in Data Annotation**: Weak supervision techniques, as implemented in the Skweak framework, can drastically reduce the time and effort required for data annotation, especially for large-scale datasets.\n",
    "\n",
    "- **Enhanced Model Performance**: Combining multiple labeling functions and using generative models for label aggregation can significantly improve the quality of annotations, leading to better-performing NER models.\n",
    "\n",
    "- **Adaptability and Scalability**: The iterative refinement process allows NER models to continuously improve, making them adaptable to evolving data requirements and scalable across different domains.\n",
    "\n",
    "- **Cost-Effective Approach**: Weak supervision provides a cost-effective alternative to manual labeling, easing the development of robust NER models with minimal human intervention.\n",
    "\n",
    "- **Transfer Learning Benefits**: Leveraging pre-trained models, such as BERT, fine-tuned on weakly labeled data, can achieve performance close to models trained on fully annotated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction Named Entity Recognition\n",
    "\n",
    "\n",
    "## What is Named Entity Recognition (NER)?\n",
    "\n",
    "Named Entity Recognition (NER) is a task within the broader field of Information Extraction. It involves identifying and classifying named entities in a text into predefined categories. The primary goal of NER is to convert unstructured textual data into structured information that machines can easily read and analyze.\n",
    "\n",
    "Named Entity Recognition (NER) is a foundational technique in Natural Language Processing (NLP) that transforms unstructured text into structured data by identifying and classifying named entities. This process is vital for various applications, making it easier for machines to understand and use textual information effectively.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**Entities**: In NER, entities refer to specific pieces of information within a text that are of interest. Any word or phrase that represents something specific in the real world can be considered an entity.\n",
    "**Named Entities**: These are specific, identifiable entities mentioned in the text. They are categorized into various groups, such as:\n",
    "\n",
    "- **`PERSON`**: Names of individuals.\n",
    "- **`ORG`**: Names of organizations.\n",
    "- **`GPE`**: Geopolitical entities, including countries, cities, and states.\n",
    "- **`TIME`**: Time expressions, such as specific times of the day.\n",
    "- **`DATE`**: Date expressions, including specific dates and periods.\n",
    "- **...among others**: There are other categories, depending on the application and domain.\n",
    "\n",
    "### Objectives of NER\n",
    "\n",
    "The primary objective of NER is to extract structured information from unstructured text. This process involves:\n",
    "\n",
    "1. **Detection**: Identifying the spans of text that correspond to named entities.\n",
    "2. **Classification**: Assigning each detected entity to a predefined category.\n",
    "\n",
    "### Importance of NER\n",
    "\n",
    "NER is essential for various applications, including:\n",
    "\n",
    "- **Information Retrieval**: Enhancing search engines to retrieve more relevant results.\n",
    "- **Question Answering Systems**: Improving the accuracy of systems designed to answer user queries.\n",
    "- **Content Recommendation**: Personalizing content based on identified entities.\n",
    "- **Text Summarization**: Extracting key information to generate concise summaries.\n",
    "- **Graph Databases**: Populating knowledge graphs with structured information.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider the following sentence:\n",
    "\n",
    "> \"Elias Jacob is a professor at the Federal University of Rio Grande do Norte, located in Natal, Brazil.\"\n",
    "\n",
    "In this sentence, NER would identify and classify the named entities as:\n",
    "\n",
    "- **`PERSON`**: Elias Jacob\n",
    "- **`ORG`**: Federal University of Rio Grande do Norte\n",
    "- **`GPE`**: Natal, Brazil\n",
    "- **`PROFESSION`**: professor\n",
    "\n",
    "> Note: GPE stands for Geopolitical Entity, which includes countries, cities, and states.\n",
    "\n",
    "### Visual Representation\n",
    "\n",
    "To better understand how NER works, refer to the visual example below, which highlights named entities within a legal text:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/NER.png\" alt=\"NER Example\" style=\"width: 80%; height: 80%\"/>\n",
    "</p>\n",
    "\n",
    "### Potential Questions\n",
    "\n",
    "- **How does NER handle ambiguous cases?**\n",
    "    - NER systems often rely on context and sophisticated algorithms to disambiguate entities. For instance, \"Apple\" could refer to a fruit or a tech company, and the surrounding text helps determine the correct classification.\n",
    "\n",
    "- **What are the challenges in NER?**\n",
    "    - Some of the main challenges include handling ambiguous entities, dealing with different languages and dialects, and recognizing entities in noisy or informal text (e.g., social media posts).\n",
    "\n",
    "- **Can NER be customized for specific domains?**\n",
    "    - Yes, NER systems can be trained on domain-specific data to improve their accuracy for particular applications, such as medical texts or legal documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Meu nome é \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elias Jacob\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " e eu moro em \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Natal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rio Grande do Norte\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Eu trabalho no \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Instituto Metrópole Digital\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", que é a unidade mais bacana da \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UFRN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Desde 2021 eu também trabalho como \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Corregedor da UFRN\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       ". Quando a pandemia começou, no início de 2020, eu estava com as malas prontas para uma viagem de férias para o \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Japão\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Eu até fui buscar meu visto no \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Consulado\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " em \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Recife\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", mas, quando chegou mais perto da viagem, meus voos foram todos cancelados pela \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United Airlines\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " e eu não viajei. No dia 11 de novembro de 2023, eu estive no show do \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Roger Waters\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " em \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    São Paulo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". Sempre que visito a cidade, eu dou uma passada no \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Kidoairaku\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", meu restaurante japonês favorito lá.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the necessary libraries from the spaCy package\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the pre-trained spaCy model for Portuguese\n",
    "# 'pt_core_news_lg' is a large model with more accuracy and features\n",
    "# You can also use 'pt_core_news_sm' for a smaller, faster model with fewer features\n",
    "# To install the model, run: python -m spacy download pt_core_news_lg\n",
    "nlp = spacy.load('pt_core_news_lg')\n",
    "\n",
    "# Define a sample text in Portuguese for Named Entity Recognition (NER)\n",
    "text_example = (\n",
    "    \"Meu nome é Elias Jacob e eu moro em Natal, Rio Grande do Norte. \"\n",
    "    \"Eu trabalho no Instituto Metrópole Digital, que é a unidade mais bacana da UFRN. \"\n",
    "    \"Desde 2021 eu também trabalho como Corregedor da UFRN. \"\n",
    "    \"Quando a pandemia começou, no início de 2020, eu estava com as malas prontas para uma viagem de férias para o Japão. \"\n",
    "    \"Eu até fui buscar meu visto no Consulado em Recife, mas, quando chegou mais perto da viagem, meus voos foram todos cancelados pela United Airlines e eu não viajei. \"\n",
    "    \"No dia 11 de novembro de 2023, eu estive no show do Roger Waters em São Paulo. \"\n",
    "    \"Sempre que visito a cidade, eu dou uma passada no Kidoairaku, meu restaurante japonês favorito lá.\"\n",
    ")\n",
    "\n",
    "# Process the text using the spaCy model\n",
    "# This step performs tokenization, part-of-speech tagging, and named entity recognition\n",
    "doc = nlp(text_example)\n",
    "\n",
    "# Use the displaCy visualizer to render the named entities in the text\n",
    "# The 'style' parameter is set to 'ent' to visualize named entities\n",
    "# The 'jupyter' parameter is set to True to display the visualization in a Jupyter Notebook\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elias Jacob', 'PER'),\n",
       " ('Natal', 'LOC'),\n",
       " ('Rio Grande do Norte', 'LOC'),\n",
       " ('Instituto Metrópole Digital', 'ORG'),\n",
       " ('UFRN', 'LOC'),\n",
       " ('Corregedor da UFRN', 'MISC'),\n",
       " ('Japão', 'LOC'),\n",
       " ('Consulado', 'MISC'),\n",
       " ('Recife', 'LOC'),\n",
       " ('United Airlines', 'ORG'),\n",
       " ('Roger Waters', 'PER'),\n",
       " ('São Paulo', 'LOC'),\n",
       " ('Kidoairaku', 'ORG')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Tuple  # Import List and Tuple from the typing module for type annotations\n",
    "\n",
    "def extract_named_entities_from_text(input_text: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract named entities from a given text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str]]: A list of tuples where each tuple contains the named entity and its label.\n",
    "    \"\"\"\n",
    "    # Parse the text with spaCy\n",
    "    # The nlp object processes the input text and returns a parsed Doc object\n",
    "    parsed_text = nlp(input_text)\n",
    "    \n",
    "    # Initialize an empty list to store the named entities\n",
    "    named_entities = []\n",
    "    \n",
    "    # Iterate over the named entities in the parsed text\n",
    "    # The parsed_text.ents attribute contains a list of named entities identified in the text\n",
    "    for entity in parsed_text.ents:\n",
    "        # Append the entity text and label to the list\n",
    "        # entity.text is the named entity, and entity.label_ is its label (e.g., PERSON, ORG, LOC)\n",
    "        named_entities.append((entity.text, entity.label_))\n",
    "    \n",
    "    # Return the list of named entities and their labels\n",
    "    return named_entities\n",
    "\n",
    "# Test the function with an example text\n",
    "# This will extract named entities from the text_example and print them\n",
    "extract_named_entities_from_text(text_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Illustrative Use Cases of NER\n",
    "\n",
    "Let's dive deeper into real-world applications of NER:\n",
    "\n",
    "- **Healthcare Text Analysis**: In clinical note analysis, NER can help identify information about diseases, symptoms, treatments, medications which can aid in improved medical decision making.\n",
    "- **News Articles**: For a news reading app that curates articles, NER can help extract information about people, organizations, and locations mentioned in the article. This can then be used to categorize or tag the articles or improve article recommendations.\n",
    "- **Customer Support**: In a customer support scenario, NER can be used to identify and separate out the important pieces of information from a customer’s query like name, email address, phone number. This can allow for more efficient handling of customer requests.\n",
    "\n",
    "In essence, NER's utility stretches across various domains, transforming unstructured textual data into structured, machine-readable data, thereby enabling more sophisticated and nuanced analyses.\n",
    "\n",
    "## Why not use regular expressions?\n",
    "\n",
    "Regex is a powerful tool for text processing and pattern matching. However, it is not a good choice for NER. The main reason is that regular expressions are not able to generalize well to unseen data. For instance, if we want to extract all the names of people in a text, we can use a regular expression such as [A-Z][a-z]*\\s[A-Z][a-z]*.\n",
    "This regular expression will match all the names of people that have a first name and a last name. However, it will not match names that have a middle name or initial. It will also not match names that have a hyphen, connectives (such as de, a, do, dos, das), suffixes (such as Júnior or Neto). It will also match strings that are not actually names such as the word “Doctor” or “Professor”.\n",
    "\n",
    "## Why not use a dictionary?\n",
    "\n",
    "A dictionary is a good choice for NER if we have a small number of entities that we want to extract. However, it is not a good choice if we have a large number of entities. For instance, if we want to extract all the names of people in a text, we can use a dictionary that contains all the names of people in the world. However, this dictionary will be very large and it will be difficult to maintain. It will also be difficult to update the dictionary when new names are added to the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative Use Cases of Named Entity Recognition (NER)\n",
    "\n",
    "Let's dive deeper into real-world applications of NER:\n",
    "\n",
    "\n",
    "### Healthcare Text Analysis\n",
    "In the healthcare sector, NER can be used to extract and structure critical information from clinical notes and medical literature. This includes identifying and categorizing:\n",
    "\n",
    "- **Diseases**: Recognizing mentions of diseases.\n",
    "- **Symptoms**: Identifying symptoms described in patient records.\n",
    "- **Treatments**: Extracting information about various treatments administered.\n",
    "- **Medications**: Identifying prescribed medications.\n",
    "\n",
    "**Benefits**:\n",
    "- **Improved Medical Decision Making**: Providing clinicians with structured and accessible patient data.\n",
    "- **Automated Cohort Identification**: For clinical trials and research studies based on specific medical conditions.\n",
    "- **Enhanced Patient Risk Stratification**: By analyzing medical histories for potential risk factors.\n",
    "\n",
    "\n",
    "### News Articles\n",
    "For news aggregation and analysis platforms, NER offers significant advantages:\n",
    "\n",
    "- **Content Categorization**: By extracting entities like people, organizations, and locations, articles can be efficiently tagged and categorized.\n",
    "- **Enhanced Recommendations**: By understanding user preferences based on frequently engaged entities, more personalized content suggestions can be made.\n",
    "- **Trend Analysis**: Tracking the frequency of certain entities over time helps in identifying and analyzing trends.\n",
    "\n",
    "\n",
    "### Customer Support\n",
    "NER streamlines customer support operations by:\n",
    "\n",
    "- **Automating Ticket Routing**: Directing queries to the appropriate department based on extracted entities like product names or issue types.\n",
    "- **Enhancing Chatbot Capabilities**: Enabling chatbots to understand and respond to user requests more effectively by identifying key entities within conversations.\n",
    "- **Personalizing Customer Interactions**: Recognizing customer information such as names and order histories to provide more tailored support.\n",
    "\n",
    "**Benefits**:\n",
    "- **Improved Efficiency**: Faster and more accurate handling of customer queries.\n",
    "- **Better Customer Experience**: More personalized and context-relevant interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weak Supervision for Named Entity Recognition\n",
    "\n",
    "## Setting the Stage\n",
    "\n",
    "Let's imagine a scenario where we want to train a Named Entity Recognition (NER) model to identify entities in a text. However, we have limited labeled data available for training. Specifically, imagine the following:\n",
    "\n",
    "1. You work for the legal department of the local governenment.\n",
    "2. Every day, people file lawsuits demanding your government to pay them for drugs that they might need to treat their diseases. Remember: according to the Brazilian Constitution, the government must provide free healthcare to everyone, including free drugs.\n",
    "3. At the sime time, the government has [UNICAT](http://www.unicat.rn.gov.br/), a agency responsible to provide drugs to the population. However, the agency don't know exactly what drugs are being demanded by the population.\n",
    "4. Remember: each lawsuit represents an extra cost to the government. Therefore, it would be better to avoid lawsuits that are not necessary by making the drugs available to the population before the lawsuits are filed.\n",
    "5. It would be very useful to have a system that could automatically identify the names of the drugs mentioned in the lawsuits. This would allow you to quickly analyze past lawsuits and identify patterns that could help you make better decisions in the future.\n",
    "6. Your goal is to build a NER model that can identify the names of the drugs mentioned in the lawsuits, so that you can analyze the data and give UNICAT the information they need to make the drugs available to the population before the lawsuits are filed.\n",
    "\n",
    "In this scenario, you have a limited number of labeled examples where the names of the drugs are annotated. You could manually annotate more examples, but this would be time-consuming and expensive. Instead, you decide to use weak supervision to train your NER model with limited labeled data.\n",
    "\n",
    "## Our Data\n",
    "\n",
    "You'll work with a dataset of legal documents containing lawsuits filed against the government. Each document contains a text description of the lawsuit, and your task is to identify the names of the drugs mentioned in these texts.\n",
    "Your train data contains 826 legal documents with no annotations, but you have a small set of labeled examples where the names of the drugs are annotated. Your development set contains 100 legal documents with annotations for evaluation. Your test set contains 255 legal documents with annotations for final evaluation. Labels use [IOB tagging](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), where \"B\" indicates the beginning of an entity, and \"I\" indicates the continuation of an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((826, 2), (255, 3), (100, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_parquet('data/ner/train.parquet')\n",
    "df_valid = pd.read_parquet('data/ner/valid.parquet')\n",
    "df_test = pd.read_parquet('data/ner/test.parquet')\n",
    "\n",
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8ddbcd4624582098ea5a635311e2fdce</td>\n",
       "      <td>FUNDAMENTAÇÃO\\nCuida-se de ação ordinária sob ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e7f8185d32e003c0fe9df13ab08011fd</td>\n",
       "      <td>RELATÓRIO\\nTrata-se de Ação Ordinária, com ped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>579cecacdd9fed378f341a5d5375ea16</td>\n",
       "      <td>Nessa linha de intelecção, subsumindo as dispo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>be2d81920265b7ffb168cee17076178f</td>\n",
       "      <td>Sugiro liberar a medicação em questão quando a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848d8b4cdd2cb5d03a4a5060ce9ce0f2</td>\n",
       "      <td>Trata-se de Ação Especial, com pedido de tutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>97b26e1d778d9c0a2604e4713241d351</td>\n",
       "      <td>Considerando que o perito nomeado por este juí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>015cf835a1eb4549347d93076c3e615e</td>\n",
       "      <td>Relatório dispensado, nos termos do art. 38 da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>f2377f7fe9be82680bc340884135d2ec</td>\n",
       "      <td>A hipossuficiência financeira para as prestaçõ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>22856c7022cc2cdba9fde68ac31bb4c7</td>\n",
       "      <td>PODER JUDICIÁRIO\\nJUSTIÇA FEDERAL NO RIO GRAND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>b7b7da17f7ba378711225573d99da9ac</td>\n",
       "      <td>(Recursos NUM_PROCESSO, ALMIRO JOSÉ DA ROCHA L...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  uid  \\\n",
       "0    8ddbcd4624582098ea5a635311e2fdce   \n",
       "1    e7f8185d32e003c0fe9df13ab08011fd   \n",
       "2    579cecacdd9fed378f341a5d5375ea16   \n",
       "3    be2d81920265b7ffb168cee17076178f   \n",
       "4    848d8b4cdd2cb5d03a4a5060ce9ce0f2   \n",
       "..                                ...   \n",
       "821  97b26e1d778d9c0a2604e4713241d351   \n",
       "822  015cf835a1eb4549347d93076c3e615e   \n",
       "823  f2377f7fe9be82680bc340884135d2ec   \n",
       "824  22856c7022cc2cdba9fde68ac31bb4c7   \n",
       "825  b7b7da17f7ba378711225573d99da9ac   \n",
       "\n",
       "                                                  text  \n",
       "0    FUNDAMENTAÇÃO\\nCuida-se de ação ordinária sob ...  \n",
       "1    RELATÓRIO\\nTrata-se de Ação Ordinária, com ped...  \n",
       "2    Nessa linha de intelecção, subsumindo as dispo...  \n",
       "3    Sugiro liberar a medicação em questão quando a...  \n",
       "4    Trata-se de Ação Especial, com pedido de tutel...  \n",
       "..                                                 ...  \n",
       "821  Considerando que o perito nomeado por este juí...  \n",
       "822  Relatório dispensado, nos termos do art. 38 da...  \n",
       "823  A hipossuficiência financeira para as prestaçõ...  \n",
       "824  PODER JUDICIÁRIO\\nJUSTIÇA FEDERAL NO RIO GRAND...  \n",
       "825  (Recursos NUM_PROCESSO, ALMIRO JOSÉ DA ROCHA L...  \n",
       "\n",
       "[826 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cc7d620236e02ee78d9977dbc7985584</td>\n",
       "      <td>TIPO “A”\\nTrata-se de ação proposta por FELIPE...</td>\n",
       "      <td>[('TIPO', 'O'), ('“', 'O'), ('A', 'O'), ('”', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3d8df5105d43c30a7dcd7aa602fb895e</td>\n",
       "      <td>Como não houve inclusão pelo Ministério da Saú...</td>\n",
       "      <td>[('Como', 'O'), ('não', 'O'), ('houve', 'O'), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fb6d2d54a1357f72dd4fdd36afb2f2a8</td>\n",
       "      <td>8. Ressalte-se, ademais, que a Nota Técnica n....</td>\n",
       "      <td>[('8', 'O'), ('.', 'O'), ('Ressalte', 'O'), ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a47ff22ae748832734a1fa90a870cc5a</td>\n",
       "      <td>. Acerca do fármacopleiteado e da patologia ap...</td>\n",
       "      <td>[('.', 'O'), ('Acerca', 'O'), ('do', 'O'), ('f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fe2d89c89c60d342bec6d7ab725ba26e</td>\n",
       "      <td>Ademais, há nos autos laudo médico emitido por...</td>\n",
       "      <td>[('Ademais', 'O'), (',', 'O'), ('há', 'O'), ('...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>404eb0d966568da63bbcad6931e5427e</td>\n",
       "      <td>11) Considerando a natureza da patologia e em ...</td>\n",
       "      <td>[('11', 'O'), (')', 'O'), ('Considerando', 'O'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>4f2b2cbcb1a2587786e5f716e08b49cb</td>\n",
       "      <td>Assim, tratando-se de medicamentosque não fora...</td>\n",
       "      <td>[('Assim', 'O'), (',', 'O'), ('tratando', 'O')...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>08be5705a599185821d1c2bbcd5395a9</td>\n",
       "      <td>DEFIRO O PEDIDO DE TUTELA PROVISÓRIA DE URGÊNC...</td>\n",
       "      <td>[('DEFIRO', 'O'), ('O', 'O'), ('PEDIDO', 'O'),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>9cb6d5b410655255080adc9af99930aa</td>\n",
       "      <td>Outrossim, seguindo a linha de diversos preced...</td>\n",
       "      <td>[('Outrossim', 'O'), (',', 'O'), ('seguindo', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>5f389bdb81f48a24ca06809448122629</td>\n",
       "      <td>5. Registrando-se que a Comissão Europeia, org...</td>\n",
       "      <td>[('5', 'O'), ('.', 'O'), ('Registrando', 'O'),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  uid  \\\n",
       "0    cc7d620236e02ee78d9977dbc7985584   \n",
       "1    3d8df5105d43c30a7dcd7aa602fb895e   \n",
       "2    fb6d2d54a1357f72dd4fdd36afb2f2a8   \n",
       "3    a47ff22ae748832734a1fa90a870cc5a   \n",
       "4    fe2d89c89c60d342bec6d7ab725ba26e   \n",
       "..                                ...   \n",
       "250  404eb0d966568da63bbcad6931e5427e   \n",
       "251  4f2b2cbcb1a2587786e5f716e08b49cb   \n",
       "252  08be5705a599185821d1c2bbcd5395a9   \n",
       "253  9cb6d5b410655255080adc9af99930aa   \n",
       "254  5f389bdb81f48a24ca06809448122629   \n",
       "\n",
       "                                                  text  \\\n",
       "0    TIPO “A”\\nTrata-se de ação proposta por FELIPE...   \n",
       "1    Como não houve inclusão pelo Ministério da Saú...   \n",
       "2    8. Ressalte-se, ademais, que a Nota Técnica n....   \n",
       "3    . Acerca do fármacopleiteado e da patologia ap...   \n",
       "4    Ademais, há nos autos laudo médico emitido por...   \n",
       "..                                                 ...   \n",
       "250  11) Considerando a natureza da patologia e em ...   \n",
       "251  Assim, tratando-se de medicamentosque não fora...   \n",
       "252  DEFIRO O PEDIDO DE TUTELA PROVISÓRIA DE URGÊNC...   \n",
       "253  Outrossim, seguindo a linha de diversos preced...   \n",
       "254  5. Registrando-se que a Comissão Europeia, org...   \n",
       "\n",
       "                                                labels  \n",
       "0    [('TIPO', 'O'), ('“', 'O'), ('A', 'O'), ('”', ...  \n",
       "1    [('Como', 'O'), ('não', 'O'), ('houve', 'O'), ...  \n",
       "2    [('8', 'O'), ('.', 'O'), ('Ressalte', 'O'), ('...  \n",
       "3    [('.', 'O'), ('Acerca', 'O'), ('do', 'O'), ('f...  \n",
       "4    [('Ademais', 'O'), (',', 'O'), ('há', 'O'), ('...  \n",
       "..                                                 ...  \n",
       "250  [('11', 'O'), (')', 'O'), ('Considerando', 'O'...  \n",
       "251  [('Assim', 'O'), (',', 'O'), ('tratando', 'O')...  \n",
       "252  [('DEFIRO', 'O'), ('O', 'O'), ('PEDIDO', 'O'),...  \n",
       "253  [('Outrossim', 'O'), (',', 'O'), ('seguindo', ...  \n",
       "254  [('5', 'O'), ('.', 'O'), ('Registrando', 'O'),...  \n",
       "\n",
       "[255 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['labels'] = df_valid['labels'].apply(lambda x: eval(x))\n",
    "df_test['labels'] = df_test['labels'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIPO “A”\n",
      "Trata-se de ação proposta por FELIPE EMANUEL MONTEIRO CAVALCANTE, representado por sua genitora FRANCISCA FLAUBÉRIA QUEIROZ MONTEIRO, com pedido de antecipação dos efeitos da tutela, em desfavor da UNIÃO FEDERAL, ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE PAU DOS FERROS, objetivando o fornecimento da segunda dose da vacina MENINGOCÓCIDA B, a ser custeado solidariamente pelos entes federativos.Relatório dispensado, na forma do art. 38 da Lei 9.099/95 c/c o art. 1o da Lei 10.259/01.\n",
      "I – FUNDAMENTAÇÃO\n",
      "1. PRELIMINAR: ILEGITIMIDADE PASSIVA \n",
      "Não merecer acolhida a preliminar de ilegitimidade passiva suscitada pela União, por ser a saúde competência comum aos entes federados, reclamando uma ação conjunta no propósito de cumprir o dever estatal consubstanciado na Constituição Federal, como se extrai das prescrições legais:\n",
      " “Art. 23. É competência comum da União, dos Estados, do Distrito Federal e dos Municípios: (.)\n",
      "II - cuidar da saúde e assistência pública, da proteção e garantia das pessoas portadoras de deficiência;”\n",
      "“Art. 196. A saúde é direito de todos e dever do Estado, garantido mediante políticas sociais e econômicas que visem à redução do risco de doença e de outros agravos e ao acesso universal e igualitário às ações e serviços para sua promoção, proteção e recuperação.”\n",
      "Logo, não podem os entes demandados se esquivarem da responsabilidade imposta, por dispor cada ente de orçamento público com destinação própria para o atendimento do bem ora reclamado, devendo adotar políticas capazes de proporcionar efetividade aos ditames consubstanciados na carta superior.\n"
     ]
    }
   ],
   "source": [
    "print(df_valid.iloc[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('TIPO', 'O'),\n",
       " ('“', 'O'),\n",
       " ('A', 'O'),\n",
       " ('”', 'O'),\n",
       " ('Trata', 'O'),\n",
       " ('-', 'O'),\n",
       " ('se', 'O'),\n",
       " ('de', 'O'),\n",
       " ('ação', 'O'),\n",
       " ('proposta', 'O'),\n",
       " ('por', 'O'),\n",
       " ('FELIPE', 'O'),\n",
       " ('EMANUEL', 'O'),\n",
       " ('MONTEIRO', 'O'),\n",
       " ('CAVALCANTE', 'O'),\n",
       " (',', 'O'),\n",
       " ('representado', 'O'),\n",
       " ('por', 'O'),\n",
       " ('sua', 'O'),\n",
       " ('genitora', 'O'),\n",
       " ('FRANCISCA', 'O'),\n",
       " ('FLAUBÉRIA', 'O'),\n",
       " ('QUEIROZ', 'O'),\n",
       " ('MONTEIRO', 'O'),\n",
       " (',', 'O'),\n",
       " ('com', 'O'),\n",
       " ('pedido', 'O'),\n",
       " ('de', 'O'),\n",
       " ('antecipação', 'O'),\n",
       " ('dos', 'O'),\n",
       " ('efeitos', 'O'),\n",
       " ('da', 'O'),\n",
       " ('tutela', 'O'),\n",
       " (',', 'O'),\n",
       " ('em', 'O'),\n",
       " ('desfavor', 'O'),\n",
       " ('da', 'O'),\n",
       " ('UNIÃO', 'O'),\n",
       " ('FEDERAL', 'O'),\n",
       " (',', 'O'),\n",
       " ('ESTADO', 'O'),\n",
       " ('DO', 'O'),\n",
       " ('RIO', 'O'),\n",
       " ('GRANDE', 'O'),\n",
       " ('DO', 'O'),\n",
       " ('NORTE', 'O'),\n",
       " ('e', 'O'),\n",
       " ('do', 'O'),\n",
       " ('MUNICÍPIO', 'O'),\n",
       " ('DE', 'O'),\n",
       " ('PAU', 'O'),\n",
       " ('DOS', 'O'),\n",
       " ('FERROS', 'O'),\n",
       " (',', 'O'),\n",
       " ('objetivando', 'O'),\n",
       " ('o', 'O'),\n",
       " ('fornecimento', 'O'),\n",
       " ('da', 'O'),\n",
       " ('segunda', 'O'),\n",
       " ('dose', 'O'),\n",
       " ('da', 'O'),\n",
       " ('vacina', 'B-MEDICAMENTO'),\n",
       " ('MENINGOCÓCIDA', 'I-MEDICAMENTO'),\n",
       " ('B', 'I-MEDICAMENTO'),\n",
       " (',', 'O'),\n",
       " ('a', 'O'),\n",
       " ('ser', 'O'),\n",
       " ('custeado', 'O'),\n",
       " ('solidariamente', 'O'),\n",
       " ('pelos', 'O'),\n",
       " ('entes', 'O'),\n",
       " ('federativos', 'O'),\n",
       " ('.', 'O'),\n",
       " ('Relatório', 'O'),\n",
       " ('dispensado', 'O'),\n",
       " (',', 'O'),\n",
       " ('na', 'O'),\n",
       " ('forma', 'O'),\n",
       " ('do', 'O'),\n",
       " ('art', 'O'),\n",
       " ('.', 'O'),\n",
       " ('38', 'O'),\n",
       " ('da', 'O'),\n",
       " ('Lei', 'O'),\n",
       " ('9', 'O'),\n",
       " ('.', 'O'),\n",
       " ('099', 'O'),\n",
       " ('/', 'O'),\n",
       " ('95', 'O'),\n",
       " ('c', 'O'),\n",
       " ('/', 'O'),\n",
       " ('c', 'O'),\n",
       " ('o', 'O'),\n",
       " ('art', 'O'),\n",
       " ('.', 'O'),\n",
       " ('1o', 'O'),\n",
       " ('da', 'O'),\n",
       " ('Lei', 'O'),\n",
       " ('10', 'O'),\n",
       " ('.', 'O'),\n",
       " ('259', 'O'),\n",
       " ('/', 'O'),\n",
       " ('01', 'O'),\n",
       " ('.', 'O'),\n",
       " ('I', 'O'),\n",
       " ('–', 'O'),\n",
       " ('FUNDAMENTAÇÃO', 'O'),\n",
       " ('1', 'O'),\n",
       " ('.', 'O'),\n",
       " ('PRELIMINAR', 'O'),\n",
       " (':', 'O'),\n",
       " ('ILEGITIMIDADE', 'O'),\n",
       " ('PASSIVA', 'O'),\n",
       " ('Não', 'O'),\n",
       " ('merecer', 'O'),\n",
       " ('acolhida', 'O'),\n",
       " ('a', 'O'),\n",
       " ('preliminar', 'O'),\n",
       " ('de', 'O'),\n",
       " ('ilegitimidade', 'O'),\n",
       " ('passiva', 'O'),\n",
       " ('suscitada', 'O'),\n",
       " ('pela', 'O'),\n",
       " ('União', 'O'),\n",
       " (',', 'O'),\n",
       " ('por', 'O'),\n",
       " ('ser', 'O'),\n",
       " ('a', 'O'),\n",
       " ('saúde', 'O'),\n",
       " ('competência', 'O'),\n",
       " ('comum', 'O'),\n",
       " ('aos', 'O'),\n",
       " ('entes', 'O'),\n",
       " ('federados', 'O'),\n",
       " (',', 'O'),\n",
       " ('reclamando', 'O'),\n",
       " ('uma', 'O'),\n",
       " ('ação', 'O'),\n",
       " ('conjunta', 'O'),\n",
       " ('no', 'O'),\n",
       " ('propósito', 'O'),\n",
       " ('de', 'O'),\n",
       " ('cumprir', 'O'),\n",
       " ('o', 'O'),\n",
       " ('dever', 'O'),\n",
       " ('estatal', 'O'),\n",
       " ('consubstanciado', 'O'),\n",
       " ('na', 'O'),\n",
       " ('Constituição', 'O'),\n",
       " ('Federal', 'O'),\n",
       " (',', 'O'),\n",
       " ('como', 'O'),\n",
       " ('se', 'O'),\n",
       " ('extrai', 'O'),\n",
       " ('das', 'O'),\n",
       " ('prescrições', 'O'),\n",
       " ('legais', 'O'),\n",
       " (':', 'O'),\n",
       " ('“', 'O'),\n",
       " ('Art', 'O'),\n",
       " ('.', 'O'),\n",
       " ('23', 'O'),\n",
       " ('.', 'O'),\n",
       " ('É', 'O'),\n",
       " ('competência', 'O'),\n",
       " ('comum', 'O'),\n",
       " ('da', 'O'),\n",
       " ('União', 'O'),\n",
       " (',', 'O'),\n",
       " ('dos', 'O'),\n",
       " ('Estados', 'O'),\n",
       " (',', 'O'),\n",
       " ('do', 'O'),\n",
       " ('Distrito', 'O'),\n",
       " ('Federal', 'O'),\n",
       " ('e', 'O'),\n",
       " ('dos', 'O'),\n",
       " ('Municípios', 'O'),\n",
       " (':', 'O'),\n",
       " ('(', 'O'),\n",
       " ('.', 'O'),\n",
       " (')', 'O'),\n",
       " ('II', 'O'),\n",
       " ('-', 'O'),\n",
       " ('cuidar', 'O'),\n",
       " ('da', 'O'),\n",
       " ('saúde', 'O'),\n",
       " ('e', 'O'),\n",
       " ('assistência', 'O'),\n",
       " ('pública', 'O'),\n",
       " (',', 'O'),\n",
       " ('da', 'O'),\n",
       " ('proteção', 'O'),\n",
       " ('e', 'O'),\n",
       " ('garantia', 'O'),\n",
       " ('das', 'O'),\n",
       " ('pessoas', 'O'),\n",
       " ('portadoras', 'O'),\n",
       " ('de', 'O'),\n",
       " ('deficiência', 'O'),\n",
       " (';', 'O'),\n",
       " ('”', 'O'),\n",
       " ('“', 'O'),\n",
       " ('Art', 'O'),\n",
       " ('.', 'O'),\n",
       " ('196', 'O'),\n",
       " ('.', 'O'),\n",
       " ('A', 'O'),\n",
       " ('saúde', 'O'),\n",
       " ('é', 'O'),\n",
       " ('direito', 'O'),\n",
       " ('de', 'O'),\n",
       " ('todos', 'O'),\n",
       " ('e', 'O'),\n",
       " ('dever', 'O'),\n",
       " ('do', 'O'),\n",
       " ('Estado', 'O'),\n",
       " (',', 'O'),\n",
       " ('garantido', 'O'),\n",
       " ('mediante', 'O'),\n",
       " ('políticas', 'O'),\n",
       " ('sociais', 'O'),\n",
       " ('e', 'O'),\n",
       " ('econômicas', 'O'),\n",
       " ('que', 'O'),\n",
       " ('visem', 'O'),\n",
       " ('à', 'O'),\n",
       " ('redução', 'O'),\n",
       " ('do', 'O'),\n",
       " ('risco', 'O'),\n",
       " ('de', 'O'),\n",
       " ('doença', 'O'),\n",
       " ('e', 'O'),\n",
       " ('de', 'O'),\n",
       " ('outros', 'O'),\n",
       " ('agravos', 'O'),\n",
       " ('e', 'O'),\n",
       " ('ao', 'O'),\n",
       " ('acesso', 'O'),\n",
       " ('universal', 'O'),\n",
       " ('e', 'O'),\n",
       " ('igualitário', 'O'),\n",
       " ('às', 'O'),\n",
       " ('ações', 'O'),\n",
       " ('e', 'O'),\n",
       " ('serviços', 'O'),\n",
       " ('para', 'O'),\n",
       " ('sua', 'O'),\n",
       " ('promoção', 'O'),\n",
       " (',', 'O'),\n",
       " ('proteção', 'O'),\n",
       " ('e', 'O'),\n",
       " ('recuperação', 'O'),\n",
       " ('.', 'O'),\n",
       " ('”', 'O'),\n",
       " ('Logo', 'O'),\n",
       " (',', 'O'),\n",
       " ('não', 'O'),\n",
       " ('podem', 'O'),\n",
       " ('os', 'O'),\n",
       " ('entes', 'O'),\n",
       " ('demandados', 'O'),\n",
       " ('se', 'O'),\n",
       " ('esquivarem', 'O'),\n",
       " ('da', 'O'),\n",
       " ('responsabilidade', 'O'),\n",
       " ('imposta', 'O'),\n",
       " (',', 'O'),\n",
       " ('por', 'O'),\n",
       " ('dispor', 'O'),\n",
       " ('cada', 'O'),\n",
       " ('ente', 'O'),\n",
       " ('de', 'O'),\n",
       " ('orçamento', 'O'),\n",
       " ('público', 'O'),\n",
       " ('com', 'O'),\n",
       " ('destinação', 'O'),\n",
       " ('própria', 'O'),\n",
       " ('para', 'O'),\n",
       " ('o', 'O'),\n",
       " ('atendimento', 'O'),\n",
       " ('do', 'O'),\n",
       " ('bem', 'O'),\n",
       " ('ora', 'O'),\n",
       " ('reclamado', 'O'),\n",
       " (',', 'O'),\n",
       " ('devendo', 'O'),\n",
       " ('adotar', 'O'),\n",
       " ('políticas', 'O'),\n",
       " ('capazes', 'O'),\n",
       " ('de', 'O'),\n",
       " ('proporcionar', 'O'),\n",
       " ('efetividade', 'O'),\n",
       " ('aos', 'O'),\n",
       " ('ditames', 'O'),\n",
       " ('consubstanciados', 'O'),\n",
       " ('na', 'O'),\n",
       " ('carta', 'O'),\n",
       " ('superior', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid.iloc[0]['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Nots?\n",
    "\n",
    "Considering the scenario described above, let's discuss why other methods, when considered alone, might not be the best fit for this task.\n",
    "\n",
    "### Why Not Manual Annotation?\n",
    "\n",
    "While manual annotation is a reliable method for creating labeled datasets, it has several limitations:\n",
    "\n",
    "- **Time-Consuming**: Manual annotation is labor-intensive and time-consuming, especially for large datasets.\n",
    "- **Costly**: Hiring annotators or using crowdsourcing platforms can be expensive.\n",
    "- **Subjectivity**: Annotations may vary between annotators, leading to inconsistencies.\n",
    "- **Scalability Issues**: Manual annotation is not easily scalable to large datasets or frequent updates.\n",
    "\n",
    "\n",
    "### Why Not Use Regular Expressions?\n",
    "\n",
    "While regular expressions (regex) are powerful for text processing and pattern matching, they have significant limitations for NER:\n",
    "\n",
    "- **Lack of Generalization**: Regex rules are often too rigid to generalize well to unseen data. For example, a pattern designed to match names may not account for variations like middle names, hyphens, or cultural variations.\n",
    "- **Overgeneralization**: Regex might incorrectly identify non-entity text that matches the pattern (e.g., mistaking \"New York\" for a person's name).\n",
    "- **Contextual Blindness**: Regex cannot consider the surrounding context, which is vital for accurate entity recognition.\n",
    "- **Maintenance Challenges**: As language evolves, maintaining an exhaustive set of regex patterns becomes increasingly complex and time-consuming.\n",
    "\n",
    "### Why Not Use a Dictionary?\n",
    "\n",
    "While dictionaries can be useful for NER in certain scenarios, they also have significant drawbacks:\n",
    "\n",
    "- **Scalability Issues**: Maintaining an up-to-date dictionary for broad entity categories (like all person names) is practically impossible due to the vast and ever-changing nature of language.\n",
    "- **Ambiguity Challenges**: Many words can be both entity and non-entity, making dictionary-based approaches prone to errors.\n",
    "- **Incomplete Coverage**: A dictionary might miss newly coined terms or entities, leading to incomplete extraction.\n",
    "\n",
    "> **Note**: Machine learning models can learn context and generalize from training data, making them more adaptable and accurate for NER tasks compared to regex or dictionary-based approaches.\n",
    "\n",
    "### Why Not Use a Pre-Trained Model?\n",
    "\n",
    "While pre-trained models offer a quick and effective way to perform NER, they may not always be suitable for specific use cases due to the following reasons:\n",
    "\n",
    "- **Domain-Specific Knowledge**: Pre-trained models might not capture domain-specific entities or terminologies.\n",
    "- **Fine-Grained Control**: Customizing pre-trained models for specific entity types or constraints can be challenging.\n",
    "- **Data Privacy Concerns**: Using pre-trained models might expose sensitive data to third-party services, raising privacy concerns.\n",
    "\n",
    "> **Note**: Training a custom NER model on domain-specific data can address these limitations and provide more tailored entity recognition capabilities.\n",
    "\n",
    "### Why Not Use Zero-Shot Learning?\n",
    "\n",
    "Zero-shot learning is a powerful technique that allows models to generalize to unseen classes. However, it has some limitations for NER tasks:\n",
    "\n",
    "- **Limited Contextual Understanding**: Zero-shot learning may struggle with complex entity relationships and context-dependent entity recognition.\n",
    "- **Data Efficiency**: Training zero-shot models often requires large amounts of data to generalize effectively to unseen entities.\n",
    "- **Fine-Grained Entity Recognition**: For tasks requiring fine-grained entity classification, zero-shot learning may not provide the necessary granularity.\n",
    "\n",
    "> **Note**: Zero-shot learning can be a valuable tool for NER tasks, especially when dealing with novel entities or limited labeled data, but it may not always outperform supervised learning approaches in all scenarios.\n",
    "\n",
    "\n",
    "While these methods have their advantages, they also come with limitations that can impact the accuracy, scalability, and adaptability of NER systems. Good thing is that we can use Weak Supervision to use the best of these methods to train a NER model. Weak supervision offers a solution to this problem by leveraging various sources of noisy or weak supervision to train models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Tool: [Skweak](https://github.com/NorskRegnesentral/skweak)\n",
    "\n",
    "Skweak is a versatile, Python-based software toolkit designed for Natural Language Processing (NLP) developers. It facilitates the application of weak supervision to various NLP tasks, particularly sequence labeling (our case). While Snorkel is a popular tool for weak supervision, it does not directly support sequence labeling tasks like Named Entity Recognition (NER). We would need way too much workarounds to use Snorkel for NER. Skweak is specifically tailored for sequence labeling tasks, such as Named Entity Recognition (NER).\n",
    "\n",
    "### Features of Skweak\n",
    "\n",
    "- **Labeling Functions**:\n",
    "    - These are custom rules or heuristics that generate noisy labels based on patterns, external sources, or other criteria.\n",
    "    - Labeling functions are essential for reducing the manual effort in data annotation.\n",
    "\n",
    "- **Label Aggregation**:\n",
    "    - Skweak combines the outputs of multiple labeling functions into a single, aggregated label for each data point.\n",
    "    - This process leverages models like HMM to ensure that the final labels are as accurate as possible despite the noise in individual labeling functions.\n",
    "\n",
    "- **Support for Named Entity Recognition (NER)**:\n",
    "    - Skweak is particularly useful for NER tasks, allowing developers to create labeling functions aimed at recognizing entities within text.\n",
    "    - This feature is crucial for tasks requiring the identification of names, dates, locations, and other entities in text data.\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "**Labeling Functions**:\n",
    "- Labeling functions are at the heart of Skweak. They are designed to apply domain-specific heuristics to label data points. For example, a labeling function might tag all capitalized words as potential entities.\n",
    "- These functions can incorporate a variety of sources, including dictionaries, regular expressions, and pre-trained models, to generate noisy labels.\n",
    "\n",
    "**Generative Model for Aggregation**:\n",
    "- After the initial labeling, Skweak uses a generative model to aggregate these noisy labels.\n",
    "- The Hidden Markov Model (HMM) is one such model used for this purpose. It helps in accounting for dependencies between labels and smoothing out inconsistencies.\n",
    "- Alternatively, a Majority Vote approach can be used, where the most common label among the functions is chosen as the final label.\n",
    "\n",
    "**Named Entity Recognition (NER)**:\n",
    "- In NER tasks, Skweak can be particularly powerful. For instance, you can create multiple labeling functions that identify entities based on different criteria, such as context or specific keywords.\n",
    "- The aggregated results from these functions can significantly improve the accuracy of NER tasks compared to using a single labeling function or manual annotation.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- For more details on Skweak, refer to the [GitHub repository](https://github.com/NorskRegnesentral/skweak).\n",
    "- The tool is discussed in depth in the paper [skweak: Weak Supervision Made Easy for NLP](https://arxiv.org/abs/2104.09683).\n",
    "\n",
    "> **Note**: Understanding the basic principles of weak supervision and generative models like HMM can significantly enhance the effective use of Skweak. I recommend exploring these concepts further to exploit Skweak optimally for NER tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow with Skweak for Named Entity Recognition (NER)\n",
    "\n",
    "This section outlines a extensive workflow for using Skweak, a framework for weak supervision in machine learning, in a Named Entity Recognition (NER) task.\n",
    "\n",
    "### 1. Data Preparation\n",
    "\n",
    "- **Organize Text Data**: Structure your text data in a format suitable for processing. This often involves tokenizing the text and performing basic preprocessing. This also involves converting the text data into a format that can be used by Skweak (spacy Doc objects).\n",
    "\n",
    "- **Create Data Splits**:\n",
    "    - **Labeled Data**: Prepare a small set of examples manually annotated with entities for training and validation.\n",
    "    - **Unlabeled Data**: Gather a larger corpus without annotations to apply Skweak's weak supervision techniques.\n",
    "    - **Held-out Test Set**: Reserve a set of data with ground truth annotations for final evaluation.\n",
    "\n",
    "> Note: The quality and diversity of the data are crucial for the success of your NER model.\n",
    "\n",
    "### 2. Labeling Functions Creation\n",
    "\n",
    "Develop a set of labeling functions that generate noisy labels based on various heuristics and rules:\n",
    "\n",
    "- **Patterns**: Use regular expressions or pattern-matching techniques (e.g., capitalization, specific sequences of words).\n",
    "- **Gazetteers**: Apply lists of known entities.\n",
    "- **Context-based Rules**: Identify entities based on surrounding words or typical contexts.\n",
    "- **External Knowledge**: Call APIs or use databases such as Wikipedia for additional context.\n",
    "- **Pre-trained Models**: Incorporate outputs from existing models like spaCy or the Google Natural Language API.\n",
    "\n",
    "> Aim for coverage and diversity to capture different aspects of entity recognition. Start with high-precision rules and gradually introduce more complex functions for better recall.\n",
    "\n",
    "### 3. Weak Supervision\n",
    "\n",
    "- **Apply Labeling Functions**: Use your functions to generate noisy labels for the unlabeled data.\n",
    "- **Aggregate Labels**: Use Skweak's generative models (e.g., Hidden Markov Model) to combine these noisy labels into a coherent set of annotations. This step resolves conflicts and leverages the collective insights of all labeling functions.\n",
    "\n",
    "### 4. Model Training\n",
    "\n",
    "- **Prepare Training Data**: Combine the small set of manually labeled data with the weakly supervised data.\n",
    "- **Choose a Model**: Select an appropriate NER model, such as a Conditional Random Field (CRF) or a Transformer-based model like BERT.\n",
    "- **Train the Model**: Use the aggregated labels as targets. Consider transfer learning to exploit pre-existing language models.\n",
    "\n",
    "### 5. Evaluation\n",
    "\n",
    "- **Performance Assessment**: Evaluate the trained model on the held-out test set with ground truth annotations.\n",
    "- **Metrics**: Measure performance using precision, recall, and F1 score. More advanced metrics like span-based F1 or partial matching can also be useful.\n",
    "- **Error Analysis**: Examine misclassified examples to identify common errors and areas for improvement.\n",
    "\n",
    "### 6. Iterative Refinement\n",
    "\n",
    "The Skweak workflow's strength lies in its iterative nature:\n",
    "\n",
    "- **Refine Labeling Functions**: Improve existing functions or develop new ones based on the errors observed during evaluation.\n",
    "- **Tune Aggregation Methods**: Experiment with different models or parameters to optimize label quality.\n",
    "- **Model Improvements**: Apply architectural changes, hyperparameter tuning, or advanced techniques to enhance model performance.\n",
    "- **Data Augmentation**: Create targeted functions or increase data for underrepresented entity types.\n",
    "\n",
    "> Remember that weak supervision and iterative refinement are key to improving the NER model over time. Each cycle of refinements will help in enhancing the model's precision and robustness.\n",
    "\n",
    "By following this workflow, you can exploit Skweak to build a robust NER system with minimal manual labeling, focusing on continuous improvement and adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Convert Text Data to Spacy Doc Objects\n",
    "\n",
    "The first step in the Skweak workflow is to convert your text data into Spacy Doc objects. Spacy is a popular NLP library that provides efficient tokenization, part-of-speech tagging, and named entity recognition capabilities. By converting your text data into Spacy Doc objects, you can exploit Spacy's functionalities for further processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the spaCy library for natural language processing\n",
    "# spaCy provides tools for tokenization, part-of-speech tagging, named entity recognition, and more\n",
    "import spacy\n",
    "\n",
    "# Import the skweak library for weak supervision\n",
    "# skweak allows us to combine multiple weak supervision sources to create high-quality training data\n",
    "import skweak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to data/bin/ner/spacy_docs_train.bin...done\n",
      "Write to data/bin/ner/spacy_docs_valid.bin...done\n",
      "Write to data/bin/ner/spacy_docs_test.bin...done\n"
     ]
    }
   ],
   "source": [
    "# Load the spaCy model for Portuguese\n",
    "# 'pt_core_news_lg' is a large model with more accuracy and features\n",
    "# You can also use 'pt_core_news_sm' for a smaller, faster model with fewer features\n",
    "# To install the model, run: python -m spacy download pt_core_news_lg\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "# Process the training dataset using the spaCy pipeline\n",
    "# The nlp.pipe method processes the text in batches, which is more efficient than processing each text individually\n",
    "# df_train['text'].values contains the text data from the training dataset\n",
    "spacy_docs_train = list(nlp.pipe(df_train['text'].values))\n",
    "\n",
    "# Process the validation dataset using the spaCy pipeline\n",
    "# df_valid['text'].values contains the text data from the validation dataset\n",
    "spacy_docs_valid = list(nlp.pipe(df_valid['text'].values))\n",
    "\n",
    "# Process the test dataset using the spaCy pipeline\n",
    "# df_test['text'].values contains the text data from the test dataset\n",
    "spacy_docs_test = list(nlp.pipe(df_test['text'].values))\n",
    "\n",
    "# Save the processed spaCy documents to disk\n",
    "# This avoids the need to run the spaCy pipeline again, saving time in future runs\n",
    "# skweak.utils.docbin_writer writes the spaCy documents to a binary file\n",
    "# The first argument is the list of spaCy documents, and the second argument is the file path\n",
    "skweak.utils.docbin_writer(spacy_docs_train, \"data/bin/ner/spacy_docs_train.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_valid, \"data/bin/ner/spacy_docs_valid.bin\")\n",
    "skweak.utils.docbin_writer(spacy_docs_test, \"data/bin/ner/spacy_docs_test.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed spaCy documents from disk\n",
    "# This avoids the need to run the spaCy pipeline again, saving time in future runs\n",
    "# skweak.utils.docbin_reader reads the spaCy documents from a binary file\n",
    "# The first argument is the file path, and the second argument is the name of the spaCy model used for processing\n",
    "\n",
    "# Load the training documents\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\"data/bin/ner/spacy_docs_train.bin\", spacy_model_name=\"pt_core_news_lg\")\n",
    "\n",
    "# Load the validation documents\n",
    "spacy_docs_valid = skweak.utils.docbin_reader(\"data/bin/ner/spacy_docs_valid.bin\", spacy_model_name=\"pt_core_news_lg\")\n",
    "\n",
    "# Load the test documents\n",
    "spacy_docs_test = skweak.utils.docbin_reader(\"data/bin/ner/spacy_docs_test.bin\", spacy_model_name=\"pt_core_news_lg\")\n",
    "\n",
    "# Convert the loaded documents to lists\n",
    "# This step ensures that the documents are in a list format, which is easier to work with in subsequent steps\n",
    "spacy_docs_train = list(spacy_docs_train)\n",
    "spacy_docs_valid = list(spacy_docs_valid)\n",
    "spacy_docs_test = list(spacy_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FUNDAMENTAÇÃO\n",
       " Cuida-se de ação ordinária sob o rito sumaríssimo, com pedido de tutela antecipada, manejada por Natália Samara Araújo Rosalem em face da União e do Estado da Paraíba, objetivando a condenação dos réus no dever de fornecer à parte autora o medicamento Micofenolato Mofetil (CELLCEPT) – 500mg, para a utilização de 3 comprimidos ao dia, nos termos da prescrição médica e enquanto perdurar a indicação clínica.\n",
       " Em apertada síntese, a parte autora afirma ser portadora de lúpus eritematoso sistêmico (CID M 32.8) e que necessita do tratamento ora requerido. Aduz que há urgência no atendimento do seu pleito, sob risco de piora irreversível no seu caso clínico em caso de não realização do procedimento pleiteado.\n",
       " Conforme relatado pelo médico particular Dr. Eduardo Sérgio Ramalho (CRM – 3295/PB), o medicamento requerido é a única alterantiva possível e que ainda não está sendo utilizada.\n",
       " Contudo, segundo alega, tal medicamento não é fornecido pelo SUS, não tendo a requerente condições financeiras de adquiri-lo junto à iniciativa privada, já que ele possui um custo mínimo de R$ 613,00 (seiscentos e treze reais), conforme orçamento contido no anexo 08.\n",
       " Após a citação, as rés apresentaram contestação.\n",
       " É o breve relatório. Passo a decidir.\n",
       " DA LEGITIMIDADE PASSIVA AD CAUSAM \n",
       " Preliminarmente, destaque-se a legitimidade dos entes públicos réus para figurar no pólo passivo da presente lide.\n",
       " A saúde, ao mesmo tempo em que consiste direito fundamental garantido a todos, constitui-se dever irrenunciável do Estado, tomado este termo em sua acepção genérica, ou seja, como Poder Estatal, englobando assim a União, os Estados Federados e os Municípios (art. 196, caput, CF/88).,\n",
       " RELATÓRIO\n",
       " Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado REUQUINOL (HIDROXICLOROQUINA) 400mg e PREGABALINA 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. \n",
       " É o que importa relatar.Decido.\n",
       " PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)\n",
       " A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento REUQUINOL (HIDROXICLOQUINA) é fornecido pelo SUS.\n",
       " Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.\n",
       " Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.\n",
       " Por tais razões, rejeito a preliminar suscitada. \n",
       " PRELIMINAR DE ILEGITIMIDADE PASSIVA\n",
       " Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_docs_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Create Labeling Functions\n",
    "\n",
    "Labelling functions are at the central of skweak. They can be constructed in several ways. The key idea behind all labelling functions is that they take a Doc object as input, and returns a list of (token-level) spans with associated labels.\n",
    "\n",
    "For sequence labelling, the spans simply corresponds to the entities one wish to detect. For text classification tasks (such as sentiment analysis), the span corresponds to the full text you wish to classify (which may be a sentence, or perhaps the full document).\n",
    "\n",
    "There are several heuriestics that can be used to create labelling functions with skweak. I recommend you to check the [documentation](https://github.com/NorskRegnesentral/skweak/wiki/Step-1:-Labelling-functions) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Using a predefined list of drugs (gazetteer)\n",
    "\n",
    "One common approach to creating labelling functions is to use a predefined list of entities, also known as a gazetteer. In our case, we can use a list of drug names to create a labelling function that identifies drug entities in the text.\n",
    "We'll load the drug list from the [\"Preço Máximo ao Consumidor\"](https://www.gov.br/anvisa/pt-br/assuntos/medicamentos/cmed/precos) (Maximum Price to the Consumer) database, which contains information about the maximum prices of drugs in Brazil. This list contains the names of various drugs that are commonly prescribed and used in healthcare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Secretaria Executiva - CMED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LISTA DE PREÇOS DE MEDICAMENTOS - PREÇOS FÁBRI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Publicada em 03/09/2024 às 16h00min.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Esta lista apresenta os preços dos medicamento...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24682</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012007</td>\n",
       "      <td>1015500910141</td>\n",
       "      <td>7896226109657</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>50.50</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24683</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012107</td>\n",
       "      <td>1015500910158</td>\n",
       "      <td>7896226109664</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>57.01</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24684</th>\n",
       "      <td>ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...</td>\n",
       "      <td>60.659.463/0029-92</td>\n",
       "      <td>ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A</td>\n",
       "      <td>500500101118422</td>\n",
       "      <td>1057302060014</td>\n",
       "      <td>7896658000010</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ACCUVIT</td>\n",
       "      <td>COM REV CT FR PLAS OPC X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>140.79</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...</td>\n",
       "      <td>61.190.096/0001-92</td>\n",
       "      <td>EUROFARMA LABORATÓRIOS S.A.</td>\n",
       "      <td>508011804138416</td>\n",
       "      <td>1004306960107</td>\n",
       "      <td>7891317469610</td>\n",
       "      <td>7891317020118</td>\n",
       "      <td>-</td>\n",
       "      <td>SIMECO PLUS</td>\n",
       "      <td>120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...</td>\n",
       "      <td>...</td>\n",
       "      <td>14.42</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24686</th>\n",
       "      <td>ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO</td>\n",
       "      <td>02.456.955/0001-83</td>\n",
       "      <td>NATULAB LABORATÓRIO S.A</td>\n",
       "      <td>540420050013307</td>\n",
       "      <td>1384100600048</td>\n",
       "      <td>7899470800325</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITER C + ZN</td>\n",
       "      <td>1 G + 10 MG COM EFERV CT TB PLAS X 10</td>\n",
       "      <td>...</td>\n",
       "      <td>35.28</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24687 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0                   1   \\\n",
       "0                            Secretaria Executiva - CMED                 NaN   \n",
       "1      LISTA DE PREÇOS DE MEDICAMENTOS - PREÇOS FÁBRI...                 NaN   \n",
       "2                   Publicada em 03/09/2024 às 16h00min.                 NaN   \n",
       "3                                                    NaN                 NaN   \n",
       "4      Esta lista apresenta os preços dos medicamento...                 NaN   \n",
       "...                                                  ...                 ...   \n",
       "24682  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24683  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24684  ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...  60.659.463/0029-92   \n",
       "24685  ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...  61.190.096/0001-92   \n",
       "24686                     ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO  02.456.955/0001-83   \n",
       "\n",
       "                                        2                3              4   \\\n",
       "0                                      NaN              NaN            NaN   \n",
       "1                                      NaN              NaN            NaN   \n",
       "2                                      NaN              NaN            NaN   \n",
       "3                                      NaN              NaN            NaN   \n",
       "4                                      NaN              NaN            NaN   \n",
       "...                                    ...              ...            ...   \n",
       "24682     MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012007  1015500910141   \n",
       "24683     MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012107  1015500910158   \n",
       "24684  ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A  500500101118422  1057302060014   \n",
       "24685          EUROFARMA LABORATÓRIOS S.A.  508011804138416  1004306960107   \n",
       "24686              NATULAB LABORATÓRIO S.A  540420050013307  1384100600048   \n",
       "\n",
       "                  5              6           7               8   \\\n",
       "0                NaN            NaN         NaN             NaN   \n",
       "1                NaN            NaN         NaN             NaN   \n",
       "2                NaN            NaN         NaN             NaN   \n",
       "3                NaN            NaN         NaN             NaN   \n",
       "4                NaN            NaN         NaN             NaN   \n",
       "...              ...            ...         ...             ...   \n",
       "24682  7896226109657         -           -       VITERGAN ZINCO   \n",
       "24683  7896226109664         -           -       VITERGAN ZINCO   \n",
       "24684  7896658000010         -           -              ACCUVIT   \n",
       "24685  7891317469610  7891317020118      -          SIMECO PLUS   \n",
       "24686  7899470800325         -           -         VITER C + ZN   \n",
       "\n",
       "                                                      9   ...      54   55  \\\n",
       "0                                                    NaN  ...     NaN  NaN   \n",
       "1                                                    NaN  ...     NaN  NaN   \n",
       "2                                                    NaN  ...     NaN  NaN   \n",
       "3                                                    NaN  ...     NaN  NaN   \n",
       "4                                                    NaN  ...     NaN  NaN   \n",
       "...                                                  ...  ...     ...  ...   \n",
       "24682       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...   50.50  Não   \n",
       "24683       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...   57.01  Não   \n",
       "24684                        COM REV CT FR PLAS OPC X 30  ...  140.79  Não   \n",
       "24685  120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...  ...   14.42  Não   \n",
       "24686              1 G + 10 MG COM EFERV CT TB PLAS X 10  ...   35.28  Não   \n",
       "\n",
       "        56   57   58   59        60   61               62   63  \n",
       "0      NaN  NaN  NaN  NaN       NaN  NaN              NaN  NaN  \n",
       "1      NaN  NaN  NaN  NaN       NaN  NaN              NaN  NaN  \n",
       "2      NaN  NaN  NaN  NaN       NaN  NaN              NaN  NaN  \n",
       "3      NaN  NaN  NaN  NaN       NaN  NaN              NaN  NaN  \n",
       "4      NaN  NaN  NaN  NaN       NaN  NaN              NaN  NaN  \n",
       "...    ...  ...  ...  ...       ...  ...              ...  ...  \n",
       "24682  Não  Não  Não  NaN  Negativa  Não  Tarja Sem Tarja  Sim  \n",
       "24683  Não  Não  Não  NaN  Negativa  Não  Tarja Sem Tarja  Sim  \n",
       "24684  Não  Não  Não  NaN  Negativa  Não  Tarja Sem Tarja  Sim  \n",
       "24685  Não  Não  Não  NaN  Negativa  Não  Tarja Sem Tarja  Sim  \n",
       "24686  Não  Não  Não  NaN  Negativa  Não           - (*)   Sim  \n",
       "\n",
       "[24687 rows x 64 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmc = pd.read_excel('data/ner/pmc_20240903.xls', header=None)\n",
    "pmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Secretaria Executiva - CMED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LISTA DE PREÇOS DE MEDICAMENTOS - PREÇOS FÁBRI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Publicada em 03/09/2024 às 16h00min.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Esta lista apresenta os preços dos medicamento...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A lista de Preços de Medicamentos contempla o ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Nesta lista foi incluída a alíquota de ICMS 0%...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Esta Lista apresenta, ainda, o Preço Máximo ao...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TIPOS DE PRODUTO:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Medicamento biológico – aquele que contém molé...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Medicamento com princípios ativos sintéticos e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Medicamento novo – medicamento com insumo farm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Medicamento genérico – medicamento similar a u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Medicamento similar – aquele que contém o mesm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Medicamento específico – produto farmacêutico,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Medicamento fitoterápico – obtidos com emprego...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>STATUS DO PRODUTO:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Medicamento de Referência é um produto inovado...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>COMERCIALIZAÇÃO 2022 – O produto foi comercial...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Notas:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(i) Todas as alíquotas anuais de ICMS serão di...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(ii) Liberado – Produtos liberados dos critéri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(iii) Medicamentos em embalagens hospitalares ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(iv) O campo “Análise Recursal” destina-se a p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(v) The \"Análise Recursal\" field informs if th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(vi) El campo \"Análise Recursal\" informa sobre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(vii) O campo \"Regime de Preço\" destina-se a p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(vii) Os registros da coluna \"Tarja\" marcados ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(viii) As apresentações desta lista exibem tod...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(AR) O preço destas das apresentações aguardam...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(1) Apresentação do medicamento ZARZIO (Código...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SUBSTÂNCIA</td>\n",
       "      <td>CNPJ</td>\n",
       "      <td>LABORATÓRIO</td>\n",
       "      <td>CÓDIGO GGREM</td>\n",
       "      <td>REGISTRO</td>\n",
       "      <td>EAN 1</td>\n",
       "      <td>EAN 2</td>\n",
       "      <td>EAN 3</td>\n",
       "      <td>PRODUTO</td>\n",
       "      <td>APRESENTAÇÃO</td>\n",
       "      <td>...</td>\n",
       "      <td>PMC 22% ALC</td>\n",
       "      <td>RESTRIÇÃO HOSPITALAR</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CONFAZ 87</td>\n",
       "      <td>ICMS 0%</td>\n",
       "      <td>ANÁLISE RECURSAL</td>\n",
       "      <td>LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/...</td>\n",
       "      <td>COMERCIALIZAÇÃO 2022</td>\n",
       "      <td>TARJA</td>\n",
       "      <td>DESTINAÇÃO COMERCIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL</td>\n",
       "      <td>18.459.628/0001-15</td>\n",
       "      <td>BAYER S.A.</td>\n",
       "      <td>538912020009303</td>\n",
       "      <td>1705600230032</td>\n",
       "      <td>7891106000956</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>BAYCUTEN N</td>\n",
       "      <td>10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G</td>\n",
       "      <td>...</td>\n",
       "      <td>45.70</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505107701157215</td>\n",
       "      <td>1018003900019</td>\n",
       "      <td>7896016806469</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505113100020505</td>\n",
       "      <td>1018003900078</td>\n",
       "      <td>7896016808197</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...</td>\n",
       "      <td>...</td>\n",
       "      <td>11381.34</td>\n",
       "      <td>Não</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060021902</td>\n",
       "      <td>1126001990018</td>\n",
       "      <td>7896382708442</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>50 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>4812.49</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060022102</td>\n",
       "      <td>1126001990034</td>\n",
       "      <td>7896382708466</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>100 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>9624.92</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060022302</td>\n",
       "      <td>1126001990050</td>\n",
       "      <td>7896382708480</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>150 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>13820.57</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060022402</td>\n",
       "      <td>1126001990069</td>\n",
       "      <td>7896382708497</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>150 MG COM REV CT BL AL AL X 60</td>\n",
       "      <td>...</td>\n",
       "      <td>27641.14</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060022502</td>\n",
       "      <td>1126001990077</td>\n",
       "      <td>7896382708503</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>200 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>17259.49</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0                   1   \\\n",
       "0                         Secretaria Executiva - CMED                 NaN   \n",
       "1   LISTA DE PREÇOS DE MEDICAMENTOS - PREÇOS FÁBRI...                 NaN   \n",
       "2                Publicada em 03/09/2024 às 16h00min.                 NaN   \n",
       "3                                                 NaN                 NaN   \n",
       "4   Esta lista apresenta os preços dos medicamento...                 NaN   \n",
       "5                                                 NaN                 NaN   \n",
       "6   A lista de Preços de Medicamentos contempla o ...                 NaN   \n",
       "7                                                 NaN                 NaN   \n",
       "8   Nesta lista foi incluída a alíquota de ICMS 0%...                 NaN   \n",
       "9                                                 NaN                 NaN   \n",
       "10  Esta Lista apresenta, ainda, o Preço Máximo ao...                 NaN   \n",
       "11                                                NaN                 NaN   \n",
       "12                                 TIPOS DE PRODUTO:                  NaN   \n",
       "13                                                NaN                 NaN   \n",
       "14  Medicamento biológico – aquele que contém molé...                 NaN   \n",
       "15                                                NaN                 NaN   \n",
       "16  Medicamento com princípios ativos sintéticos e...                 NaN   \n",
       "17  Medicamento novo – medicamento com insumo farm...                 NaN   \n",
       "18  Medicamento genérico – medicamento similar a u...                 NaN   \n",
       "19  Medicamento similar – aquele que contém o mesm...                 NaN   \n",
       "20                                                NaN                 NaN   \n",
       "21  Medicamento específico – produto farmacêutico,...                 NaN   \n",
       "22                                                NaN                 NaN   \n",
       "23  Medicamento fitoterápico – obtidos com emprego...                 NaN   \n",
       "24                                                NaN                 NaN   \n",
       "25                                 STATUS DO PRODUTO:                 NaN   \n",
       "26  Medicamento de Referência é um produto inovado...                 NaN   \n",
       "27  COMERCIALIZAÇÃO 2022 – O produto foi comercial...                 NaN   \n",
       "28                                                NaN                 NaN   \n",
       "29                                             Notas:                 NaN   \n",
       "30  (i) Todas as alíquotas anuais de ICMS serão di...                 NaN   \n",
       "31  (ii) Liberado – Produtos liberados dos critéri...                 NaN   \n",
       "32  (iii) Medicamentos em embalagens hospitalares ...                 NaN   \n",
       "33  (iv) O campo “Análise Recursal” destina-se a p...                 NaN   \n",
       "34  (v) The \"Análise Recursal\" field informs if th...                 NaN   \n",
       "35  (vi) El campo \"Análise Recursal\" informa sobre...                 NaN   \n",
       "36  (vii) O campo \"Regime de Preço\" destina-se a p...                 NaN   \n",
       "37  (vii) Os registros da coluna \"Tarja\" marcados ...                 NaN   \n",
       "38  (viii) As apresentações desta lista exibem tod...                 NaN   \n",
       "39  (AR) O preço destas das apresentações aguardam...                 NaN   \n",
       "40  (1) Apresentação do medicamento ZARZIO (Código...                 NaN   \n",
       "41                                         SUBSTÂNCIA                CNPJ   \n",
       "42             21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL  18.459.628/0001-15   \n",
       "43                                         ABATACEPTE  56.998.982/0001-07   \n",
       "44                                         ABATACEPTE  56.998.982/0001-07   \n",
       "45                                       ABEMACICLIBE  43.940.618/0001-44   \n",
       "46                                       ABEMACICLIBE  43.940.618/0001-44   \n",
       "47                                       ABEMACICLIBE  43.940.618/0001-44   \n",
       "48                                       ABEMACICLIBE  43.940.618/0001-44   \n",
       "49                                       ABEMACICLIBE  43.940.618/0001-44   \n",
       "\n",
       "                                        2                3              4   \\\n",
       "0                                      NaN              NaN            NaN   \n",
       "1                                      NaN              NaN            NaN   \n",
       "2                                      NaN              NaN            NaN   \n",
       "3                                      NaN              NaN            NaN   \n",
       "4                                      NaN              NaN            NaN   \n",
       "5                                      NaN              NaN            NaN   \n",
       "6                                      NaN              NaN            NaN   \n",
       "7                                      NaN              NaN            NaN   \n",
       "8                                      NaN              NaN            NaN   \n",
       "9                                      NaN              NaN            NaN   \n",
       "10                                     NaN              NaN            NaN   \n",
       "11                                     NaN              NaN            NaN   \n",
       "12                                     NaN              NaN            NaN   \n",
       "13                                     NaN              NaN            NaN   \n",
       "14                                     NaN              NaN            NaN   \n",
       "15                                     NaN              NaN            NaN   \n",
       "16                                     NaN              NaN            NaN   \n",
       "17                                     NaN              NaN            NaN   \n",
       "18                                     NaN              NaN            NaN   \n",
       "19                                     NaN              NaN            NaN   \n",
       "20                                     NaN              NaN            NaN   \n",
       "21                                     NaN              NaN            NaN   \n",
       "22                                     NaN              NaN            NaN   \n",
       "23                                     NaN              NaN            NaN   \n",
       "24                                     NaN              NaN            NaN   \n",
       "25                                     NaN              NaN            NaN   \n",
       "26                                     NaN              NaN            NaN   \n",
       "27                                     NaN              NaN            NaN   \n",
       "28                                     NaN              NaN            NaN   \n",
       "29                                     NaN              NaN            NaN   \n",
       "30                                     NaN              NaN            NaN   \n",
       "31                                     NaN              NaN            NaN   \n",
       "32                                     NaN              NaN            NaN   \n",
       "33                                     NaN              NaN            NaN   \n",
       "34                                     NaN              NaN            NaN   \n",
       "35                                     NaN              NaN            NaN   \n",
       "36                                     NaN              NaN            NaN   \n",
       "37                                     NaN              NaN            NaN   \n",
       "38                                     NaN              NaN            NaN   \n",
       "39                                     NaN              NaN            NaN   \n",
       "40                                     NaN              NaN            NaN   \n",
       "41                             LABORATÓRIO     CÓDIGO GGREM       REGISTRO   \n",
       "42                              BAYER S.A.  538912020009303  1705600230032   \n",
       "43  BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505107701157215  1018003900019   \n",
       "44  BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505113100020505  1018003900078   \n",
       "45                ELI LILLY DO BRASIL LTDA  507619060021902  1126001990018   \n",
       "46                ELI LILLY DO BRASIL LTDA  507619060022102  1126001990034   \n",
       "47                ELI LILLY DO BRASIL LTDA  507619060022302  1126001990050   \n",
       "48                ELI LILLY DO BRASIL LTDA  507619060022402  1126001990069   \n",
       "49                ELI LILLY DO BRASIL LTDA  507619060022502  1126001990077   \n",
       "\n",
       "               5           6           7           8   \\\n",
       "0             NaN         NaN         NaN         NaN   \n",
       "1             NaN         NaN         NaN         NaN   \n",
       "2             NaN         NaN         NaN         NaN   \n",
       "3             NaN         NaN         NaN         NaN   \n",
       "4             NaN         NaN         NaN         NaN   \n",
       "5             NaN         NaN         NaN         NaN   \n",
       "6             NaN         NaN         NaN         NaN   \n",
       "7             NaN         NaN         NaN         NaN   \n",
       "8             NaN         NaN         NaN         NaN   \n",
       "9             NaN         NaN         NaN         NaN   \n",
       "10            NaN         NaN         NaN         NaN   \n",
       "11            NaN         NaN         NaN         NaN   \n",
       "12            NaN         NaN         NaN         NaN   \n",
       "13            NaN         NaN         NaN         NaN   \n",
       "14            NaN         NaN         NaN         NaN   \n",
       "15            NaN         NaN         NaN         NaN   \n",
       "16            NaN         NaN         NaN         NaN   \n",
       "17            NaN         NaN         NaN         NaN   \n",
       "18            NaN         NaN         NaN         NaN   \n",
       "19            NaN         NaN         NaN         NaN   \n",
       "20            NaN         NaN         NaN         NaN   \n",
       "21            NaN         NaN         NaN         NaN   \n",
       "22            NaN         NaN         NaN         NaN   \n",
       "23            NaN         NaN         NaN         NaN   \n",
       "24            NaN         NaN         NaN         NaN   \n",
       "25            NaN         NaN         NaN         NaN   \n",
       "26            NaN         NaN         NaN         NaN   \n",
       "27            NaN         NaN         NaN         NaN   \n",
       "28            NaN         NaN         NaN         NaN   \n",
       "29            NaN         NaN         NaN         NaN   \n",
       "30            NaN         NaN         NaN         NaN   \n",
       "31            NaN         NaN         NaN         NaN   \n",
       "32            NaN         NaN         NaN         NaN   \n",
       "33            NaN         NaN         NaN         NaN   \n",
       "34            NaN         NaN         NaN         NaN   \n",
       "35            NaN         NaN         NaN         NaN   \n",
       "36            NaN         NaN         NaN         NaN   \n",
       "37            NaN         NaN         NaN         NaN   \n",
       "38            NaN         NaN         NaN         NaN   \n",
       "39            NaN         NaN         NaN         NaN   \n",
       "40            NaN         NaN         NaN         NaN   \n",
       "41          EAN 1       EAN 2       EAN 3     PRODUTO   \n",
       "42  7891106000956      -           -       BAYCUTEN N   \n",
       "43  7896016806469      -           -          ORENCIA   \n",
       "44  7896016808197      -           -          ORENCIA   \n",
       "45  7896382708442      -           -        VERZENIOS   \n",
       "46  7896382708466      -           -        VERZENIOS   \n",
       "47  7896382708480      -           -        VERZENIOS   \n",
       "48  7896382708497      -           -        VERZENIOS   \n",
       "49  7896382708503      -           -        VERZENIOS   \n",
       "\n",
       "                                                   9   ...           54  \\\n",
       "0                                                 NaN  ...          NaN   \n",
       "1                                                 NaN  ...          NaN   \n",
       "2                                                 NaN  ...          NaN   \n",
       "3                                                 NaN  ...          NaN   \n",
       "4                                                 NaN  ...          NaN   \n",
       "5                                                 NaN  ...          NaN   \n",
       "6                                                 NaN  ...          NaN   \n",
       "7                                                 NaN  ...          NaN   \n",
       "8                                                 NaN  ...          NaN   \n",
       "9                                                 NaN  ...          NaN   \n",
       "10                                                NaN  ...          NaN   \n",
       "11                                                NaN  ...          NaN   \n",
       "12                                                NaN  ...          NaN   \n",
       "13                                                NaN  ...          NaN   \n",
       "14                                                NaN  ...          NaN   \n",
       "15                                                NaN  ...          NaN   \n",
       "16                                                NaN  ...          NaN   \n",
       "17                                                NaN  ...          NaN   \n",
       "18                                                NaN  ...          NaN   \n",
       "19                                                NaN  ...          NaN   \n",
       "20                                                NaN  ...          NaN   \n",
       "21                                                NaN  ...          NaN   \n",
       "22                                                NaN  ...          NaN   \n",
       "23                                                NaN  ...          NaN   \n",
       "24                                                NaN  ...          NaN   \n",
       "25                                                NaN  ...          NaN   \n",
       "26                                                NaN  ...          NaN   \n",
       "27                                                NaN  ...          NaN   \n",
       "28                                                NaN  ...          NaN   \n",
       "29                                                NaN  ...          NaN   \n",
       "30                                                NaN  ...          NaN   \n",
       "31                                                NaN  ...          NaN   \n",
       "32                                                NaN  ...          NaN   \n",
       "33                                                NaN  ...          NaN   \n",
       "34                                                NaN  ...          NaN   \n",
       "35                                                NaN  ...          NaN   \n",
       "36                                                NaN  ...          NaN   \n",
       "37                                                NaN  ...          NaN   \n",
       "38                                                NaN  ...          NaN   \n",
       "39                                                NaN  ...          NaN   \n",
       "40                                                NaN  ...          NaN   \n",
       "41                                       APRESENTAÇÃO  ...  PMC 22% ALC   \n",
       "42     10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G  ...        45.70   \n",
       "43   250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL  ...          NaN   \n",
       "44  125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...  ...     11381.34   \n",
       "45                     50 MG COM REV CT BL AL AL X 30  ...      4812.49   \n",
       "46                    100 MG COM REV CT BL AL AL X 30  ...      9624.92   \n",
       "47                    150 MG COM REV CT BL AL AL X 30  ...     13820.57   \n",
       "48                    150 MG COM REV CT BL AL AL X 60  ...     27641.14   \n",
       "49                    200 MG COM REV CT BL AL AL X 30  ...     17259.49   \n",
       "\n",
       "                      55   56         57       58                59  \\\n",
       "0                    NaN  NaN        NaN      NaN               NaN   \n",
       "1                    NaN  NaN        NaN      NaN               NaN   \n",
       "2                    NaN  NaN        NaN      NaN               NaN   \n",
       "3                    NaN  NaN        NaN      NaN               NaN   \n",
       "4                    NaN  NaN        NaN      NaN               NaN   \n",
       "5                    NaN  NaN        NaN      NaN               NaN   \n",
       "6                    NaN  NaN        NaN      NaN               NaN   \n",
       "7                    NaN  NaN        NaN      NaN               NaN   \n",
       "8                    NaN  NaN        NaN      NaN               NaN   \n",
       "9                    NaN  NaN        NaN      NaN               NaN   \n",
       "10                   NaN  NaN        NaN      NaN               NaN   \n",
       "11                   NaN  NaN        NaN      NaN               NaN   \n",
       "12                   NaN  NaN        NaN      NaN               NaN   \n",
       "13                   NaN  NaN        NaN      NaN               NaN   \n",
       "14                   NaN  NaN        NaN      NaN               NaN   \n",
       "15                   NaN  NaN        NaN      NaN               NaN   \n",
       "16                   NaN  NaN        NaN      NaN               NaN   \n",
       "17                   NaN  NaN        NaN      NaN               NaN   \n",
       "18                   NaN  NaN        NaN      NaN               NaN   \n",
       "19                   NaN  NaN        NaN      NaN               NaN   \n",
       "20                   NaN  NaN        NaN      NaN               NaN   \n",
       "21                   NaN  NaN        NaN      NaN               NaN   \n",
       "22                   NaN  NaN        NaN      NaN               NaN   \n",
       "23                   NaN  NaN        NaN      NaN               NaN   \n",
       "24                   NaN  NaN        NaN      NaN               NaN   \n",
       "25                   NaN  NaN        NaN      NaN               NaN   \n",
       "26                   NaN  NaN        NaN      NaN               NaN   \n",
       "27                   NaN  NaN        NaN      NaN               NaN   \n",
       "28                   NaN  NaN        NaN      NaN               NaN   \n",
       "29                   NaN  NaN        NaN      NaN               NaN   \n",
       "30                   NaN  NaN        NaN      NaN               NaN   \n",
       "31                   NaN  NaN        NaN      NaN               NaN   \n",
       "32                   NaN  NaN        NaN      NaN               NaN   \n",
       "33                   NaN  NaN        NaN      NaN               NaN   \n",
       "34                   NaN  NaN        NaN      NaN               NaN   \n",
       "35                   NaN  NaN        NaN      NaN               NaN   \n",
       "36                   NaN  NaN        NaN      NaN               NaN   \n",
       "37                   NaN  NaN        NaN      NaN               NaN   \n",
       "38                   NaN  NaN        NaN      NaN               NaN   \n",
       "39                   NaN  NaN        NaN      NaN               NaN   \n",
       "40                   NaN  NaN        NaN      NaN               NaN   \n",
       "41  RESTRIÇÃO HOSPITALAR  CAP  CONFAZ 87  ICMS 0%  ANÁLISE RECURSAL   \n",
       "42                   Não  Não        Não      Não               NaN   \n",
       "43                   Sim  Sim        Não      Não               NaN   \n",
       "44                   Não  Sim        Sim      Não               NaN   \n",
       "45                   Não  Não        Não      Não               NaN   \n",
       "46                   Não  Não        Não      Não               NaN   \n",
       "47                   Não  Não        Não      Não               NaN   \n",
       "48                   Não  Não        Não      Não               NaN   \n",
       "49                   Não  Não        Não      Não               NaN   \n",
       "\n",
       "                                                   60                    61  \\\n",
       "0                                                 NaN                   NaN   \n",
       "1                                                 NaN                   NaN   \n",
       "2                                                 NaN                   NaN   \n",
       "3                                                 NaN                   NaN   \n",
       "4                                                 NaN                   NaN   \n",
       "5                                                 NaN                   NaN   \n",
       "6                                                 NaN                   NaN   \n",
       "7                                                 NaN                   NaN   \n",
       "8                                                 NaN                   NaN   \n",
       "9                                                 NaN                   NaN   \n",
       "10                                                NaN                   NaN   \n",
       "11                                                NaN                   NaN   \n",
       "12                                                NaN                   NaN   \n",
       "13                                                NaN                   NaN   \n",
       "14                                                NaN                   NaN   \n",
       "15                                                NaN                   NaN   \n",
       "16                                                NaN                   NaN   \n",
       "17                                                NaN                   NaN   \n",
       "18                                                NaN                   NaN   \n",
       "19                                                NaN                   NaN   \n",
       "20                                                NaN                   NaN   \n",
       "21                                                NaN                   NaN   \n",
       "22                                                NaN                   NaN   \n",
       "23                                                NaN                   NaN   \n",
       "24                                                NaN                   NaN   \n",
       "25                                                NaN                   NaN   \n",
       "26                                                NaN                   NaN   \n",
       "27                                                NaN                   NaN   \n",
       "28                                                NaN                   NaN   \n",
       "29                                                NaN                   NaN   \n",
       "30                                                NaN                   NaN   \n",
       "31                                                NaN                   NaN   \n",
       "32                                                NaN                   NaN   \n",
       "33                                                NaN                   NaN   \n",
       "34                                                NaN                   NaN   \n",
       "35                                                NaN                   NaN   \n",
       "36                                                NaN                   NaN   \n",
       "37                                                NaN                   NaN   \n",
       "38                                                NaN                   NaN   \n",
       "39                                                NaN                   NaN   \n",
       "40                                                NaN                   NaN   \n",
       "41  LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/...  COMERCIALIZAÇÃO 2022   \n",
       "42                                           Negativa                   Sim   \n",
       "43                                           Positiva                   Sim   \n",
       "44                                           Positiva                   Sim   \n",
       "45                                           Negativa                   Sim   \n",
       "46                                           Negativa                   Sim   \n",
       "47                                           Negativa                   Sim   \n",
       "48                                           Negativa                   Sim   \n",
       "49                                           Negativa                   Sim   \n",
       "\n",
       "                62                     63  \n",
       "0              NaN                    NaN  \n",
       "1              NaN                    NaN  \n",
       "2              NaN                    NaN  \n",
       "3              NaN                    NaN  \n",
       "4              NaN                    NaN  \n",
       "5              NaN                    NaN  \n",
       "6              NaN                    NaN  \n",
       "7              NaN                    NaN  \n",
       "8              NaN                    NaN  \n",
       "9              NaN                    NaN  \n",
       "10             NaN                    NaN  \n",
       "11             NaN                    NaN  \n",
       "12             NaN                    NaN  \n",
       "13             NaN                    NaN  \n",
       "14             NaN                    NaN  \n",
       "15             NaN                    NaN  \n",
       "16             NaN                    NaN  \n",
       "17             NaN                    NaN  \n",
       "18             NaN                    NaN  \n",
       "19             NaN                    NaN  \n",
       "20             NaN                    NaN  \n",
       "21             NaN                    NaN  \n",
       "22             NaN                    NaN  \n",
       "23             NaN                    NaN  \n",
       "24             NaN                    NaN  \n",
       "25             NaN                    NaN  \n",
       "26             NaN                    NaN  \n",
       "27             NaN                    NaN  \n",
       "28             NaN                    NaN  \n",
       "29             NaN                    NaN  \n",
       "30             NaN                    NaN  \n",
       "31             NaN                    NaN  \n",
       "32             NaN                    NaN  \n",
       "33             NaN                    NaN  \n",
       "34             NaN                    NaN  \n",
       "35             NaN                    NaN  \n",
       "36             NaN                    NaN  \n",
       "37             NaN                    NaN  \n",
       "38             NaN                    NaN  \n",
       "39             NaN                    NaN  \n",
       "40             NaN                    NaN  \n",
       "41           TARJA  DESTINAÇÃO COMERCIAL   \n",
       "42          - (*)                     Sim  \n",
       "43  Tarja Vermelha                    Sim  \n",
       "44          - (*)                     Sim  \n",
       "45  Tarja Vermelha                    Não  \n",
       "46  Tarja Vermelha                    Não  \n",
       "47  Tarja Vermelha                    Não  \n",
       "48  Tarja Vermelha                    Não  \n",
       "49  Tarja Vermelha                    Não  \n",
       "\n",
       "[50 rows x 64 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmc.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SUBSTÂNCIA</td>\n",
       "      <td>CNPJ</td>\n",
       "      <td>LABORATÓRIO</td>\n",
       "      <td>CÓDIGO GGREM</td>\n",
       "      <td>REGISTRO</td>\n",
       "      <td>EAN 1</td>\n",
       "      <td>EAN 2</td>\n",
       "      <td>EAN 3</td>\n",
       "      <td>PRODUTO</td>\n",
       "      <td>APRESENTAÇÃO</td>\n",
       "      <td>...</td>\n",
       "      <td>PMC 22% ALC</td>\n",
       "      <td>RESTRIÇÃO HOSPITALAR</td>\n",
       "      <td>CAP</td>\n",
       "      <td>CONFAZ 87</td>\n",
       "      <td>ICMS 0%</td>\n",
       "      <td>ANÁLISE RECURSAL</td>\n",
       "      <td>LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/...</td>\n",
       "      <td>COMERCIALIZAÇÃO 2022</td>\n",
       "      <td>TARJA</td>\n",
       "      <td>DESTINAÇÃO COMERCIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL</td>\n",
       "      <td>18.459.628/0001-15</td>\n",
       "      <td>BAYER S.A.</td>\n",
       "      <td>538912020009303</td>\n",
       "      <td>1705600230032</td>\n",
       "      <td>7891106000956</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>BAYCUTEN N</td>\n",
       "      <td>10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G</td>\n",
       "      <td>...</td>\n",
       "      <td>45.70</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505107701157215</td>\n",
       "      <td>1018003900019</td>\n",
       "      <td>7896016806469</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505113100020505</td>\n",
       "      <td>1018003900078</td>\n",
       "      <td>7896016808197</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...</td>\n",
       "      <td>...</td>\n",
       "      <td>11381.34</td>\n",
       "      <td>Não</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060021902</td>\n",
       "      <td>1126001990018</td>\n",
       "      <td>7896382708442</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>50 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>4812.49</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24682</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012007</td>\n",
       "      <td>1015500910141</td>\n",
       "      <td>7896226109657</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>50.50</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24683</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012107</td>\n",
       "      <td>1015500910158</td>\n",
       "      <td>7896226109664</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>57.01</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24684</th>\n",
       "      <td>ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...</td>\n",
       "      <td>60.659.463/0029-92</td>\n",
       "      <td>ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A</td>\n",
       "      <td>500500101118422</td>\n",
       "      <td>1057302060014</td>\n",
       "      <td>7896658000010</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ACCUVIT</td>\n",
       "      <td>COM REV CT FR PLAS OPC X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>140.79</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...</td>\n",
       "      <td>61.190.096/0001-92</td>\n",
       "      <td>EUROFARMA LABORATÓRIOS S.A.</td>\n",
       "      <td>508011804138416</td>\n",
       "      <td>1004306960107</td>\n",
       "      <td>7891317469610</td>\n",
       "      <td>7891317020118</td>\n",
       "      <td>-</td>\n",
       "      <td>SIMECO PLUS</td>\n",
       "      <td>120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...</td>\n",
       "      <td>...</td>\n",
       "      <td>14.42</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24686</th>\n",
       "      <td>ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO</td>\n",
       "      <td>02.456.955/0001-83</td>\n",
       "      <td>NATULAB LABORATÓRIO S.A</td>\n",
       "      <td>540420050013307</td>\n",
       "      <td>1384100600048</td>\n",
       "      <td>7899470800325</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITER C + ZN</td>\n",
       "      <td>1 G + 10 MG COM EFERV CT TB PLAS X 10</td>\n",
       "      <td>...</td>\n",
       "      <td>35.28</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24646 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0                   1   \\\n",
       "41                                            SUBSTÂNCIA                CNPJ   \n",
       "42                21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL  18.459.628/0001-15   \n",
       "43                                            ABATACEPTE  56.998.982/0001-07   \n",
       "44                                            ABATACEPTE  56.998.982/0001-07   \n",
       "45                                          ABEMACICLIBE  43.940.618/0001-44   \n",
       "...                                                  ...                 ...   \n",
       "24682  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24683  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24684  ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...  60.659.463/0029-92   \n",
       "24685  ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...  61.190.096/0001-92   \n",
       "24686                     ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO  02.456.955/0001-83   \n",
       "\n",
       "                                           2                3              4   \\\n",
       "41                                LABORATÓRIO     CÓDIGO GGREM       REGISTRO   \n",
       "42                                 BAYER S.A.  538912020009303  1705600230032   \n",
       "43     BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505107701157215  1018003900019   \n",
       "44     BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505113100020505  1018003900078   \n",
       "45                   ELI LILLY DO BRASIL LTDA  507619060021902  1126001990018   \n",
       "...                                       ...              ...            ...   \n",
       "24682        MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012007  1015500910141   \n",
       "24683        MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012107  1015500910158   \n",
       "24684     ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A  500500101118422  1057302060014   \n",
       "24685             EUROFARMA LABORATÓRIOS S.A.  508011804138416  1004306960107   \n",
       "24686                 NATULAB LABORATÓRIO S.A  540420050013307  1384100600048   \n",
       "\n",
       "                  5              6           7               8   \\\n",
       "41             EAN 1          EAN 2       EAN 3         PRODUTO   \n",
       "42     7891106000956         -           -           BAYCUTEN N   \n",
       "43     7896016806469         -           -              ORENCIA   \n",
       "44     7896016808197         -           -              ORENCIA   \n",
       "45     7896382708442         -           -            VERZENIOS   \n",
       "...              ...            ...         ...             ...   \n",
       "24682  7896226109657         -           -       VITERGAN ZINCO   \n",
       "24683  7896226109664         -           -       VITERGAN ZINCO   \n",
       "24684  7896658000010         -           -              ACCUVIT   \n",
       "24685  7891317469610  7891317020118      -          SIMECO PLUS   \n",
       "24686  7899470800325         -           -         VITER C + ZN   \n",
       "\n",
       "                                                      9   ...           54  \\\n",
       "41                                          APRESENTAÇÃO  ...  PMC 22% ALC   \n",
       "42        10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G  ...        45.70   \n",
       "43      250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL  ...          NaN   \n",
       "44     125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...  ...     11381.34   \n",
       "45                        50 MG COM REV CT BL AL AL X 30  ...      4812.49   \n",
       "...                                                  ...  ...          ...   \n",
       "24682       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...        50.50   \n",
       "24683       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...        57.01   \n",
       "24684                        COM REV CT FR PLAS OPC X 30  ...       140.79   \n",
       "24685  120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...  ...        14.42   \n",
       "24686              1 G + 10 MG COM EFERV CT TB PLAS X 10  ...        35.28   \n",
       "\n",
       "                         55   56         57       58                59  \\\n",
       "41     RESTRIÇÃO HOSPITALAR  CAP  CONFAZ 87  ICMS 0%  ANÁLISE RECURSAL   \n",
       "42                      Não  Não        Não      Não               NaN   \n",
       "43                      Sim  Sim        Não      Não               NaN   \n",
       "44                      Não  Sim        Sim      Não               NaN   \n",
       "45                      Não  Não        Não      Não               NaN   \n",
       "...                     ...  ...        ...      ...               ...   \n",
       "24682                   Não  Não        Não      Não               NaN   \n",
       "24683                   Não  Não        Não      Não               NaN   \n",
       "24684                   Não  Não        Não      Não               NaN   \n",
       "24685                   Não  Não        Não      Não               NaN   \n",
       "24686                   Não  Não        Não      Não               NaN   \n",
       "\n",
       "                                                      60  \\\n",
       "41     LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/...   \n",
       "42                                              Negativa   \n",
       "43                                              Positiva   \n",
       "44                                              Positiva   \n",
       "45                                              Negativa   \n",
       "...                                                  ...   \n",
       "24682                                           Negativa   \n",
       "24683                                           Negativa   \n",
       "24684                                           Negativa   \n",
       "24685                                           Negativa   \n",
       "24686                                           Negativa   \n",
       "\n",
       "                         61               62                     63  \n",
       "41     COMERCIALIZAÇÃO 2022            TARJA  DESTINAÇÃO COMERCIAL   \n",
       "42                      Sim           - (*)                     Sim  \n",
       "43                      Sim   Tarja Vermelha                    Sim  \n",
       "44                      Sim           - (*)                     Sim  \n",
       "45                      Sim   Tarja Vermelha                    Não  \n",
       "...                     ...              ...                    ...  \n",
       "24682                   Não  Tarja Sem Tarja                    Sim  \n",
       "24683                   Não  Tarja Sem Tarja                    Sim  \n",
       "24684                   Não  Tarja Sem Tarja                    Sim  \n",
       "24685                   Não  Tarja Sem Tarja                    Sim  \n",
       "24686                   Não           - (*)                     Sim  \n",
       "\n",
       "[24646 rows x 64 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the first 41 rows\n",
    "pmc = pmc.drop(range(41))\n",
    "pmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>41</th>\n",
       "      <th>SUBSTÂNCIA</th>\n",
       "      <th>CNPJ</th>\n",
       "      <th>LABORATÓRIO</th>\n",
       "      <th>CÓDIGO GGREM</th>\n",
       "      <th>REGISTRO</th>\n",
       "      <th>EAN 1</th>\n",
       "      <th>EAN 2</th>\n",
       "      <th>EAN 3</th>\n",
       "      <th>PRODUTO</th>\n",
       "      <th>APRESENTAÇÃO</th>\n",
       "      <th>...</th>\n",
       "      <th>PMC 22% ALC</th>\n",
       "      <th>RESTRIÇÃO HOSPITALAR</th>\n",
       "      <th>CAP</th>\n",
       "      <th>CONFAZ 87</th>\n",
       "      <th>ICMS 0%</th>\n",
       "      <th>ANÁLISE RECURSAL</th>\n",
       "      <th>LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/COFINS)</th>\n",
       "      <th>COMERCIALIZAÇÃO 2022</th>\n",
       "      <th>TARJA</th>\n",
       "      <th>DESTINAÇÃO COMERCIAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL</td>\n",
       "      <td>18.459.628/0001-15</td>\n",
       "      <td>BAYER S.A.</td>\n",
       "      <td>538912020009303</td>\n",
       "      <td>1705600230032</td>\n",
       "      <td>7891106000956</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>BAYCUTEN N</td>\n",
       "      <td>10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G</td>\n",
       "      <td>...</td>\n",
       "      <td>45.70</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505107701157215</td>\n",
       "      <td>1018003900019</td>\n",
       "      <td>7896016806469</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ABATACEPTE</td>\n",
       "      <td>56.998.982/0001-07</td>\n",
       "      <td>BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA</td>\n",
       "      <td>505113100020505</td>\n",
       "      <td>1018003900078</td>\n",
       "      <td>7896016808197</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ORENCIA</td>\n",
       "      <td>125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...</td>\n",
       "      <td>...</td>\n",
       "      <td>11381.34</td>\n",
       "      <td>Não</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positiva</td>\n",
       "      <td>Sim</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060021902</td>\n",
       "      <td>1126001990018</td>\n",
       "      <td>7896382708442</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>50 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>4812.49</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ABEMACICLIBE</td>\n",
       "      <td>43.940.618/0001-44</td>\n",
       "      <td>ELI LILLY DO BRASIL LTDA</td>\n",
       "      <td>507619060022102</td>\n",
       "      <td>1126001990034</td>\n",
       "      <td>7896382708466</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VERZENIOS</td>\n",
       "      <td>100 MG COM REV CT BL AL AL X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>9624.92</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Sim</td>\n",
       "      <td>Tarja Vermelha</td>\n",
       "      <td>Não</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24682</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012007</td>\n",
       "      <td>1015500910141</td>\n",
       "      <td>7896226109657</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>50.50</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24683</th>\n",
       "      <td>ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...</td>\n",
       "      <td>60.726.692/0001-81</td>\n",
       "      <td>MARJAN INDÚSTRIA E COMÉRCIO LTDA</td>\n",
       "      <td>524821050012107</td>\n",
       "      <td>1015500910158</td>\n",
       "      <td>7896226109664</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITERGAN ZINCO</td>\n",
       "      <td>COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15</td>\n",
       "      <td>...</td>\n",
       "      <td>57.01</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24684</th>\n",
       "      <td>ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...</td>\n",
       "      <td>60.659.463/0029-92</td>\n",
       "      <td>ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A</td>\n",
       "      <td>500500101118422</td>\n",
       "      <td>1057302060014</td>\n",
       "      <td>7896658000010</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ACCUVIT</td>\n",
       "      <td>COM REV CT FR PLAS OPC X 30</td>\n",
       "      <td>...</td>\n",
       "      <td>140.79</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24685</th>\n",
       "      <td>ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...</td>\n",
       "      <td>61.190.096/0001-92</td>\n",
       "      <td>EUROFARMA LABORATÓRIOS S.A.</td>\n",
       "      <td>508011804138416</td>\n",
       "      <td>1004306960107</td>\n",
       "      <td>7891317469610</td>\n",
       "      <td>7891317020118</td>\n",
       "      <td>-</td>\n",
       "      <td>SIMECO PLUS</td>\n",
       "      <td>120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...</td>\n",
       "      <td>...</td>\n",
       "      <td>14.42</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>Tarja Sem Tarja</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24686</th>\n",
       "      <td>ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO</td>\n",
       "      <td>02.456.955/0001-83</td>\n",
       "      <td>NATULAB LABORATÓRIO S.A</td>\n",
       "      <td>540420050013307</td>\n",
       "      <td>1384100600048</td>\n",
       "      <td>7899470800325</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>VITER C + ZN</td>\n",
       "      <td>1 G + 10 MG COM EFERV CT TB PLAS X 10</td>\n",
       "      <td>...</td>\n",
       "      <td>35.28</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>Não</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Negativa</td>\n",
       "      <td>Não</td>\n",
       "      <td>- (*)</td>\n",
       "      <td>Sim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24645 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "41                                            SUBSTÂNCIA                CNPJ  \\\n",
       "42                21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL  18.459.628/0001-15   \n",
       "43                                            ABATACEPTE  56.998.982/0001-07   \n",
       "44                                            ABATACEPTE  56.998.982/0001-07   \n",
       "45                                          ABEMACICLIBE  43.940.618/0001-44   \n",
       "46                                          ABEMACICLIBE  43.940.618/0001-44   \n",
       "...                                                  ...                 ...   \n",
       "24682  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24683  ÓXIDO CÚPRICO;ACETATO DE RACEALFATOCOFEROL;BET...  60.726.692/0001-81   \n",
       "24684  ÓXIDO CÚPRICO;SELENATO DE SÓDIO;ACETATO DE RAC...  60.659.463/0029-92   \n",
       "24685  ÓXIDO DE MAGNÉSIO;SIMETICONA;HIDRÓXIDO DE ALUM...  61.190.096/0001-92   \n",
       "24686                     ÓXIDO DE ZINCO;ÁCIDO ASCÓRBICO  02.456.955/0001-83   \n",
       "\n",
       "41                                LABORATÓRIO     CÓDIGO GGREM       REGISTRO  \\\n",
       "42                                 BAYER S.A.  538912020009303  1705600230032   \n",
       "43     BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505107701157215  1018003900019   \n",
       "44     BRISTOL-MYERS SQUIBB FARMACÊUTICA LTDA  505113100020505  1018003900078   \n",
       "45                   ELI LILLY DO BRASIL LTDA  507619060021902  1126001990018   \n",
       "46                   ELI LILLY DO BRASIL LTDA  507619060022102  1126001990034   \n",
       "...                                       ...              ...            ...   \n",
       "24682        MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012007  1015500910141   \n",
       "24683        MARJAN INDÚSTRIA E COMÉRCIO LTDA  524821050012107  1015500910158   \n",
       "24684     ACHÉ LABORATÓRIOS FARMACÊUTICOS S.A  500500101118422  1057302060014   \n",
       "24685             EUROFARMA LABORATÓRIOS S.A.  508011804138416  1004306960107   \n",
       "24686                 NATULAB LABORATÓRIO S.A  540420050013307  1384100600048   \n",
       "\n",
       "41             EAN 1          EAN 2       EAN 3         PRODUTO  \\\n",
       "42     7891106000956         -           -           BAYCUTEN N   \n",
       "43     7896016806469         -           -              ORENCIA   \n",
       "44     7896016808197         -           -              ORENCIA   \n",
       "45     7896382708442         -           -            VERZENIOS   \n",
       "46     7896382708466         -           -            VERZENIOS   \n",
       "...              ...            ...         ...             ...   \n",
       "24682  7896226109657         -           -       VITERGAN ZINCO   \n",
       "24683  7896226109664         -           -       VITERGAN ZINCO   \n",
       "24684  7896658000010         -           -              ACCUVIT   \n",
       "24685  7891317469610  7891317020118      -          SIMECO PLUS   \n",
       "24686  7899470800325         -           -         VITER C + ZN   \n",
       "\n",
       "41                                          APRESENTAÇÃO  ... PMC 22% ALC  \\\n",
       "42        10 MG/G + 0,443 MG/G CREM DERM CT BG AL X 40 G  ...       45.70   \n",
       "43      250 MG PO LIOF SOL INJ CT 1 FA + SER DESCARTÁVEL  ...         NaN   \n",
       "44     125 MG/ML SOL INJ SC CT 4 SER PREENC VD TRANS ...  ...    11381.34   \n",
       "45                        50 MG COM REV CT BL AL AL X 30  ...     4812.49   \n",
       "46                       100 MG COM REV CT BL AL AL X 30  ...     9624.92   \n",
       "...                                                  ...  ...         ...   \n",
       "24682       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...       50.50   \n",
       "24683       COM REV CT BL AL PLAS PVC/PE/PVDC TRANS X 15  ...       57.01   \n",
       "24684                        COM REV CT FR PLAS OPC X 30  ...      140.79   \n",
       "24685  120 MG/ML + 60 MG/ML + 7 MG/ML SUS OR CT FR VD...  ...       14.42   \n",
       "24686              1 G + 10 MG COM EFERV CT TB PLAS X 10  ...       35.28   \n",
       "\n",
       "41    RESTRIÇÃO HOSPITALAR  CAP CONFAZ 87 ICMS 0% ANÁLISE RECURSAL  \\\n",
       "42                     Não  Não       Não     Não              NaN   \n",
       "43                     Sim  Sim       Não     Não              NaN   \n",
       "44                     Não  Sim       Sim     Não              NaN   \n",
       "45                     Não  Não       Não     Não              NaN   \n",
       "46                     Não  Não       Não     Não              NaN   \n",
       "...                    ...  ...       ...     ...              ...   \n",
       "24682                  Não  Não       Não     Não              NaN   \n",
       "24683                  Não  Não       Não     Não              NaN   \n",
       "24684                  Não  Não       Não     Não              NaN   \n",
       "24685                  Não  Não       Não     Não              NaN   \n",
       "24686                  Não  Não       Não     Não              NaN   \n",
       "\n",
       "41    LISTA DE CONCESSÃO DE CRÉDITO TRIBUTÁRIO (PIS/COFINS)  \\\n",
       "42                                              Negativa      \n",
       "43                                              Positiva      \n",
       "44                                              Positiva      \n",
       "45                                              Negativa      \n",
       "46                                              Negativa      \n",
       "...                                                  ...      \n",
       "24682                                           Negativa      \n",
       "24683                                           Negativa      \n",
       "24684                                           Negativa      \n",
       "24685                                           Negativa      \n",
       "24686                                           Negativa      \n",
       "\n",
       "41    COMERCIALIZAÇÃO 2022            TARJA DESTINAÇÃO COMERCIAL   \n",
       "42                     Sim           - (*)                    Sim  \n",
       "43                     Sim   Tarja Vermelha                   Sim  \n",
       "44                     Sim           - (*)                    Sim  \n",
       "45                     Sim   Tarja Vermelha                   Não  \n",
       "46                     Sim   Tarja Vermelha                   Não  \n",
       "...                    ...              ...                   ...  \n",
       "24682                  Não  Tarja Sem Tarja                   Sim  \n",
       "24683                  Não  Tarja Sem Tarja                   Sim  \n",
       "24684                  Não  Tarja Sem Tarja                   Sim  \n",
       "24685                  Não  Tarja Sem Tarja                   Sim  \n",
       "24686                  Não           - (*)                    Sim  \n",
       "\n",
       "[24645 rows x 64 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the first row the header\n",
    "pmc.columns = pmc.iloc[0]\n",
    "pmc = pmc.drop(41)\n",
    "pmc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['21-ACETATO DE DEXAMETASONA;CLOTRIMAZOL', 'ABATACEPTE',\n",
       "       'ABATACEPTE', 'ABEMACICLIBE', 'ABEMACICLIBE'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substance_names = pmc['SUBSTÂNCIA'].values\n",
    "substance_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BAYCUTEN N', 'ORENCIA', 'ORENCIA', 'VERZENIOS', 'VERZENIOS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_names = pmc['PRODUTO'].values\n",
    "brand_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Initialize an empty list to store drug names\n",
    "drugs = []\n",
    "\n",
    "# Split pharmacological substances\n",
    "for substance in substance_names:\n",
    "    # Split the substance names by commas, semicolons, or plus signs\n",
    "    split_substance = re.split(r'[,;\\+]', substance)\n",
    "    # Strip any leading or trailing whitespace from each split part\n",
    "    split_substance = [s.strip() for s in split_substance]\n",
    "    # Extend the drugs list with the split parts\n",
    "    drugs.extend(split_substance)\n",
    "\n",
    "# Extend the drugs list with brand names\n",
    "drugs.extend(brand_names)\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "drugs = list(set(drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FORTEN',\n",
       " 'TACROLIMO',\n",
       " 'ATEROMA',\n",
       " 'EZET',\n",
       " 'LOCERYL',\n",
       " 'RIVACRIST',\n",
       " 'TRAZIMERA',\n",
       " 'CONTRACTUBEX',\n",
       " 'LISADOR DIP',\n",
       " 'MEPSEVII']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ACIDO GLUTAMICO' in drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EZET',\n",
       " 'FORTEN',\n",
       " 'TACROLIMO',\n",
       " 'ATEROMA',\n",
       " 'LOCERYL',\n",
       " 'RIVACRIST',\n",
       " 'TRAZIMERA',\n",
       " 'CONTRACTUBEX',\n",
       " 'LISADOR DIP',\n",
       " 'MEPSEVII']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers.text import remove_accented_characters\n",
    "\n",
    "# Remove accented characters from each drug name\n",
    "# This is useful for standardizing the drug names for further processing\n",
    "drugs.extend([remove_accented_characters(d) for d in drugs])\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "# This ensures that each drug name appears only once in the list\n",
    "drugs = list(set(drugs))\n",
    "\n",
    "# Display the first 10 drug names\n",
    "# This is useful for quickly checking the contents of the list\n",
    "drugs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ACIDO GLUTAMICO' in drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['picbam',\n",
       " 'lfm-leflunomida',\n",
       " 'riduzi',\n",
       " 'ecalta',\n",
       " 'hormotrop',\n",
       " 'diuremida',\n",
       " 'brometo de ipratroprio',\n",
       " 'obinutuzumabe',\n",
       " 'nidazofarma',\n",
       " 'neumosin']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all drug names to lowercase\n",
    "# This ensures that the comparison of drug names is case-insensitive\n",
    "# The list comprehension iterates over each drug name in the 'drugs' list and converts it to lowercase\n",
    "drugs = [d.lower() for d in drugs]\n",
    "\n",
    "# Remove duplicate drug names\n",
    "# The set() function removes duplicates by converting the list to a set, which only keeps unique elements\n",
    "# The list() function converts the set back to a list\n",
    "drugs = list(set(drugs))\n",
    "\n",
    "# Display the first 10 drug names\n",
    "# This allows us to inspect a sample of the processed drug names\n",
    "# The slicing operation [:10] retrieves the first 10 elements from the 'drugs' list\n",
    "drugs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACIDO GLUTAMICO\n",
      "cloridrato de lidocaína\n",
      "Metilprednisolona\n"
     ]
    }
   ],
   "source": [
    "def remove_hydrated_compounds(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove words related to hydrated compounds from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text containing chemical compound names.\n",
    "\n",
    "    Returns:\n",
    "        str: The text with hydrated compound names removed.\n",
    "    \"\"\"\n",
    "    # Replace words like \"monoidratada\" with an empty string\n",
    "    cleaned_text = re.sub(r' \\b\\S*idratad[oa]\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove any extra spaces that may result from the substitution\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "print(remove_hydrated_compounds('ACIDO GLUTAMICO MONOIDRATADO')) # Expected: 'ACIDO GLUTAMICO'\n",
    "print(remove_hydrated_compounds('cloridrato de lidocaína monoidratada')) # Expected: 'cloridrato de lidocaína'\n",
    "print(remove_hydrated_compounds('Metilprednisolona')) # Expected: 'Metilprednisolona'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8016"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the remove_hydrated_compounds function to each drug name in the drugs list\n",
    "drugs = [remove_hydrated_compounds(d) for d in drugs]\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "# This ensures that each drug name appears only once in the list\n",
    "drugs = list(set(drugs))\n",
    "\n",
    "# Calculate the number of unique drug names\n",
    "# This is useful for understanding the size of the dataset after processing\n",
    "len(drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pantoprazol\n",
      "metformina\n",
      "paracetamol\n"
     ]
    }
   ],
   "source": [
    "def extract_active_ingredient(full_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the active ingredient name from the full medication name.\n",
    "\n",
    "    Args:\n",
    "        full_name (str): The full name of the medication.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the active ingredient.\n",
    "    \"\"\"\n",
    "    # List of words that indicate the start of the active ingredient name\n",
    "    indicators = ['de', 'do', 'da', 'dos', 'das']\n",
    "    \n",
    "    # Split the full name into words and convert to lowercase\n",
    "    words = full_name.lower().split()\n",
    "    \n",
    "    # Find the index of the first indicator word\n",
    "    active_ingredient_index = None\n",
    "    for i, word in enumerate(words):\n",
    "        if word in indicators and i < len(words) - 1:\n",
    "            active_ingredient_index = i\n",
    "            break\n",
    "    \n",
    "    # If an indicator is found, return the words after it\n",
    "    if active_ingredient_index is not None:\n",
    "        return ' '.join(words[active_ingredient_index + 1:])\n",
    "    else:\n",
    "        # If no indicator is found, return the full name\n",
    "        return full_name\n",
    "\n",
    "# Examples of usage\n",
    "print(extract_active_ingredient(\"micofenolato de pantoprazol\"))  # Should return \"pantoprazol\"\n",
    "print(extract_active_ingredient(\"cloridrato de metformina\"))     # Should return \"metformina\"\n",
    "print(extract_active_ingredient(\"paracetamol\"))                  # Should return \"paracetamol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extract_active_ingredient function to each drug name in the drugs list\n",
    "drugs.extend([extract_active_ingredient(d) for d in drugs])\n",
    "\n",
    "# Remove duplicates by converting the list to a set and back to a list\n",
    "# This ensures that each active ingredient appears only once in the list\n",
    "drugs = list(set(drugs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8579"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter the drugs list to keep only the names with more than 4 characters\n",
    "# This step helps to remove very short names that are likely not meaningful drug names\n",
    "drugs = [d for d in drugs if len(d) > 4]\n",
    "\n",
    "# Calculate the number of remaining drug names\n",
    "# This gives an idea of how many drug names are left after filtering\n",
    "len(drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['picbam', 'lfm-leflunomida', 'riduzi', 'ecalta', 'hormotrop']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drugs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data from data/ner/drugs_gazetteer.json\n",
      "Populating trie for class MEDICAMENTO (number: 8579)\n"
     ]
    }
   ],
   "source": [
    "# Import the json module to handle JSON data\n",
    "import json\n",
    "\n",
    "# Initialize an empty dictionary to store the drug names\n",
    "json_drugs = {}\n",
    "\n",
    "# Add the list of drug names to the dictionary under the key 'MEDICAMENTO'\n",
    "json_drugs['MEDICAMENTO'] = drugs\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "# The ensure_ascii=False parameter allows for non-ASCII characters to be saved correctly\n",
    "# The indent=4 parameter makes the JSON file more readable by adding indentation\n",
    "with open('data/ner/drugs_gazetteer.json', 'w') as f:\n",
    "    json.dump(json_drugs, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Load the JSON file into a gazetteer for weak supervision\n",
    "# The extract_json_data function reads the JSON file and prepares it for use with skweak\n",
    "tries_drugs = skweak.gazetteers.extract_json_data('data/ner/drugs_gazetteer.json', spacy_model=\"pt_core_news_lg\")\n",
    "\n",
    "# Create a GazetteerAnnotator for labeling the data\n",
    "# The GazetteerAnnotator uses the gazetteer to annotate text with drug names\n",
    "# The case_sensitive=False parameter makes the annotation case-insensitive\n",
    "lf_drugs_gazetteer = skweak.gazetteers.GazetteerAnnotator(\"drugs_gazetteer\", tries_drugs, case_sensitive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">O paciente foi medicado com \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ácido glutâmico\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " monoidratado , \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cloridrato de lidocaína\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rivotril\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " .</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a sample text containing drug names for testing the function\n",
    "text = \"O paciente foi medicado com ácido glutâmico monoidratado , Cloridrato de lidocaína e Rivotril .\"\n",
    "\n",
    "# Process the text using the spaCy NLP pipeline\n",
    "# This step tokenizes the text and applies linguistic annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply the GazetteerAnnotator to the processed text\n",
    "# This annotates the text with drug names using the gazetteer\n",
    "lf_drugs_gazetteer(doc)\n",
    "\n",
    "# Display the annotated entities in the text\n",
    "# This function highlights the recognized drug names in the text for visualization\n",
    "skweak.utils.display_entities(doc, \"drugs_gazetteer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">FUNDAMENTAÇÃO<br>Cuida-se de ação ordinária sob o rito sumaríssimo, com pedido de tutela antecipada, manejada por Natália Samara Araújo Rosalem em face da União e do Estado da Paraíba, objetivando a condenação dos réus no dever de fornecer à parte autora o medicamento Micofenolato Mofetil (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CELLCEPT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") – 500mg, para a utilização de 3 comprimidos ao dia, nos termos da prescrição médica e enquanto perdurar a indicação clínica.<br>Em apertada síntese, a parte autora afirma ser portadora de lúpus eritematoso sistêmico (CID M 32.8) e que necessita do tratamento ora requerido. Aduz que há urgência no atendimento do seu pleito, sob risco de piora irreversível no seu caso clínico em caso de não realização do procedimento pleiteado.<br>Conforme relatado pelo médico particular Dr. Eduardo Sérgio Ramalho (CRM – 3295/PB), o medicamento requerido é a única alterantiva possível e que ainda não está sendo utilizada.<br>Contudo, segundo alega, tal medicamento não é fornecido pelo SUS, não tendo a requerente condições financeiras de adquiri-lo junto à iniciativa privada, já que ele possui um custo mínimo de R$ 613,00 (seiscentos e treze reais), conforme orçamento contido no anexo 08.<br>Após a citação, as rés apresentaram contestação.<br>É o breve relatório. Passo a decidir.<br>DA LEGITIMIDADE PASSIVA AD CAUSAM <br>Preliminarmente, destaque-se a legitimidade dos entes públicos réus para figurar no pólo passivo da presente lide.<br>A saúde, ao mesmo tempo em que consiste direito fundamental garantido a todos, constitui-se dever irrenunciável do Estado, tomado este termo em sua acepção genérica, ou seja, como Poder Estatal, englobando assim a União, os Estados Federados e os Municípios (art. 196, caput, CF/88).</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the first document from the training set of spaCy documents\n",
    "# This document will be used to demonstrate the annotation process\n",
    "doc = spacy_docs_train[0]\n",
    "\n",
    "# Apply the GazetteerAnnotator to the selected document\n",
    "# This annotates the document with drug names using the gazetteer\n",
    "lf_drugs_gazetteer(doc)\n",
    "\n",
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized drug names in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"drugs_gazetteer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the first document from the training set of spaCy documents\n",
    "# This document will be used to demonstrate the annotation process\n",
    "doc = spacy_docs_train[1]\n",
    "\n",
    "# Apply the GazetteerAnnotator to the selected document\n",
    "# This annotates the document with drug names using the gazetteer\n",
    "lf_drugs_gazetteer(doc)\n",
    "\n",
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized drug names in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"drugs_gazetteer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Leveraging Pretrained Transformer Models for Labeling Functions\n",
    "\n",
    "In addition to rule-based approaches, pretrained language models offer a powerful alternative for constructing labeling functions. These models, trained on vast amounts of text data, can be employed to identify entities within our target text.\n",
    "\n",
    "This section focuses on using a pretrained Named Entity Recognition (NER) model from the Hugging Face Transformers library to build a labeling function specifically for identifying **drug entities**. We'll be applying the `pucpr/clinicalnerpt-chemical` model. This model is particularly well-suited for our purpose as it has been fine-tuned on a corpus of clinical text data, enabling it to effectively recognize chemical entities, including drug names.\n",
    "\n",
    "**Why this model?**\n",
    "\n",
    "- **Domain Specificity:** Fine-tuning on clinical text makes the model more accurate for our use case compared to a general-purpose NER model.\n",
    "- **Direct Applicability:** The model's output directly aligns with our goal of identifying drug entities, simplifying the labeling function creation process.\n",
    "\n",
    "This approach leverages the power of transfer learning, allowing us to benefit from the extensive training these models have undergone and apply their knowledge to our specific task. You can find more details about this model on its Hugging Face model card: [https://huggingface.co/pucpr/clinicalnerpt-chemical](https://huggingface.co/pucpr/clinicalnerpt-chemical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-20 14:46:35.816812: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-20 14:46:35.828291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-20 14:46:35.831665: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-20 14:46:35.841040: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-20 14:46:36.806438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f32f72e5bf4483b2e3403f299c45f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  43%|####2     | 304M/709M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b41ab37d8c64b9aafa2742cf6eda11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19653439da4446f0851c4b3807015aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d417631000704dd88086119140082f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'ChemicalDrugs', 'score': 0.5152051, 'word': 'medicamento', 'start': 278, 'end': 289}, {'entity_group': 'ChemicalDrugs', 'score': 0.92696726, 'word': 'reuquinol', 'start': 301, 'end': 310}, {'entity_group': 'ChemicalDrugs', 'score': 0.9846222, 'word': 'hidroxicloroquina', 'start': 312, 'end': 329}, {'entity_group': 'ChemicalDrugs', 'score': 0.8043461, 'word': 'pregabalina', 'start': 339, 'end': 350}, {'entity_group': 'ChemicalDrugs', 'score': 0.77164274, 'word': 'medicamento', 'start': 715, 'end': 726}, {'entity_group': 'ChemicalDrugs', 'score': 0.89936185, 'word': 'reuquinol', 'start': 727, 'end': 736}, {'entity_group': 'ChemicalDrugs', 'score': 0.9620483, 'word': 'hidroxicloquina', 'start': 738, 'end': 753}]\n"
     ]
    }
   ],
   "source": [
    "# Import the pipeline function from the transformers library\n",
    "# This function is used to create a named entity recognition (NER) pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create an NER pipeline using a pre-trained model for clinical chemical entities\n",
    "# The model 'pucpr/clinicalnerpt-chemical' is specifically trained for recognizing chemical entities in clinical texts\n",
    "# The aggregation_strategy=\"first\" parameter ensures that only the first sub-token of a word is used for entity recognition\n",
    "# The device=-1 parameter indicates that the pipeline should run on the CPU (use 0 or a positive integer for GPU)\n",
    "ner_pipeline = pipeline(\"ner\", model='pucpr/clinicalnerpt-chemical', aggregation_strategy=\"first\", device=-1)\n",
    "\n",
    "# Apply the NER pipeline to the text of the spaCy document\n",
    "# This step performs named entity recognition on the text, identifying chemical entities\n",
    "ner_results = ner_pipeline(doc.text)\n",
    "\n",
    "# Display the NER results\n",
    "# This will show the recognized chemical entities along with their positions and labels\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the pipeline object works for any given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">O paciente foi medicado com \n",
       "<mark class=\"entity\" style=\"background: #20f8fc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ácido\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ChemicalDrugs</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #20f8fc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    glutâmico monoidratado\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ChemicalDrugs</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #20f8fc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Cloridrato de lidocaína\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ChemicalDrugs</span>\n",
       "</mark>\n",
       " e \n",
       "<mark class=\"entity\" style=\"background: #20f8fc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Rivotril\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ChemicalDrugs</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helpers.ner import TransformerNERAnnotator, render_entity_data_from_pipeline\n",
    "\n",
    "# Define a sample text containing drug names for testing the NER pipeline\n",
    "sample_text = \"O paciente foi medicado com ácido glutâmico monoidratado, Cloridrato de lidocaína e Rivotril.\"\n",
    "\n",
    "# Apply the NER pipeline to the sample text\n",
    "# This step performs named entity recognition on the text, identifying chemical entities\n",
    "ner_results = ner_pipeline(sample_text)\n",
    "\n",
    "# Render the annotated entities in the sample text\n",
    "# This function highlights the recognized chemical entities in the text for visualization\n",
    "render_entity_data_from_pipeline(sample_text, ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform it into a labelling function that can be used with Skweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a spaCy model for Portuguese language processing\n",
    "# The \"pt_core_news_lg\" model is a large model that provides detailed linguistic annotations\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n",
    "\n",
    "# Define a custom label mapping for the NER annotator\n",
    "# This mapping translates the model's \"ChemicalDrugs\" label to \"MEDICAMENTO\"\n",
    "custom_label_mapping_lf_transformer_1 = {\n",
    "    \"ChemicalDrugs\": \"MEDICAMENTO\",\n",
    "}\n",
    "\n",
    "# Create an instance of the TransformerNERAnnotator with the specified parameters\n",
    "# - \"lf_transformer_1\": Name of the annotator\n",
    "# - model_name: Pre-trained model for recognizing chemical entities in clinical texts\n",
    "# - label_mapping: Custom label mapping defined above\n",
    "# - score_threshold: Minimum confidence score for an entity to be considered valid\n",
    "# - words_to_skip: List of words to ignore during annotation. This is useful for excluding false positives\n",
    "lf_transformer_1 = TransformerNERAnnotator(\n",
    "    \"lf_transformer_1\",\n",
    "    model_name='pucpr/clinicalnerpt-chemical',\n",
    "    label_mapping=custom_label_mapping_lf_transformer_1,\n",
    "    score_threshold=0.5,  # Set your desired threshold here\n",
    "    words_to_skip=['medicamento', 'medicamentos']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the TransformerNERAnnotator to the spaCy document\n",
    "# This step annotates the document with named entities using the pre-trained model\n",
    "lf_transformer_1(doc)\n",
    "\n",
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized entities in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"lf_transformer_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Using Zero-Shot NER with GLiNER for Enhanced Entity Recognition\n",
    "\n",
    "\n",
    "Named Entity Recognition (NER) systems traditionally focus on identifying a pre-defined set of entity types. However, this approach proves limiting when dealing with diverse and evolving entity types in real-world data. GLiNER, a novel NER model, overcomes this limitation by employing bidirectional transformer encoders, similar to the architecture of BERT, to detect **any** entity type. This capability distinguishes GLiNER from traditional NER systems and positions it as an efficient alternative to large language models (LLMs), particularly in resource-constrained environments where deploying large LLMs can be impractical.\n",
    "\n",
    "##### Evolution and Advancements in GLiNER Architecture\n",
    "\n",
    "Early iterations of GLiNER relied on older encoder architectures such as BERT and DeBERTA. These versions, trained on relatively smaller datasets, lacked the benefits of modern optimization techniques like flash attention and were limited by a restricted context window of 512 tokens, hindering their performance and applicability to tasks requiring broader textual context.\n",
    "\n",
    "To address these limitations, recent developments in GLiNER have focused on:\n",
    "\n",
    "* **Advanced Encoder Architectures:** Shifting from older architectures to more advanced ones that capitalize on the LLM2Vec method. This method transforms the initial decoder model into a bidirectional encoder, leading to enhanced performance.\n",
    "\n",
    "* **Extensive Pre-training:** Pre-training the model on a massive scale using the Wikipedia corpus and masked token prediction tasks. This results in several advantages, including:\n",
    "* **Integration of Flash Attention:** Enabling faster training and inference processes.\n",
    "* **Expanded Context Window:** Extending the context window up to 32k tokens, allowing the model to capture longer-range dependencies within the text, which is crucial for understanding complex relationships and improving accuracy in tasks requiring broader textual context.\n",
    "* **Improved Generalization:** The model's ability to generalize and perform well on unseen data is significantly enhanced.\n",
    "\n",
    "These advancements collectively contribute to a more robust and efficient GLiNER model capable of handling diverse NER tasks.\n",
    "\n",
    "##### Key Advantages of the Enhanced GLiNER Model\n",
    "\n",
    "The latest GLiNER model offers substantial improvements over its predecessors, including:\n",
    "\n",
    "* **Enhanced Performance and Generalization:** Exhibiting superior performance and better generalization capabilities due to architectural improvements and extensive pre-training.\n",
    "* **Flash Attention Support:** Integrating flash attention for faster training and inference, making it more efficient for real-world applications.\n",
    "* **Extended Context Window:** Expanding the context window to accommodate up to 32k tokens, allowing for a more thorough understanding of textual relationships and improved accuracy in tasks requiring a wider range of textual information.\n",
    "\n",
    "For a more in-depth understanding of the GLiNER model and its evolution, refer to the research paper available [here](https://arxiv.org/pdf/2406.12925)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ced6c67d6a4aa197e9bcc258c72419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fe3e6afa0b4ca29235ae49ffd72685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/682 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16383151ed14ef3914fa2034a1c865d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/43.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb27157d934c42a4b4acb4f318553b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5b86903dae49d9857e2fae8a3cab40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/3.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f884fe818a4f9aa5d1e4b9af545571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde9e844c2af4d7b9e123cd0bf445f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d5838e4ae0475ba8e516bec65be35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361796ee013a47009507c956728accb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700fe7edf1d840bfadb8d0ea2116f392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/2.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6e2c547a61413aa35d6d144f53f821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.41k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff84689301a4bd099b984e169526bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decoder model using LLM2Vec...\n"
     ]
    }
   ],
   "source": [
    "# Import the GLiNER class from the gliner library\n",
    "# GLiNER is used for named entity recognition (NER) tasks\n",
    "from gliner import GLiNER\n",
    "\n",
    "# Import the helpers.ner module\n",
    "# This module contains helper functions for NER tasks\n",
    "import helpers.ner \n",
    "\n",
    "# Load a pre-trained GLiNER model\n",
    "# The model \"knowledgator/gliner-llama-1.3B-v1.0\" is specifically trained for NER tasks\n",
    "# The from_pretrained method loads the model weights and configuration\n",
    "model_gliner_llama = GLiNER.from_pretrained(\"knowledgator/gliner-llama-1.3B-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the gliner model works for any given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 301, 'end': 310, 'text': 'REUQUINOL', 'label': 'medicamento', 'score': 0.95865797996521}, {'start': 312, 'end': 329, 'text': 'HIDROXICLOROQUINA', 'label': 'medicamento', 'score': 0.9063383936882019}, {'start': 339, 'end': 350, 'text': 'PREGABALINA', 'label': 'medicamento', 'score': 0.745392918586731}, {'start': 727, 'end': 736, 'text': 'REUQUINOL', 'label': 'medicamento', 'score': 0.9443965554237366}, {'start': 738, 'end': 753, 'text': 'HIDROXICLOQUINA', 'label': 'medicamento', 'score': 0.8267961144447327}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #b99a79; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #b99a79; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #b99a79; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #b99a79; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #b99a79; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract the text content from the spaCy document\n",
    "# This text will be used as input for the GLiNER model\n",
    "text = doc.text\n",
    "\n",
    "# Define the list of labels to be recognized by the GLiNER model\n",
    "# In this case, we are interested in recognizing entities labeled as 'medicamento'\n",
    "labels = ['medicamento']\n",
    "\n",
    "# Use the GLiNER model to predict entities in the text\n",
    "# The predict_entities method takes the text, labels, and a confidence threshold as input\n",
    "# The threshold parameter specifies the minimum confidence score for an entity to be considered valid\n",
    "result = model_gliner_llama.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Display the prediction results\n",
    "# This will show the recognized entities along with their positions and confidence scores\n",
    "print(result)\n",
    "\n",
    "# Render the annotated entities in the text using the helpers.ner module\n",
    "helpers.ner.render_entity_data_from_pipeline(text, result, label_key_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0a21184116456c8829a2077a69442b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3251bc189d24c83b151ae0e44fb6314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0176ea42dd346f9bbc37635e319d7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4c65c32f7e433b979b69ad15abfa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/3.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0cd01518e546eba9d01e437ec264e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821998f1b3554c60802d55f9ec1afdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c26cb4d89340778fd30f8cdee0d8eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/6.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7080ad823e943a48322c1179c104fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11e1c82c08c4f82b0e9485940018915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/640 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f113a45ff248b3b4086f4d7c92fed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/2.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcf1b92b71643eaada1db220e53b111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e555f0155764502bdd1b45f11148eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1339ef8fc1413f905c9d5b52907ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decoder model using LLM2Vec...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/mambaforge/envs/datacentric_ai/lib/python3.12/site-packages/gliner/model.py:90: UserWarning: Vocab size of the model (151648) does't match length of tokenizer (151649). \n",
      "                            You should to consider manually add new tokens to tokenizer or to load tokenizer with added tokens.\n",
      "  warnings.warn(f\"\"\"Vocab size of the model ({config.vocab_size}) does't match length of tokenizer ({len(self.data_processor.transformer_tokenizer)}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado REUQUINOL (HIDROXICLOROQUINA) 400mg e PREGABALINA 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento REUQUINOL (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained GLiNER model\n",
    "# The model \"knowledgator/gliner-qwen-1.5B-v1.0\" is specifically trained for NER tasks\n",
    "# The from_pretrained method loads the model weights and configuration\n",
    "model_gliner_qwen = GLiNER.from_pretrained(\"knowledgator/gliner-qwen-1.5B-v1.0\")\n",
    "\n",
    "# Extract the text content from the spaCy document\n",
    "# This text will be used as input for the GLiNER model\n",
    "text = doc.text\n",
    "\n",
    "# Define the list of labels to be recognized by the GLiNER model\n",
    "# In this case, we are interested in recognizing entities labeled as 'medicamento'\n",
    "labels = ['medicamento']\n",
    "\n",
    "# Use the GLiNER model to predict entities in the text\n",
    "# The predict_entities method takes the text, labels, and a confidence threshold as input\n",
    "# The threshold parameter specifies the minimum confidence score for an entity to be considered valid\n",
    "result = model_gliner_qwen.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Print the prediction results\n",
    "# This will show the recognized entities along with their positions and confidence scores\n",
    "print(result)\n",
    "\n",
    "# Render the annotated entities in the text\n",
    "# This function highlights the recognized entities in the text for visualization\n",
    "helpers.ner.render_entity_data_from_pipeline(text, result, label_key_name='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80aea880a3ad462983f506bef2439d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb236b4d08f4951a0cd8cd5601ef3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f79ee301ea442f5830a7c0194aa14f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/5.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18949c90e8ee44c8a44426b7a297f135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cca3f194e994d27b937c2fa253732d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e489472e8347be9b4255cf8318c20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980fd79bd2cd4f79b7b6a5f365a4fecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/970 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f0dfe0a2ce48a1a538241bda1e11a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967eccd0203f402da3a488dd6dfb6895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5a18fe065f432daaabe34a6dbad67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0478cd0c044046adbe206fd3553a9dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "trainer_state.json:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c961f3b669c401fb3fa87828fc76c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213b168f17764eeea7a1a2986b118b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe69753460745119de556dcb1c6b35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8401578da724fdfb84e71a664f4bcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #bb13e2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #bb13e2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #bb13e2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #bb13e2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #bb13e2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained GLiNER model\n",
    "# The model \"knowledgator/gliner-bi-large-v1.0\" is specifically trained for NER tasks\n",
    "# The from_pretrained method loads the model weights and configuration\n",
    "model_gliner_bi_large = GLiNER.from_pretrained(\"knowledgator/gliner-bi-large-v1.0\")\n",
    "\n",
    "# Extract the text content from the spaCy document\n",
    "# This text will be used as input for the GLiNER model\n",
    "text = doc.text\n",
    "\n",
    "# Define the list of labels to be recognized by the GLiNER model\n",
    "# In this case, we are interested in recognizing entities labeled as 'medicamento'\n",
    "labels = ['medicamento']\n",
    "\n",
    "# Use the GLiNER model to predict entities in the text\n",
    "# The predict_entities method takes the text, labels, and a confidence threshold as input\n",
    "# The threshold parameter specifies the minimum confidence score for an entity to be considered valid\n",
    "result = model_gliner_bi_large.predict_entities(text, labels, threshold=0.5)\n",
    "\n",
    "# Render the prediction results\n",
    "# This function highlights the recognized entities in the text for visualization\n",
    "# The label_key_name parameter specifies the key name for the entity labels in the result\n",
    "helpers.ner.render_entity_data_from_pipeline(text, result, label_key_name='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform them into a labelling function that can be used with Skweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db33da38a2b94ce897c29a86acccf9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decoder model using LLM2Vec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c61b49cd789417097cbc214bfa2a263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading decoder model using LLM2Vec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf7602cb32f4d4dbca4ff966f3db4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an instance of GLiNERAnnotator for the \"llama\" model\n",
    "# This annotator is configured to recognize entities labeled as \"medicamento\"\n",
    "# - annotator_name: Unique name for the annotator instance\n",
    "# - model_name: Pre-trained model for NER tasks\n",
    "# - labels: List of entity labels to recognize\n",
    "# - score_threshold: Minimum confidence score for an entity to be considered valid\n",
    "# - words_to_skip: List of words to ignore during annotation\n",
    "lf_gliner_llama = helpers.ner.GLiNERAnnotator(\n",
    "    annotator_name=\"lf_gliner_llama\",\n",
    "    model_name=\"knowledgator/gliner-llama-1.3B-v1.0\",\n",
    "    labels=[\"medicamento\"],\n",
    "    score_threshold=0.6,\n",
    "    words_to_skip=[\"medicamento\", \"medicamentos\"],\n",
    ")\n",
    "\n",
    "# Create an instance of GLiNERAnnotator for the \"qwen\" model\n",
    "# This annotator is configured similarly to the \"llama\" annotator but uses a different pre-trained model\n",
    "lf_gliner_qwen = helpers.ner.GLiNERAnnotator(\n",
    "    annotator_name=\"lf_gliner_qwen\",\n",
    "    model_name=\"knowledgator/gliner-qwen-1.5B-v1.0\",\n",
    "    labels=[\"medicamento\"],\n",
    "    score_threshold=0.6,\n",
    "    words_to_skip=[\"medicamento\", \"medicamentos\"],\n",
    ")\n",
    "\n",
    "# Create an instance of GLiNERAnnotator for the \"bi_large\" model\n",
    "# This annotator is configured similarly to the previous annotators but uses another different pre-trained model\n",
    "lf_gliner_bi_large = helpers.ner.GLiNERAnnotator(\n",
    "    annotator_name=\"lf_gliner_bi_large\",\n",
    "    model_name=\"knowledgator/gliner-bi-large-v1.0\",\n",
    "    labels=[\"medicamento\"],\n",
    "    score_threshold=0.6,\n",
    "    words_to_skip=[\"medicamento\", \"medicamentos\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RELATÓRIO\n",
       "Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado REUQUINOL (HIDROXICLOROQUINA) 400mg e PREGABALINA 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. \n",
       "É o que importa relatar.Decido.\n",
       "PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)\n",
       "A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento REUQUINOL (HIDROXICLOQUINA) é fornecido pelo SUS.\n",
       "Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.\n",
       "Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.\n",
       "Por tais razões, rejeito a preliminar suscitada. \n",
       "PRELIMINAR DE ILEGITIMIDADE PASSIVA\n",
       "Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the GLiNER model with the \"llama\" configuration to the spaCy document\n",
    "# This step annotates the document with named entities using the \"llama\" pre-trained model\n",
    "lf_gliner_llama(doc)\n",
    "\n",
    "# Apply the GLiNER model with the \"qwen\" configuration to the spaCy document\n",
    "# This step annotates the document with named entities using the \"qwen\" pre-trained model\n",
    "lf_gliner_qwen(doc)\n",
    "\n",
    "# Apply the GLiNER model with the \"bi_large\" configuration to the spaCy document\n",
    "# This step annotates the document with named entities using the \"bi_large\" pre-trained model\n",
    "lf_gliner_bi_large(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized entities in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"lf_gliner_llama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado REUQUINOL (HIDROXICLOROQUINA) 400mg e PREGABALINA 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento REUQUINOL (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized entities in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"lf_gliner_qwen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado REUQUINOL (HIDROXICLOROQUINA) 400mg e PREGABALINA 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized entities in the document for visualization\n",
    "skweak.utils.display_entities(doc, \"lf_gliner_bi_large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Using Zero-Shot NER with NuNER for Enhanced Entity Recognition\n",
    "\n",
    "**NuNER** is a novel approach to building a specialized NER model that leverages the power of large language models (LLMs) to enhance the efficiency of smaller, task-specific models. This approach addresses the limitations of traditional NER models, which often struggle with data efficiency and generalization across different entity types and domains.\n",
    "\n",
    "##### NuNER: A Task-Specific Foundation Model\n",
    "\n",
    "Traditional NER models, typically based on pre-trained transformer encoders like BERT, often require extensive fine-tuning on human-annotated data for specific entity types and domains. While large language models (LLMs) like GPT have shown impressive zero-shot NER capabilities, their size makes them computationally expensive for real-world applications.\n",
    "\n",
    "NuNER aims to bridge this gap by creating a **task-specific foundation model** specifically for NER. Unlike **domain-specific** foundation models like SciBERT (for scientific text) or BioBERT (for biomedical text), which are common, task-specific models are rare due to the lack of suitable datasets. NuNER leverages the power of LLMs to overcome this bottleneck.\n",
    "\n",
    "##### NuNER Creation Process:\n",
    "\n",
    "1. **Dataset Creation with LLM Annotation**:\n",
    "    - A large, diverse dataset (a subset of the C4 corpus) is automatically annotated with entity labels using an LLM (GPT-3.5). This eliminates the need for costly and time-consuming human annotation.\n",
    "    - The LLM is prompted to identify a wide range of entities and assign them appropriate concepts (entity types/topics). This unconstrained approach allows for the extraction of a diverse set of entities, going beyond traditional NER datasets.\n",
    "    - This results in a dataset with millions of entity annotations spanning hundreds of thousands of unique concepts, exhibiting a more realistic long-tailed distribution of concept frequencies compared to curated datasets.\n",
    "\n",
    "2. **Pre-training with Contrastive Learning**:\n",
    "    - A smaller model (RoBERTa-base) is further pre-trained on this LLM-annotated dataset using contrastive learning. This method helps the model learn to distinguish between similar entities by focusing on contrasting features.\n",
    "\n",
    "##### Factors Influencing NuNER's Performance:\n",
    "\n",
    "Ablation studies reveal that **concept diversity** and **dataset size** are crucial factors contributing to NuNER’s performance.\n",
    "\n",
    "- Increasing the variety of concepts in the pre-training dataset leads to better generalization across different entity types.\n",
    "- Larger pre-training datasets further improve performance, indicating the model’s capacity to learn from more data.\n",
    "\n",
    "Surprisingly, **text diversity** (using C4 vs. Wikipedia) has a less significant impact when the LLM annotation process remains consistent. This highlights the importance of the LLM-driven annotation in capturing diverse entities and concepts.\n",
    "\n",
    "NuNER demonstrates the feasibility of leveraging LLMs to develop highly effective and efficient task-specific models for NER. This approach holds immense potential for other NLP tasks, paving the way for more accessible and adaptable solutions in real-world applications.\n",
    "\n",
    "> For a more in-depth understanding of the NuNER model and its evolution, refer to the research paper available [here](https://arxiv.org/abs/2402.15343)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the NuNer model works for any given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e38080bb3c4db5af11d73e240339f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae66bd4047744a8b9a582d50fed3276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NuZero_token_token_metrics.txt:   0%|          | 0.00/961 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd9e62225643f78f0f7e908cfc27d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "gliner_config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946aa7ea4c9e4b6da70dc52100dbc521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f496e26f82543f4abc1d94b37e1bb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2949140eda416bad8685d16fd46267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348c5486f33c4ce68a1aa3f43de4ebd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "zero_shot_performance_unzero_token.png:   0%|          | 0.00/43.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba05e74a3ffb4292a7cfa5cf456345e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b276b4b80b8145058e079a4555c0ffab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120616ba70744531a068153b8b30bc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/mambaforge/envs/datacentric_ai/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 301, 'end': 310, 'text': 'REUQUINOL', 'label': 'medicamento', 'score': 0.9951326251029968}, {'start': 312, 'end': 329, 'text': 'HIDROXICLOROQUINA', 'label': 'medicamento', 'score': 0.6647377610206604}, {'start': 339, 'end': 350, 'text': 'PREGABALINA', 'label': 'medicamento', 'score': 0.9287405014038086}, {'start': 727, 'end': 736, 'text': 'REUQUINOL', 'label': 'medicamento', 'score': 0.989434540271759}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #74ba27; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #74ba27; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #74ba27; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #74ba27; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained NuNerZero model\n",
    "# The from_pretrained method loads the model weights and configuration\n",
    "nuner_zero = GLiNER.from_pretrained(\"numind/NuNerZero\")\n",
    "\n",
    "# Use the GLiNER model to predict entities in the text\n",
    "# The predict_entities method takes the text and labels as input\n",
    "# The labels specify the types of entities to recognize\n",
    "result = nuner_zero.predict_entities(text, labels)\n",
    "\n",
    "# Merge adjacent entities in the prediction results\n",
    "# This step combines entities that are next to each other into a single entity\n",
    "# The merge_adjacent_entities function takes the prediction results and the original text as input\n",
    "result = helpers.ner.merge_adjacent_entities(result, text)\n",
    "\n",
    "# Display the final prediction results\n",
    "# This will show the recognized entities along with their positions and confidence scores\n",
    "print(result)\n",
    "\n",
    "# Render the annotated entities in the text\n",
    "# This function highlights the recognized entities in the text for visualization\n",
    "helpers.ner.render_entity_data_from_pipeline(text, result, label_key_name='label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform it into a labelling function that can be used with Skweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a58f182d0a4aa7a66b6728e9e408e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the annotated entities in the document\n",
    "# This function highlights the recognized entities in the document for visualization\n",
    "lf_nuner_zero = helpers.ner.GLiNERAnnotator(\n",
    "    annotator_name=\"lf_nuner_zero\",\n",
    "    model_name=\"numind/NuNerZero\",\n",
    "    labels=[\"medicamento\"],\n",
    "    score_threshold=0.6,\n",
    "    words_to_skip=[\"medicamento\", \"medicamentos\"],\n",
    ")\n",
    "\n",
    "lf_nuner_zero(doc)\n",
    "skweak.utils.display_entities(doc, \"lf_nuner_zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Using Zero-Shot NER with LLMs and Function Calling Capabilities\n",
    "\n",
    "Instruction-tuned large language models (LLMs) have revolutionized natural language processing tasks, including Named Entity Recognition (NER). These powerful models, such as GPT-4, can perform zero-shot learning, enabling them to tackle tasks without explicit training on labeled data. By leveraging the function calling capabilities of LLMs, we can create custom functions that extract entities from text, effectively performing NER without extensive training or fine-tuning.\n",
    "\n",
    "##### LangChain: Bridging LLMs and NER Tasks\n",
    "\n",
    "[LangChain](https://www.langchain.com/) serves as a crucial intermediary, simplifying the process of interacting with LLMs for various language-related tasks, including NER. This platform offers several key features that make it an ideal choice for integrating LLMs into NER workflows:\n",
    "\n",
    "1. **Seamless LLM Integration**: LangChain supports a wide range of popular LLMs, including OpenAI's GPT models and Google's BERT, ensuring compatibility with state-of-the-art language models.\n",
    "\n",
    "2. **Intuitive API and Documentation**: The platform provides a well-documented, user-friendly API, complete with code examples and tutorials, assisting easy incorporation of LLMs into applications.\n",
    "\n",
    "3. **Flexible Input and Output Handling**: LangChain supports various input formats and offers customizable output handling, allowing for versatile processing of diverse content types.\n",
    "\n",
    "4. **Task-Specific Modules**: Pre-configured modules optimized for common language tasks, including NER, streamline the process of achieving high-quality results.\n",
    "\n",
    "##### Implementing NER with LangChain and LLMs\n",
    "\n",
    "To perform NER using LangChain and LLMs, follow these general steps:\n",
    "\n",
    "1. **Setup and Installation**: Install LangChain and configure necessary API keys for the chosen LLM.\n",
    "\n",
    "2. **Model Initialization**: Import required LangChain modules and initialize the LLM instances with desired configurations.\n",
    "\n",
    "3. **Input Preparation**: Preprocess and format the input text to ensure compatibility with the chosen LLM.\n",
    "\n",
    "4. **Entity Extraction**: Use LangChain's API to pass the prepared input to the LLM and generate output containing extracted entities.\n",
    "\n",
    "5. **Post-processing**: Process the generated output to extract relevant entity information and integrate it into your application's workflow.\n",
    "\n",
    "##### Structured Data Extraction with Pydantic\n",
    "\n",
    "To systematically extract and structure entity information, we employ Pydantic, a powerful library for data validation and settings management using Python type annotations. Pydantic allows us to define schemas that act as blueprints for the entities we wish to extract, ensuring consistency and adherence to predefined formats.\n",
    "\n",
    "###### Key Benefits of Using Pydantic for NER\n",
    "\n",
    "- **Data Consistency**: Schemas ensure extracted data follows a uniform structure.\n",
    "- **Type Validation**: Pydantic's type checking reduces the risk of errors in extracted data.\n",
    "- **Improved Readability**: Declarative schema definitions enhance code maintainability.\n",
    "- **Efficient Processing**: Structured data facilitates easier analysis and downstream task integration.\n",
    "\n",
    "> With LLMs, we transform unstructured text into structured, actionable data, enhancing both the efficiency and effectiveness of legal text analysis.\n",
    "\n",
    "We'll use OpenAI GPT-4o-mini to perform zero-shot NER on legal documents, extracting entities with high accuracy and minimal manual intervention. This approach showcases the power of LLMs in automating complex NER tasks and streamlining information extraction processes.\n",
    "\n",
    "> *Note: You'll need to an OpenAI API key to access GPT-4o-mini through LangChain. Ensure you have the necessary permissions and credentials to apply the model effectively.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 characters of API key: sk-zY\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# This is useful for keeping sensitive information like API keys out of your code\n",
    "load_dotenv()  # You are expected to have a .env file with the OpenAI API KEY `OPENAI_API_KEY`\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "# We're only displaying the first 5 characters for security reasons\n",
    "# This is a good practice to verify the key is loaded without exposing it entirely\n",
    "api_key_preview = os.getenv('OPENAI_API_KEY')[:5]\n",
    "print(f\"First 5 characters of API key: {api_key_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Oi! Como posso ajudar você hoje?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 9, 'total_tokens': 17, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-27aa44c8-dff1-4a03-8687-b24805a78b5a-0', usage_metadata={'input_tokens': 9, 'output_tokens': 8, 'total_tokens': 17})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a Pydantic model for Medicamento\n",
    "# This model represents information about a medication\n",
    "# Note that I'm using Portuguese text to define this class, as it will be passed to the Ollama model and I want it to work with Portuguese texts. This kinda \"prime\" the model to work with Portuguese texts.\n",
    "class Medicamento(BaseModel):\n",
    "    \"\"\"\n",
    "    Informação sobre um medicamento.\n",
    "    \"\"\"\n",
    "    nome: Optional[str] = Field(\n",
    "        default_factory=str,\n",
    "        description=\"Nome comercial ou genérico do medicamento.\"\n",
    "    )\n",
    "    principio_ativo: Optional[str] = Field(\n",
    "        default_factory=str,\n",
    "        description=\"Princípio ativo do medicamento.\"\n",
    "    )\n",
    "    dosagem: Optional[str] = Field(\n",
    "        default_factory=str,\n",
    "        description=\"Dosagem do medicamento.\"\n",
    "    )\n",
    "\n",
    "# Define a Pydantic model for a list of Medicamentos\n",
    "# This model represents a list of medications\n",
    "class ListaMedicamentos(BaseModel):\n",
    "    \"\"\"\n",
    "    Lista de medicamentos.\n",
    "    \"\"\"\n",
    "    medicamentos: Optional[List[Medicamento]] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Lista de medicamentos.\"\n",
    "    )\n",
    "\n",
    "# Create a chat prompt template for the model\n",
    "# The system message primes the model to extract named entities related to medications\n",
    "# The human message is a placeholder for the text input\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Você é um algoritmo perfeito para extração de entidades nomeadas sobre medicamentos. \"\n",
    "            \"Você deve extrair informações relevantes do texto, exatamente como está escrito no texto. \"\n",
    "            \"Se você não souber o valor de um atributo solicitado para extrair, retorne nulo para o valor do atributo.\",\n",
    "        ),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "# - model: Specify the version of the model to use\n",
    "# - temperature: Controls the randomness of the output (0.0 to 1.0)\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Invoke the model with a test message to ensure it's working\n",
    "model_openai.invoke('Oi!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an extractor by combining the prompt template with the OpenAI model\n",
    "# The extractor is configured to output structured data in the form of ListaMedicamentos\n",
    "# include_raw=True ensures that the raw response from the model is included in the output\n",
    "# with_retry is used to handle retries in case of failures, with a maximum of 3 attempts\n",
    "# wait_exponential_jitter=True adds randomness to the wait time between retries to avoid collision\n",
    "openai_ner = prompt | model_openai.with_structured_output(ListaMedicamentos, include_raw=False).with_retry(\n",
    "    stop_after_attempt=3,  # Retry up to 3 times in case of failure\n",
    "    wait_exponential_jitter=True  # Add randomness to the wait time between retries\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the Zero-Shot model works for any given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medicamentos=[Medicamento(nome='REUQUINOL', principio_ativo='HIDROXICLOROQUINA', dosagem='400mg'), Medicamento(nome='PREGABALINA', principio_ativo=None, dosagem='75 mg')]\n"
     ]
    }
   ],
   "source": [
    "result = openai_ner.invoke(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform it into a labelling function that can be used with Skweak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">RELATÓRIO<br>Trata-se de Ação Ordinária, com pedido de tutela antecipada, proposta por WIGNETT NASCIMENTO SILVA em face da UNIÃO, do ESTADO DO RIO GRANDE DO NORTE e do MUNICÍPIO DE MOSSORÓ/RN, por meio da qual a parte autora pleiteia o fornecimento imediato, de forma gratuita, do medicamento denominado \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    HIDROXICLOROQUINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       ") 400mg e \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PREGABALINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " 75 mg, nas quantidades e formas prescritas pela médica que acompanha a parte autora, no escopo de tratar SÍNDROME DE SJÖGREN ASSOCIADO A POLIARTRALGIA CRÔNICA (CID 10 M35 / M 25.5), de que se diz portadora. <br>É o que importa relatar.Decido.<br>PRELIMINAR DE FALTA DE INTERESSE DE AGIR (UNIÃO)<br>A União alega que faltaria à parte autora interesse de agir, uma vez que o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    REUQUINOL\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">medicamento</span>\n",
       "</mark>\n",
       " (HIDROXICLOQUINA) é fornecido pelo SUS.<br>Ocorre que, consoante declaração expedida pelos próprios agentes do Poder Público, o medicamento se encontra em falta.<br>Assim sendo, o provimento jurisdicional buscado reveste-se de utilidade, uma vez que, não obstante previsto na lista de dispensação, o fármaco não está efetivamente sendo disponibilizado à demandante.<br>Por tais razões, rejeito a preliminar suscitada. <br>PRELIMINAR DE ILEGITIMIDADE PASSIVA<br>Preambularmente, reconheço a legitimidade dos três entes públicos réus para figurar no polo passivo da presente lide.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lf_langchain_openai = helpers.ner.LangChainAnnotator(\n",
    "    annotator_name=\"lf_langchain_openai\",\n",
    "    langchain_runnable=openai_ner,\n",
    "    pydantic_model=ListaMedicamentos,\n",
    "    words_to_skip=[\"medicamento\", \"medicamentos\"],\n",
    ")\n",
    "\n",
    "lf_langchain_openai(doc)\n",
    "skweak.utils.display_entities(doc, \"lf_langchain_openai\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Using Regex Patterns for Labelling Functions\n",
    "\n",
    "Regular expressions (regex) are a powerful tool for pattern matching in text data. By defining specific patterns that correspond to entities of interest, we can create labeling functions that automatically identify these entities in the text. This approach is particularly useful for Named Entity Recognition (NER) tasks where entities follow consistent patterns or formats.\n",
    "\n",
    "##### Application to Drug Names\n",
    "\n",
    "In pharmacological data, especially when handling drug names in Portuguese, regex is exceptionally practical. The typical format for drug names includes the salt and the substance name. For instance, in \"Cloridrato de Propranolol,\" \"Cloridrato\" is the salt, and \"Propranolol\" is the substance. Recognizing such patterns is crucial for creating effective regex patterns.\n",
    "\n",
    "##### Example Pattern: Drug Names in Portuguese\n",
    "\n",
    "- **Generic Structure**: The pattern typically follows the format `salt + \" de \" + substance name`.\n",
    "- **Example**: \"Cloridrato de Propranolol\"\n",
    "- **Regex Pattern Design**: The pattern must:\n",
    "    - Identify common salts (e.g., \"Cloridrato\", \"Sulfato\").\n",
    "    - Recognize the connector \"de\".\n",
    "    - Capture the subsequent substance name.\n",
    "\n",
    "##### Crafting Effective Regex Patterns\n",
    "\n",
    "To create robust labeling functions using regex:\n",
    "\n",
    "1. **Start Broad, Refine Gradually**: Begin with a general pattern and iteratively refine it to improve precision.\n",
    "2. **Account for Variations**: Consider different spellings, abbreviations, or formatting of the same entity.\n",
    "3. **Use Capturing Groups**: Isolate specific parts of the match for further processing or validation.\n",
    "4. **Incorporate Boundaries**: Use word boundaries (\\b) to ensure you're matching whole words or phrases.\n",
    "\n",
    "##### Challenges and Considerations\n",
    "\n",
    "- **Coverage**: Ensure the regex pattern covers various common salts and their variations.\n",
    "- **Edge Cases**: Be mindful of drug names that do not follow the standard format or include additional descriptors.\n",
    "- **False Positives/Negatives**: Regularly validate the regex against a diverse dataset to minimize incorrect labeling.\n",
    "- **Iterative Refinement**: Develop regex patterns iteratively, refining them based on practical results and edge cases encountered.\n",
    "- **Testing and Validation**: Consistently test regex patterns against annotated datasets to ensure accuracy and reliability.\n",
    "\n",
    "> **Note**: While regex is a powerful method for automating NER tasks, balance is essential. Capture as many relevant entities as possible without introducing excessive complexity that might lead to errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nitrato',\n",
       " 'amoxicilina+clavulanato',\n",
       " 'oxalato',\n",
       " 'sulfeto',\n",
       " 'selenito',\n",
       " 'isocaproato',\n",
       " 'mononitrato',\n",
       " 'bitartarato',\n",
       " 'metotrexato',\n",
       " 'hialuronato',\n",
       " 'antimoniato',\n",
       " 'maleato',\n",
       " 'fusidato',\n",
       " 'malato',\n",
       " 'subacetato',\n",
       " 'propilenoglicolato',\n",
       " 'mesilato',\n",
       " 'furoato',\n",
       " 'butilbrometo',\n",
       " 'gadopentetato',\n",
       " 'fumarato',\n",
       " 'bicarbonato',\n",
       " 'sulfato',\n",
       " 'palmitato',\n",
       " 'lisinato',\n",
       " 'cloridrato',\n",
       " 'isetionato',\n",
       " 'polissulfato',\n",
       " 'glicerofosfato',\n",
       " 'dinitrato',\n",
       " 'besilato',\n",
       " 'acetato',\n",
       " 'extrato',\n",
       " 'tosilato',\n",
       " 'pidolato',\n",
       " 'bissulfato',\n",
       " 'brimonidina+maleato',\n",
       " 'undecilato',\n",
       " 'borato',\n",
       " 'iodeto',\n",
       " 'tartarato',\n",
       " 'hemifumarato',\n",
       " 'docusato',\n",
       " 'carbonato',\n",
       " 'alfaoxofenilpropionato',\n",
       " 'hidroxinaftoato',\n",
       " 'estolato',\n",
       " 'hexamidina+cloridrato',\n",
       " 'aspartato',\n",
       " 'paracetamol+fosfato',\n",
       " 'decanoato',\n",
       " 'betacipionato',\n",
       " 'metilbrometo',\n",
       " 'hidroxibenzoato',\n",
       " 'esilato',\n",
       " 'etabonato',\n",
       " 'estradiol+acetato',\n",
       " 'pantotenato',\n",
       " 'subgalato',\n",
       " 'levodopa+cloridrato',\n",
       " 'mucato',\n",
       " 'racealfaoxobetametilbutanoato',\n",
       " 'dimaleato',\n",
       " 'micofenolato',\n",
       " 'undecilenato',\n",
       " 'propionato',\n",
       " 'selenato',\n",
       " 'gliconato',\n",
       " 'racealfa-hidroxigamametiltiobutanoato',\n",
       " 'diatrizoato',\n",
       " 'glicinato',\n",
       " 'fendizoato',\n",
       " 'triancinolona+sulfato',\n",
       " 'cloreto',\n",
       " 'alendronato',\n",
       " 'laurilsulfato',\n",
       " 'ascorbato',\n",
       " 'oleato',\n",
       " 'fosfato',\n",
       " 'levolisinato',\n",
       " 'ibandronato',\n",
       " 'lactato',\n",
       " 'trifenatato',\n",
       " 'cromoglicato',\n",
       " 'difosfato',\n",
       " 'dipropionato',\n",
       " 'citrato',\n",
       " '21-acetato',\n",
       " 'clavulanato',\n",
       " 'metilsulfato',\n",
       " 'enantato',\n",
       " 'dicloridrato',\n",
       " 'nitroprusseto',\n",
       " 'levomalato',\n",
       " 'clonixinato',\n",
       " 'hexafluoreto',\n",
       " 'hiclato',\n",
       " 'ciclossilicato',\n",
       " 'diaspartato',\n",
       " 'folinato',\n",
       " 'betametasona+sulfato',\n",
       " 'embonato',\n",
       " 'racealfaoxobetametilpentanoato',\n",
       " 'bromidrato',\n",
       " 'racealfaoxogamametilpentanoato',\n",
       " 'poliestirenossulfonato',\n",
       " 'ditosilato',\n",
       " 'cipionato',\n",
       " 'resinato',\n",
       " 'molibdato',\n",
       " 'etanolato',\n",
       " 'dobesilato',\n",
       " 'salicilato',\n",
       " 'fluoreto',\n",
       " 'divalproato',\n",
       " 'aceponato',\n",
       " 'valproato',\n",
       " 'succinato',\n",
       " 'pivalato',\n",
       " 'brometo',\n",
       " 'dorzolamida+maleato',\n",
       " 'picossulfato',\n",
       " 'valerato',\n",
       " 'hemitartarato',\n",
       " 'ferededato',\n",
       " 'colistimetato',\n",
       " 'pamoato',\n",
       " 'benzoato',\n",
       " 'etexilato',\n",
       " 'clatrato',\n",
       " 'sacarato',\n",
       " 'gadobenato',\n",
       " 'fempropionato',\n",
       " 'xinafoato',\n",
       " 'dimesilato',\n",
       " 'dicloreto',\n",
       " 'cetoconazol+dipropionato',\n",
       " 'monofosfato',\n",
       " 'hidrogenotartarato']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get some idea for salt names\n",
    "\n",
    "# Initialize an empty list to store potential salt names\n",
    "salts = []\n",
    "\n",
    "# Iterate over each drug name in the drugs list\n",
    "for drug in drugs:\n",
    "    # Split the drug name into words\n",
    "    split_drug = drug.split()\n",
    "    \n",
    "    # Check if the drug name has more than one word\n",
    "    if len(split_drug) > 1:\n",
    "        # Check if the second word is 'de'\n",
    "        if split_drug[1] == 'de':\n",
    "            # Check if the first word ends with 'ato', 'ito', or 'eto'\n",
    "            if split_drug[0].endswith('ato') or split_drug[0].endswith('ito') or split_drug[0].endswith('eto'):\n",
    "                # Append the first word to the salts list\n",
    "                salts.append(split_drug[0])\n",
    "\n",
    "# Remove duplicates from the salts list by converting it to a set and back to a list\n",
    "salts = list(set(salts))\n",
    "\n",
    "# Display the unique salt names\n",
    "salts # we'll use this list to create a regex pattern to extract drugs from the text. Check the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.No tocante à existência de outros medicamentos no mercado brasileiro com efeitos farmacológicos idênticos ou similares ao SPIRIVA RESPIMAT, o perito informou que “O medicamento em questão faz parte de um grupo farmacológico de medicamentos denominados anticolinérgicos; neste grupo temos os de curta duração, resentado no Brasil pelo BROMETO DE IPATRÓPIO que tem indicação para sintomas eventuais da DPOC e os de longa duração representados pelo BROMETO DE TIOTRÓPIO que segundo o consenso brasileiro de DPOC reduz o número de exacerbações e hospitalizações e melhora a qualidade de vida relacionada ao estado de saúde, comparado com placebo e ipratrópio”. \n",
      "25.Conquanto tal informação conflite, em princípio, com aquela constante na Cartilha de Apoio Médico e Científico ao Judiciário, disponibilizada no site da Federação das Unimeds do Estado de São Paulo (URL da qual se extrai que “Existe evidência de efetividade, mas não de superioridade da droga brometo de tiotrópio no controle dos pacientes com DPOC se comparada à terapia com salmeterol. Sua indicação deve ser limitada a casos altamente selecionados de pacientes que não respondem ou apresentam contraindicação ao tratamento convencional.” (destaquei), o senhor perito informou, ainda, que: \n",
      "“(.) no entanto, no estágio que se encontra a doença do autor considero importante manter o fornecimento do BROMETO DE TIOTRÓPIO pois trouxe benefícios ao autor reduziu o número de exacerbações e hospitalizações e melhora na qualidade de vida. \n",
      "O consenso também fala em FARAMACOECONOMIA NA DPOC informando que a doença impõe uma sobrecarga econômica sobre o indivíduo portador e a sociedade é defende que a melhor forma de economizar recursos e reduzindo a taxa de admissão hospitalar, que foi conseguido com o autor após a adição do medicamento ao tratamento.” (grifei) (anexo 28)\n"
     ]
    }
   ],
   "source": [
    "# Select the 9th document from the training set of spaCy documents\n",
    "doc = spacy_docs_train[8]\n",
    "\n",
    "# Print the text of the selected document for reference\n",
    "print(doc.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">24.No tocante à existência de outros medicamentos no mercado brasileiro com efeitos farmacológicos idênticos ou similares ao SPIRIVA RESPIMAT, o perito informou que “O medicamento em questão faz parte de um grupo farmacológico de medicamentos denominados anticolinérgicos; neste grupo temos os de curta duração, resentado no Brasil pelo \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BROMETO DE IPATRÓPIO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " que tem indicação para sintomas eventuais da DPOC e os de longa duração representados pelo \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BROMETO DE TIOTRÓPIO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " que segundo o consenso brasileiro de DPOC reduz o número de exacerbações e hospitalizações e melhora a qualidade de vida relacionada ao estado de saúde, comparado com placebo e ipratrópio”. <br>25.Conquanto tal informação conflite, em princípio, com aquela constante na Cartilha de Apoio Médico e Científico ao Judiciário, disponibilizada no site da Federação das Unimeds do Estado de São Paulo (URL da qual se extrai que “Existe evidência de efetividade, mas não de superioridade da droga \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    brometo de tiotrópio\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " no controle dos pacientes com DPOC se comparada à terapia com salmeterol. Sua indicação deve ser limitada a casos altamente selecionados de pacientes que não respondem ou apresentam contraindicação ao tratamento convencional.” (destaquei), o senhor perito informou, ainda, que: <br>“(.) no entanto, no estágio que se encontra a doença do autor considero importante manter o fornecimento do \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    BROMETO DE TIOTRÓPIO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " pois trouxe benefícios ao autor reduziu o número de exacerbações e hospitalizações e melhora na qualidade de vida. <br>O consenso também fala em FARAMACOECONOMIA NA DPOC informando que a doença impõe uma sobrecarga econômica sobre o indivíduo portador e a sociedade é defende que a melhor forma de economizar recursos e reduzindo a taxa de admissão hospitalar, que foi conseguido com o autor após a adição do medicamento ao tratamento.” (grifei) (anexo 28)</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the SaltAnnotator from the helpers.ner module\n",
    "lf_salt = helpers.ner.SaltAnnotator()\n",
    "\n",
    "# Apply the SaltAnnotator to the selected document\n",
    "lf_salt(doc)\n",
    "\n",
    "# Display the entities annotated by the SaltAnnotator using skweak's display_entities function\n",
    "skweak.utils.display_entities(doc, \"lf_salt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Apply Labeling Functions\n",
    "\n",
    "Labelling functions are fundamental to skweak. These functions can be created using various methods. Essentially, labelling functions operate by taking a Doc object as input and outputting a list of spans at the token level, each accompanied by a label.\n",
    "\n",
    "In the context of sequence labelling, the spans represent the entities that need to be identified. For tasks involving text classification, such as sentiment analysis, the span pertains to the entire text segment that requires classification, which could be a single sentence or the entire document.\n",
    "\n",
    "##### Applying a Single Labeling Function\n",
    "\n",
    "To apply an individual labeling function to a document:\n",
    "\n",
    "- Use the command: `name_of_lf_object(doc)`\n",
    "- This operation modifies the document by adding new annotations from the labeling function.\n",
    "- To verify that your labeling function has worked correctly, inspect `doc.spans[\"name_of_your_labeling_function\"]`. This attribute should contain the detected spans along with their labels, accessible via the attribute `label_`.\n",
    "\n",
    "##### Combining Multiple Labeling Functions\n",
    "\n",
    "If you have multiple labeling functions, it can be cumbersome to manage and apply each one individually. To streamline this process, use the `CombinedAnnotator` from the `base` module.\n",
    "\n",
    "##### Steps to Combine Labeling Functions:\n",
    "\n",
    "1. **Instantiate CombinedAnnotator:**\n",
    "    - Create an instance of `CombinedAnnotator`.\n",
    "\n",
    "2. **Add Each Labeling Function:**\n",
    "    - Use the `add_annotator` method to include all your labeling functions.\n",
    "    - Example:\n",
    "\n",
    "    ```python\n",
    "    combined = CombinedAnnotator()\n",
    "    combined.add_annotator(lf_object1)\n",
    "    combined.add_annotator(lf_object2)\n",
    "    ```\n",
    "\n",
    "##### Applying Combined Labeling Functions\n",
    "\n",
    "Once you have created a combined annotator:\n",
    "\n",
    "1. For a single document, apply the combined annotator as you would an individual function.\n",
    "2. For multiple documents, use the `pipe` method for efficient processing:\n",
    "    - Example: `docs = list(combined.pipe(docs))`\n",
    "        - **Lazy Evaluation:** The `pipe` method processes documents in a lazy manner, computing results only when needed, saving time and memory.\n",
    "\n",
    "\n",
    "##### Important Considerations\n",
    "\n",
    "- **Efficiency:** Using combined annotators and the `pipe` method can significantly speed up processing for large document collections.\n",
    "- **Flexibility:** You can easily add or remove labeling functions from the combined annotator as needed.\n",
    "- **Verification:** Always check the output of your labeling functions to ensure they are working as expected.\n",
    "\n",
    ">\n",
    "> **Note**: After applying labeling functions, consider using the `DocBin` format for efficient storage and retrieval of large batches of annotated documents (refer to the previous section for details).\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CombinedAnnotator instance\n",
    "# The CombinedAnnotator allows us to combine multiple weak supervision sources (annotators)\n",
    "# Each annotator will provide its own annotations, which will be combined to create a final set of annotations\n",
    "combined = skweak.base.CombinedAnnotator()\n",
    "\n",
    "# Add various annotators to the CombinedAnnotator\n",
    "# Each annotator is responsible for a different type of annotation or uses a different method to generate annotations\n",
    "\n",
    "# Add a gazetteer-based annotator for drug names\n",
    "combined.add_annotator(lf_drugs_gazetteer)\n",
    "\n",
    "# Add a transformer-based annotator (e.g., BERT, RoBERTa)\n",
    "combined.add_annotator(lf_transformer_1)\n",
    "\n",
    "# Add a GLINER-based annotator using the LLaMA model\n",
    "combined.add_annotator(lf_gliner_llama)\n",
    "\n",
    "# Add a GLINER-based annotator using the Qwen model\n",
    "combined.add_annotator(lf_gliner_qwen)\n",
    "\n",
    "# Add a GLINER-based annotator using a large BiLSTM model\n",
    "combined.add_annotator(lf_gliner_bi_large)\n",
    "\n",
    "# Add a NuNERZero-based annotator\n",
    "combined.add_annotator(lf_nuner_zero)\n",
    "\n",
    "# Add a LangChain-based annotator using OpenAI's models\n",
    "combined.add_annotator(lf_langchain_openai)\n",
    "\n",
    "# Add a regex-based annotator for salt names\n",
    "combined.add_annotator(lf_salt)\n",
    "\n",
    "# Apply the combined annotators to the training documents\n",
    "# The combined.pipe method processes the documents in batches, applying all the annotators\n",
    "# This step generates the combined annotations for each document in the training dataset\n",
    "spacy_docs_train = list(combined.pipe(spacy_docs_train))\n",
    "\n",
    "# Note: The running time for this process is approximately 1 hour and 40 minutes\n",
    "# The time required may vary depending on the size of the dataset and the complexity of the annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write to data/bin/ner/spacy_docs_train_annotated.bin...done\n"
     ]
    }
   ],
   "source": [
    "# We can save the weakly annotated spacy_docs to disk to avoid running the labelling functions again\n",
    "skweak.utils.docbin_writer(spacy_docs_train, \"data/bin/ner/spacy_docs_train_annotated.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skweak\n",
    "\n",
    "# Now we can load them back\n",
    "spacy_docs_train = skweak.utils.docbin_reader(\"data/bin/ner/spacy_docs_train_annotated.bin\", spacy_model_name=\"pt_core_news_lg\")\n",
    "spacy_docs_train = list(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8 Document-Level Labelling Functions\n",
    "\n",
    "Skweak provides a powerful mechanism for creating document-level labelling functions that capitalize on the global context of a document. One significant feature is the ability to exploit label consistency within a document. This means that entities appearing multiple times within the same document are likely to belong to the same category. For example, the entity [\"Frontal\"](https://www.saudedireta.com.br/catinc/drugs/bulas/frontal.pdf) could refer to either the a brand name of the drug Alprazolam or a region in front of something, but within a single document, it is unlikely to refer to both.\n",
    "\n",
    "##### DocumentMajorityAnnotator\n",
    "\n",
    "The **DocumentMajorityAnnotator** is a labelling function designed to use this label consistency. Here’s how it works:\n",
    "\n",
    "1. **Initial Predictions**: The process begins by using predictions from another labelling function. This could be any function that assigns preliminary labels to entities within the document.\n",
    "2. **Frequency Computation**: For each unique entity string in the document, the frequency of each assigned label is calculated.\n",
    "3. **Majority Label Selection**: The most common label for each entity string is selected.\n",
    "4. **Label Assignment**: This majority label is then consistently assigned to every occurrence of the entity throughout the document.\n",
    "\n",
    "##### Aggregating Labelling Functions\n",
    "\n",
    "To accurately count the label frequencies, it is essential to aggregate predictions from multiple labelling functions. There are two main approaches to achieve this:\n",
    "\n",
    "1. **MajorityVoter**:\n",
    "    - This is the simplest and quickest method.\n",
    "    - It aggregates the predictions from all available labelling functions by taking a majority vote.\n",
    "    - This approach ensures that the most frequently predicted label across all functions is selected.\n",
    "\n",
    "2. **Generative Model**:\n",
    "    - This method is more complex and involves fitting a full generative model.\n",
    "    - The model is first fitted without the document-level functions to establish baseline predictions.\n",
    "    - It is then refitted with the document-level functions included, allowing for more refined and accurate predictions.\n",
    "    - Although this approach is more computationally intensive, it can provide more nuanced results by considering the dependencies between different labelling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MajorityVoter instance for aggregating annotations\n",
    "# The MajorityVoter combines annotations from multiple annotators using a majority voting scheme\n",
    "# \"doclevel_voter\" is the name of the voter\n",
    "# [\"MEDICAMENTO\"] specifies the entity types to consider for voting (in this case, \"MEDICAMENTO\")\n",
    "# initial_weights={\"doc_majority\":0.0} sets the initial weight for the \"doc_majority\" annotator to 0.0\n",
    "# This means we do not want to include the \"doc_majority\" annotator itself in the vote\n",
    "majority_voter = skweak.aggregation.MajorityVoter(\n",
    "    \"doclevel_voter\", \n",
    "    [\"MEDICAMENTO\"], \n",
    "    initial_weights={\"doc_majority\": 0.0}\n",
    ")\n",
    "\n",
    "# Apply the MajorityVoter to the training documents\n",
    "# The majority_voter.pipe method processes the documents in batches, applying the majority voting scheme\n",
    "# This step generates the final aggregated annotations for each document in the training dataset\n",
    "spacy_docs_train = list(majority_voter.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DocumentMajorityAnnotator instance\n",
    "# The DocumentMajorityAnnotator assigns labels to documents based on the majority vote of the annotations\n",
    "# \"doc_majority\" is the name of the annotator\n",
    "# \"doclevel_voter\" is the name of the voter used for majority voting\n",
    "# case_sensitive=False indicates that the annotation process is case-insensitive\n",
    "doc_majority = skweak.doclevel.DocumentMajorityAnnotator(\"doc_majority\", \"doclevel_voter\", case_sensitive=False)\n",
    "\n",
    "# Apply the DocumentMajorityAnnotator to the training documents\n",
    "# The doc_majority.pipe method processes the documents in batches, applying the majority voting scheme at the document level\n",
    "# This step generates the final aggregated annotations for each document in the training dataset\n",
    "spacy_docs_train = list(doc_majority.pipe(spacy_docs_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Generating Aggregated Labels\n",
    "\n",
    "Having generated annotations from multiple labeling functions, we now need to combine these potentially noisy labels into a final, high-quality dataset for our NER model. This process of consolidating annotations from various sources is known as **label aggregation**. Skweak provides two primary methods for achieving this: generative models and majority voting.\n",
    "\n",
    "#### 3.1 Aggregating Labels with Generative Models\n",
    "\n",
    "Generative models offer a powerful approach to label aggregation by learning the fundamental structure and dependencies between labels. In the context of NER, a **Hidden Markov Model (HMM)** is a particularly well-suited generative model. Let's break down how it works:\n",
    "\n",
    "1. **Sequence Representation**: The HMM treats the text as a sequence of tokens, with each token assigned a corresponding label.\n",
    "\n",
    "2. **States and Transitions**: The model assumes hidden \"states\" represent the true fundamental labels and attempts to infer these states from the observed, noisy labels assigned by our labeling functions. Transitions between states are governed by probabilities, reflecting the likelihood of moving from one label to another.\n",
    "\n",
    "3. **Learning Model Parameters**: The HMM estimates two crucial sets of parameters:\n",
    "    - **Emission Probabilities**: These represent the probability of observing a particular noisy label given a specific hidden state (true label).\n",
    "    - **Transition Probabilities**: These represent the probability of transitioning between hidden states.\n",
    "\n",
    "4. **Baum-Welch Algorithm**: This is an Expectation-Maximization (EM) algorithm variant used to estimate the emission and transition probabilities. It applies the forward-backward algorithm to compute the necessary statistics.\n",
    "\n",
    "**Process:**\n",
    "1. **Application of Labeling Functions**: Apply multiple labeling functions to the unlabeled data, resulting in a set of noisy labels.\n",
    "2. **Parameter Estimation**: Use the Baum-Welch algorithm to estimate the emission and transition probabilities for the HMM.\n",
    "3. **Label Aggregation**: The HMM combines the noisy labels to generate the most likely sequence of labels, smoothing out inconsistencies.\n",
    "\n",
    "**Advantages:**\n",
    "- **Context-Aware**: They consider the relationships between adjacent labels, leading to more coherent annotations.\n",
    "- **Noise Reduction**: By leveraging patterns in the data, they can effectively filter out noise from individual labeling functions.\n",
    "- **Uncertainty Quantification**: These models provide posterior probabilities for each label, offering insights into the model's confidence.\n",
    "\n",
    "> **Note**: While the final annotations in `doc.spans[\"name_of_aggregator\"]` show only the most likely labels, the full posterior probabilities can be accessed via `doc.spans[\"name_of_aggregator\"].attrs['probs']`.\n",
    "\n",
    "#### 3.2 Aggregating Labels with Majority Vote\n",
    "\n",
    "For situations where simplicity and computational efficiency are priorities, Skweak also offers the **Majority Vote** method.\n",
    "\n",
    "**How Majority Vote Works:**\n",
    "1. For each token, count the labels assigned by all labeling functions.\n",
    "2. Select the label with the highest count as the final annotation.\n",
    "\n",
    "**Advantages:**\n",
    "- **Simplicity**: Easy to implement and understand.\n",
    "- **Efficiency**: Computationally less demanding than generative models.\n",
    "- **Transparency**: The decision process is straightforward to interpret.\n",
    "\n",
    "**Limitations:**\n",
    "- **Lack of Context**: Doesn't consider label dependencies or sequence information.\n",
    "- **Susceptibility to Noise**: Can be heavily influenced by low-quality labeling functions if they outnumber high-quality ones.\n",
    "\n",
    "#### Choosing the Right Aggregation Method\n",
    "\n",
    "The choice between generative models and Majority Vote depends on your specific NER task and resources:\n",
    "\n",
    "1. **Data Complexity**: For tasks with complex label dependencies, generative models are often superior.\n",
    "2. **Computational Resources**: If processing time or computational power is limited, Majority Vote might be preferable.\n",
    "3. **Labeling Function Quality**: With highly reliable labeling functions, Majority Vote can perform well. For noisier functions, generative models are more robust.\n",
    "4. **Interpretability Needs**: If understanding the decision process is crucial, Majority Vote offers more transparency.\n",
    "\n",
    "> **Best Practice**: When possible, experiment with both methods and compare their performance on a validation set to determine the most effective approach for your specific NER task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 1\n",
      "Finished E-step with 826 documents\n",
      "Starting iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1  -32201.73755650             +nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 826 documents\n",
      "Starting iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         2  -31813.62040693    +388.11714957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 826 documents\n",
      "Starting iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         3  -31771.67621353     +41.94419340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished E-step with 826 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         4  -31759.32330449     +12.35290903\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the Hidden Markov Model (HMM) for weak supervision\n",
    "# The HMM is used to combine multiple weak labels into a single probabilistic label\n",
    "# \"hmm\" is the name of the Hidden Markov Model\n",
    "# labels=[\"MEDICAMENTO\"] specifies the entity types that the HMM will consider (in this case, \"MEDICAMENTO\")\n",
    "hmm = skweak.generative.HMM(\"hmm\", labels=[\"MEDICAMENTO\"])\n",
    "\n",
    "# Fit the HMM model to the training documents\n",
    "# The fit method trains the HMM model using the weak labels from the training dataset\n",
    "# This step involves learning the transition and emission probabilities from the weak labels\n",
    "# The spacy_docs_train contains the training documents with weak labels generated by the annotators\n",
    "hmm.fit(spacy_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM model with following parameters:\n",
      "Output labels: ['O', 'B-MEDICAMENTO', 'I-MEDICAMENTO']\n",
      "--------\n",
      "Start distribution:\n",
      "O                1.0\n",
      "B-MEDICAMENTO    0.0\n",
      "I-MEDICAMENTO    0.0\n",
      "dtype: float64\n",
      "--------\n",
      "Transition model:\n",
      "                  O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              0.99           0.01           0.00\n",
      "B-MEDICAMENTO  0.70           0.05           0.25\n",
      "I-MEDICAMENTO  0.66           0.00           0.34\n",
      "--------\n",
      "Labelling functions in model: ['lf_gliner_bi_large', 'lf_salt', 'lf_gliner_qwen', 'lf_langchain_openai', 'lf_transformer_1', 'doc_majority', 'drugs_gazetteer', 'lf_gliner_llama', 'lf_nuner_zero']\n",
      "Emission model for: doc_majority\n",
      "                  O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.00           0.00           0.00\n",
      "B-MEDICAMENTO  0.29           0.71           0.00\n",
      "I-MEDICAMENTO  0.43           0.05           0.52\n",
      "weights        1.00           1.00           1.00\n",
      "--------\n",
      "Emission model for: drugs_gazetteer\n",
      "                  O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.00           0.00           0.00\n",
      "B-MEDICAMENTO  0.34           0.66           0.00\n",
      "I-MEDICAMENTO  0.53           0.10           0.37\n",
      "weights        1.00           1.00           1.00\n",
      "--------\n",
      "Emission model for: lf_gliner_bi_large\n",
      "                 O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.0            0.0            0.0\n",
      "B-MEDICAMENTO  1.0            0.0            0.0\n",
      "I-MEDICAMENTO  1.0            0.0            0.0\n",
      "weights        1.0            1.0            1.0\n",
      "--------\n",
      "Emission model for: lf_gliner_llama\n",
      "                 O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.0            0.0            0.0\n",
      "B-MEDICAMENTO  1.0            0.0            0.0\n",
      "I-MEDICAMENTO  1.0            0.0            0.0\n",
      "weights        1.0            1.0            1.0\n",
      "--------\n",
      "Emission model for: lf_gliner_qwen\n",
      "                 O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.0            0.0            0.0\n",
      "B-MEDICAMENTO  1.0            0.0            0.0\n",
      "I-MEDICAMENTO  1.0            0.0            0.0\n",
      "weights        1.0            1.0            1.0\n",
      "--------\n",
      "Emission model for: lf_langchain_openai\n",
      "                 O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.0            0.0            0.0\n",
      "B-MEDICAMENTO  1.0            0.0            0.0\n",
      "I-MEDICAMENTO  1.0            0.0            0.0\n",
      "weights        1.0            1.0            1.0\n",
      "--------\n",
      "Emission model for: lf_nuner_zero\n",
      "                 O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.0            0.0            0.0\n",
      "B-MEDICAMENTO  1.0            0.0            0.0\n",
      "I-MEDICAMENTO  1.0            0.0            0.0\n",
      "weights        1.0            1.0            1.0\n",
      "--------\n",
      "Emission model for: lf_salt\n",
      "                  O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.00           0.00           0.00\n",
      "B-MEDICAMENTO  0.77           0.23           0.00\n",
      "I-MEDICAMENTO  0.38           0.01           0.62\n",
      "weights        1.00           1.00           1.00\n",
      "--------\n",
      "Emission model for: lf_transformer_1\n",
      "                  O  B-MEDICAMENTO  I-MEDICAMENTO\n",
      "O              1.00           0.00           0.00\n",
      "B-MEDICAMENTO  0.04           0.96           0.00\n",
      "I-MEDICAMENTO  0.07           0.05           0.89\n",
      "weights        1.00           1.00           1.00\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Print the learned parameters of the Hidden Markov Model (HMM)\n",
    "# The pretty_print method displays the transition and emission probabilities in a readable format\n",
    "# This helps us understand how the HMM has combined the weak labels into a single probabilistic label\n",
    "# The output will show the probabilities of transitioning between different states (labels)\n",
    "# and the probabilities of emitting different observations (tokens) given a state\n",
    "hmm.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SequentialMajorityVoter for weak supervision\n",
    "# The SequentialMajorityVoter combines annotations from multiple annotators using a majority voting scheme\n",
    "# \"maj_voter\" is the name of the voter\n",
    "# labels=[\"MEDICAMENTO\"] specifies the entity types to consider for voting (in this case, \"MEDICAMENTO\")\n",
    "# This voter will sequentially process the annotations and assign the most common label to each token\n",
    "maj_voter = skweak.voting.SequentialMajorityVoter(\"maj_voter\", labels=[\"MEDICAMENTO\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Hidden Markov Model (HMM) to the training documents\n",
    "# The hmm.pipe method processes the documents in batches, applying the HMM to generate probabilistic labels\n",
    "# This step refines the weak labels by combining them into a single probabilistic label for each token\n",
    "# The output is a list of spaCy documents with updated annotations based on the HMM\n",
    "spacy_docs_train = list(hmm.pipe(spacy_docs_train))\n",
    "\n",
    "# Apply the SequentialMajorityVoter to the training documents\n",
    "# The maj_voter.pipe method processes the documents in batches, applying the majority voting scheme\n",
    "# This step further refines the labels by assigning the most common label to each token based on the combined annotations\n",
    "# The output is a list of spaCy documents with final annotations based on the majority vote\n",
    "spacy_docs_train = list(maj_voter.pipe(spacy_docs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': {47: {'B-MEDICAMENTO': 0.999999999993701},\n",
       "  48: {'I-MEDICAMENTO': 0.9999999999693117},\n",
       "  49: {'I-MEDICAMENTO': 0.9999942257605258},\n",
       "  50: {'B-MEDICAMENTO': 0.9999999994686206},\n",
       "  51: {'I-MEDICAMENTO': 0.9999957757910435}},\n",
       " 'aggregated': True,\n",
       " 'sources': ['drugs_gazetteer',\n",
       "  'lf_transformer_1',\n",
       "  'lf_gliner_llama',\n",
       "  'lf_gliner_qwen',\n",
       "  'lf_nuner_zero',\n",
       "  'lf_langchain_openai']}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spacy_docs_train[0].spans['hmm'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': {97: {'B-MEDICAMENTO': 0.9999998236204718},\n",
       "  265: {'B-MEDICAMENTO': 0.9999999637318389},\n",
       "  266: {'I-MEDICAMENTO': 0.9997333780839212},\n",
       "  268: {'B-MEDICAMENTO': 0.9999998236204718}},\n",
       " 'aggregated': True,\n",
       " 'sources': ['drugs_gazetteer',\n",
       "  'lf_transformer_1',\n",
       "  'lf_gliner_llama',\n",
       "  'lf_nuner_zero',\n",
       "  'doc_majority']}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spacy_docs_train[2].spans['hmm'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': {94: {'B-MEDICAMENTO': 0.75},\n",
       "  97: {'B-MEDICAMENTO': 0.98780483},\n",
       "  164: {'B-MEDICAMENTO': 0.75},\n",
       "  265: {'B-MEDICAMENTO': 0.9473684},\n",
       "  266: {'B-MEDICAMENTO': 0.4736842, 'I-MEDICAMENTO': 0.4736842},\n",
       "  268: {'B-MEDICAMENTO': 0.98780483},\n",
       "  303: {'B-MEDICAMENTO': 0.75}},\n",
       " 'aggregated': True,\n",
       " 'sources': ['drugs_gazetteer',\n",
       "  'lf_transformer_1',\n",
       "  'lf_gliner_llama',\n",
       "  'lf_nuner_zero',\n",
       "  'doc_majority']}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_docs_train[2].spans['maj_voter'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drugs_gazetteer': [escitalopram, Espran, escitalopram], 'lf_transformer_1': [escitalopram, escitalopram, medicamento Espran], 'lf_gliner_llama': [escitalopram, escitalopram, Espran], 'lf_gliner_qwen': [], 'lf_gliner_bi_large': [], 'lf_nuner_zero': [escitalopram, escitalopram, Espran], 'lf_langchain_openai': [], 'lf_salt': [], 'doclevel_voter': [escitalopram, medicamento, Espran, escitalopram], 'doc_majority': [medicamento, escitalopram, medicamento, medicamento, escitalopram, medicamento], 'hmm': [escitalopram, medicamento Espran, escitalopram], 'maj_voter': [medicamento, escitalopram, medicamento, medicamento, Espran, escitalopram, medicamento]}\n"
     ]
    }
   ],
   "source": [
    "print(spacy_docs_train[2].spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">FUNDAMENTAÇÃO<br>Cuida-se de ação ordinária sob o rito sumaríssimo, com pedido de tutela antecipada, manejada por Natália Samara Araújo Rosalem em face da União e do Estado da Paraíba, objetivando a condenação dos réus no dever de fornecer à parte autora o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Micofenolato Mofetil (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CELLCEPT)\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " – 500mg, para a utilização de 3 comprimidos ao dia, nos termos da prescrição médica e enquanto perdurar a indicação clínica.<br>Em apertada síntese, a parte autora afirma ser portadora de lúpus eritematoso sistêmico (CID M 32.8) e que necessita do tratamento ora requerido. Aduz que há urgência no atendimento do seu pleito, sob risco de piora irreversível no seu caso clínico em caso de não realização do procedimento pleiteado.<br>Conforme relatado pelo médico particular Dr. Eduardo Sérgio Ramalho (CRM – 3295/PB), o medicamento requerido é a única alterantiva possível e que ainda não está sendo utilizada.<br>Contudo, segundo alega, tal medicamento não é fornecido pelo SUS, não tendo a requerente condições financeiras de adquiri-lo junto à iniciativa privada, já que ele possui um custo mínimo de R$ 613,00 (seiscentos e treze reais), conforme orçamento contido no anexo 08.<br>Após a citação, as rés apresentaram contestação.<br>É o breve relatório. Passo a decidir.<br>DA LEGITIMIDADE PASSIVA AD CAUSAM <br>Preliminarmente, destaque-se a legitimidade dos entes públicos réus para figurar no pólo passivo da presente lide.<br>A saúde, ao mesmo tempo em que consiste direito fundamental garantido a todos, constitui-se dever irrenunciável do Estado, tomado este termo em sua acepção genérica, ou seja, como Poder Estatal, englobando assim a União, os Estados Federados e os Municípios (art. 196, caput, CF/88).</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skweak.utils.display_entities(spacy_docs_train[0], \"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">FUNDAMENTAÇÃO<br>Cuida-se de ação ordinária sob o rito sumaríssimo, com pedido de tutela antecipada, manejada por Natália Samara Araújo Rosalem em face da União e do Estado da Paraíba, objetivando a condenação dos réus no dever de fornecer à parte autora o medicamento \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Micofenolato Mofetil (\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CELLCEPT)\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " – 500mg, para a utilização de 3 comprimidos ao dia, nos termos da prescrição médica e enquanto perdurar a indicação clínica.<br>Em apertada síntese, a parte autora afirma ser portadora de lúpus eritematoso sistêmico (CID M 32.8) e que necessita do tratamento ora requerido. Aduz que há urgência no atendimento do seu pleito, sob risco de piora irreversível no seu caso clínico em caso de não realização do procedimento pleiteado.<br>Conforme relatado pelo médico particular Dr. Eduardo Sérgio Ramalho (CRM – 3295/PB), o medicamento requerido é a única alterantiva possível e que ainda não está sendo utilizada.<br>Contudo, segundo alega, tal medicamento não é fornecido pelo SUS, não tendo a requerente condições financeiras de adquiri-lo junto à iniciativa privada, já que ele possui um custo mínimo de R$ 613,00 (seiscentos e treze reais), conforme orçamento contido no anexo 08.<br>Após a citação, as rés apresentaram contestação.<br>É o breve relatório. Passo a decidir.<br>DA LEGITIMIDADE PASSIVA AD CAUSAM <br>Preliminarmente, destaque-se a legitimidade dos entes públicos réus para figurar no pólo passivo da presente lide.<br>A saúde, ao mesmo tempo em que consiste direito fundamental garantido a todos, constitui-se dever irrenunciável do Estado, tomado este termo em sua acepção genérica, ou seja, como Poder Estatal, englobando assim a União, os Estados Federados e os Municípios (art. 196, caput, CF/88).</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skweak.utils.display_entities(spacy_docs_train[0], \"maj_voter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Nessa linha de intelecção, subsumindo as disposições legais ao caso ora em apreço, verifico que se encontra presente o requisito da verossimilhança das alegações contidas na petição inicial. De tato, a autora jungiu aos autos, consoante anexos 5, 6, 7 e 15, atestados e receituários elaborados por médicos, declarando que a requerente é portadora deEpisódios Depressivos e Transtorno Depressivo Recorrente (CID-10 F32 e CID 10 F33.4), confirmada, inclusive, por laudo pericial anexado aos autos, razão pela qual necessita do medicamento relacionado (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") para o tratamento.<br>Além disso, o fundado receio de dano irreparável consiste nas complicações que a ausência de tratamento pode ocasionar à saúde da autora, sofrendo até mesmo o risco de morte. <br>Por fim, observo que a autora, na condição de hipossuficiente (anexo no 6), não tem condições de arcar com a aquisição do medicamento em questão, cujo tratamento anual custa em torno de R$ 852,60. <br>D I S P O S I T I V O<br>À luz do exposto e de tudo o mais que dos autos consta, RESOLVO O MÉRITO DA PRESENTE DEMANDA, acolhendo os pedidos iniciais (art. 269, I, fine, CPC) para o fim de determinar que os réus forneçam de forma solidária e gratuita, imediatamente e independentemente do trânsito em julgado desta, porquanto, CONFIRMANDO OS EFEITOS DA TUTELA CONCEDIDA (anexo no 28), o \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    medicamento Espran\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "), de conformidade com as prescrições médicas do anexo no 6.<br>O fornecimento em apreço deverá ser acordado entre os entes da federação demandados, ficando o encargo da entrega do medicamento sob a responsabilidade dos requeridos, os quais deverão adotar os meios necessários para fazê-lo chegar à paciente.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skweak.utils.display_entities(spacy_docs_train[2], \"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Nessa linha de intelecção, subsumindo as disposições legais ao caso ora em apreço, verifico que se encontra presente o requisito da verossimilhança das alegações contidas na petição inicial. De tato, a autora jungiu aos autos, consoante anexos 5, 6, 7 e 15, atestados e receituários elaborados por médicos, declarando que a requerente é portadora deEpisódios Depressivos e Transtorno Depressivo Recorrente (CID-10 F32 e CID 10 F33.4), confirmada, inclusive, por laudo pericial anexado aos autos, razão pela qual necessita do \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    medicamento\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " relacionado (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") para o tratamento.<br>Além disso, o fundado receio de dano irreparável consiste nas complicações que a ausência de tratamento pode ocasionar à saúde da autora, sofrendo até mesmo o risco de morte. <br>Por fim, observo que a autora, na condição de hipossuficiente (anexo no 6), não tem condições de arcar com a aquisição do \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    medicamento\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " em questão, cujo tratamento anual custa em torno de R$ 852,60. <br>D I S P O S I T I V O<br>À luz do exposto e de tudo o mais que dos autos consta, RESOLVO O MÉRITO DA PRESENTE DEMANDA, acolhendo os pedidos iniciais (art. 269, I, fine, CPC) para o fim de determinar que os réus forneçam de forma solidária e gratuita, imediatamente e independentemente do trânsito em julgado desta, porquanto, CONFIRMANDO OS EFEITOS DA TUTELA CONCEDIDA (anexo no 28), o \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    medicamento\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Espran\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "), de conformidade com as prescrições médicas do anexo no 6.<br>O fornecimento em apreço deverá ser acordado entre os entes da federação demandados, ficando o encargo da entrega do \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    medicamento\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " sob a responsabilidade dos requeridos, os quais deverão adotar os meios necessários para fazê-lo chegar à paciente.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skweak.utils.display_entities(spacy_docs_train[2], \"maj_voter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(medicamento, 0.75),\n",
       " (escitalopram, 0.9878048300743103),\n",
       " (medicamento, 0.75),\n",
       " (medicamento, 0.9473683834075928),\n",
       " (Espran, 0.9473683834075928),\n",
       " (escitalopram, 0.9878048300743103),\n",
       " (medicamento, 0.75)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skweak.utils.get_spans_with_probs(spacy_docs_train[2], \"maj_voter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(escitalopram, 0.9999998236204718),\n",
       " (medicamento Espran, 0.9998666709078801),\n",
       " (escitalopram, 0.9999998236204718)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skweak.utils.get_spans_with_probs(spacy_docs_train[2], \"hmm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b24658f5ce4abaad2024317620cd9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer  # Import the AutoTokenizer from the transformers library\n",
    "from tqdm.auto import tqdm  # Import tqdm for progress bars\n",
    "import helpers.ner  # Import custom helper functions for Named Entity Recognition (NER)\n",
    "\n",
    "# Load the tokenizer for the BERT model\n",
    "# The tokenizer will handle tokenization and padding/truncation of input sequences\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# List of entity names to remove from the extracted entities\n",
    "# These are common terms that we do not want to include in our NER annotations\n",
    "entities_to_remove = ['medicamento', 'medicamentos', 'medicação', 'fármaco', 'fármacos', 'droga', 'drogas']\n",
    "\n",
    "# Initialize an empty list to store the training labels generated by the majority voter\n",
    "train_labels_maj_voter = []\n",
    "\n",
    "# Iterate over each document in the training dataset\n",
    "# tqdm is used to display a progress bar\n",
    "for doc in tqdm(spacy_docs_train):\n",
    "    text = doc.text  # Extract the text from the spaCy document\n",
    "    # Extract entities using the majority voter annotations\n",
    "    # The entities_to_remove list is used to filter out unwanted entities\n",
    "    entities = helpers.ner.extract_entities_in_gliner_format(doc, \"maj_voter\", entities_to_remove)\n",
    "    # Convert the extracted entities to IOB format\n",
    "    # IOB format is commonly used for NER tasks and stands for Inside-Outside-Beginning\n",
    "    iob_format = helpers.ner.convert_to_IOB(\n",
    "        entity_spans=[(ent['start'], ent['end'], ent['text'], ent['label']) for ent in entities],\n",
    "        input_text=text,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    # Append the text, entities, and IOB format annotations to the list\n",
    "    train_labels_maj_voter.append({\n",
    "        'text': text,\n",
    "        'entities': entities,\n",
    "        'iob': iob_format\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63152354c044486881ac6adc8d73fe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/826 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the training labels generated by the HMM\n",
    "train_labels_hmm = []\n",
    "\n",
    "# Iterate over each document in the training dataset\n",
    "# tqdm is used to display a progress bar\n",
    "for doc in tqdm(spacy_docs_train):\n",
    "    text = doc.text  # Extract the text from the spaCy document\n",
    "    # Extract entities using the HMM annotations\n",
    "    # The entities_to_remove list is used to filter out unwanted entities\n",
    "    entities = helpers.ner.extract_entities_in_gliner_format(doc, \"hmm\", entities_to_remove)\n",
    "    # Convert the extracted entities to IOB format\n",
    "    # IOB format is commonly used for NER tasks and stands for Inside-Outside-Beginning\n",
    "    iob_format = helpers.ner.convert_to_IOB(\n",
    "        entity_spans=[(ent['start'], ent['end'], ent['text'], ent['label']) for ent in entities],\n",
    "        input_text=text,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    # Append the text, entities, and IOB format annotations to the list\n",
    "    train_labels_hmm.append({\n",
    "        'text': text,\n",
    "        'entities': entities,\n",
    "        'iob': iob_format\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to render an example with named entities\n",
    "# This function uses a helper function to visualize the named entities in the text\n",
    "def render_example(example):\n",
    "    # Call the helper function to render the entity data\n",
    "    # text: The input text containing the named entities\n",
    "    # pipeline_results: The list of entities extracted from the text\n",
    "    # label_key_name: The key in the entity dictionary that contains the label (e.g., 'MEDICAMENTO')\n",
    "    # colors: A dictionary specifying the colors to use for different entity labels\n",
    "    helpers.ner.render_entity_data_from_pipeline(\n",
    "        text=example['text'],  # The input text to be rendered\n",
    "        pipeline_results=example['entities'],  # The entities extracted from the text\n",
    "        label_key_name='label',  # The key in the entity dictionary that contains the label\n",
    "        colors={'MEDICAMENTO': 'lightgreen'}  # The color to use for the 'MEDICAMENTO' label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Nessa linha de intelecção, subsumindo as disposições legais ao caso ora em apreço, verifico que se encontra presente o requisito da verossimilhança das alegações contidas na petição inicial. De tato, a autora jungiu aos autos, consoante anexos 5, 6, 7 e 15, atestados e receituários elaborados por médicos, declarando que a requerente é portadora deEpisódios Depressivos e Transtorno Depressivo Recorrente (CID-10 F32 e CID 10 F33.4), confirmada, inclusive, por laudo pericial anexado aos autos, razão pela qual necessita do medicamento relacionado (\n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") para o tratamento.<br>Além disso, o fundado receio de dano irreparável consiste nas complicações que a ausência de tratamento pode ocasionar à saúde da autora, sofrendo até mesmo o risco de morte. <br>Por fim, observo que a autora, na condição de hipossuficiente (anexo no 6), não tem condições de arcar com a aquisição do medicamento em questão, cujo tratamento anual custa em torno de R$ 852,60. <br>D I S P O S I T I V O<br>À luz do exposto e de tudo o mais que dos autos consta, RESOLVO O MÉRITO DA PRESENTE DEMANDA, acolhendo os pedidos iniciais (art. 269, I, fine, CPC) para o fim de determinar que os réus forneçam de forma solidária e gratuita, imediatamente e independentemente do trânsito em julgado desta, porquanto, CONFIRMANDO OS EFEITOS DA TUTELA CONCEDIDA (anexo no 28), o medicamento \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Espran\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "), de conformidade com as prescrições médicas do anexo no 6.<br>O fornecimento em apreço deverá ser acordado entre os entes da federação demandados, ficando o encargo da entrega do medicamento sob a responsabilidade dos requeridos, os quais deverão adotar os meios necessários para fazê-lo chegar à paciente.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_example(train_labels_maj_voter[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Nessa linha de intelecção, subsumindo as disposições legais ao caso ora em apreço, verifico que se encontra presente o requisito da verossimilhança das alegações contidas na petição inicial. De tato, a autora jungiu aos autos, consoante anexos 5, 6, 7 e 15, atestados e receituários elaborados por médicos, declarando que a requerente é portadora deEpisódios Depressivos e Transtorno Depressivo Recorrente (CID-10 F32 e CID 10 F33.4), confirmada, inclusive, por laudo pericial anexado aos autos, razão pela qual necessita do medicamento relacionado (\n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ") para o tratamento.<br>Além disso, o fundado receio de dano irreparável consiste nas complicações que a ausência de tratamento pode ocasionar à saúde da autora, sofrendo até mesmo o risco de morte. <br>Por fim, observo que a autora, na condição de hipossuficiente (anexo no 6), não tem condições de arcar com a aquisição do medicamento em questão, cujo tratamento anual custa em torno de R$ 852,60. <br>D I S P O S I T I V O<br>À luz do exposto e de tudo o mais que dos autos consta, RESOLVO O MÉRITO DA PRESENTE DEMANDA, acolhendo os pedidos iniciais (art. 269, I, fine, CPC) para o fim de determinar que os réus forneçam de forma solidária e gratuita, imediatamente e independentemente do trânsito em julgado desta, porquanto, CONFIRMANDO OS EFEITOS DA TUTELA CONCEDIDA (anexo no 28), o medicamento \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Espran\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    escitalopram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       "), de conformidade com as prescrições médicas do anexo no 6.<br>O fornecimento em apreço deverá ser acordado entre os entes da federação demandados, ficando o encargo da entrega do medicamento sob a responsabilidade dos requeridos, os quais deverão adotar os meios necessários para fazê-lo chegar à paciente.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_example(train_labels_hmm[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Grifo nosso.<br>Entendimento contrário advogaria em dissonância com o princípio da separação dos poderes, haja vista que cabe ao Poder Executivo a efetivação das políticas públicas, inclusive o fornecimento de medicamentos e/ou tratamentos, não cabendo ao Poder Judiciário, no exercício da função jurisdicional, estabelecer pormenorizadamente o modo de sua realização.<br> Isso posto, extingo o processo com resolução do mérito para julgar procedente o pedido e condenar, solidariamente, a União Federal e o Estado de Pernambuco a fornecer ao autor, o medicamento \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    QUETIAPINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ", 200mg em caixa de 180 comprimidos de modo contínuo e trimestral, enquanto durar o tratamento. Deve o autor, de seu \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    turno\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ", submeter-se ao regular trâmite administrativo para o fornecimento da medicação em questão, inclusive prazo de validade da receita médica.<br>Intime-se o Estado de Pernambuco para, no prazo de 10 dias, juntar a prova do cumprimento da determinação, sob pena de multa diária de $100,00 (cem reais).<br> Sem condenação em custas e honorários, consoante disposição do art. 55 da Lei no 9.099/95</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_example(train_labels_maj_voter[22])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Grifo nosso.<br>Entendimento contrário advogaria em dissonância com o princípio da separação dos poderes, haja vista que cabe ao Poder Executivo a efetivação das políticas públicas, inclusive o fornecimento de medicamentos e/ou tratamentos, não cabendo ao Poder Judiciário, no exercício da função jurisdicional, estabelecer pormenorizadamente o modo de sua realização.<br> Isso posto, extingo o processo com resolução do mérito para julgar procedente o pedido e condenar, solidariamente, a União Federal e o Estado de Pernambuco a fornecer ao autor, o medicamento \n",
       "<mark class=\"entity\" style=\"background: lightgreen; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    QUETIAPINA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MEDICAMENTO</span>\n",
       "</mark>\n",
       ", 200mg em caixa de 180 comprimidos de modo contínuo e trimestral, enquanto durar o tratamento. Deve o autor, de seu turno, submeter-se ao regular trâmite administrativo para o fornecimento da medicação em questão, inclusive prazo de validade da receita médica.<br>Intime-se o Estado de Pernambuco para, no prazo de 10 dias, juntar a prova do cumprimento da determinação, sob pena de multa diária de $100,00 (cem reais).<br> Sem condenação em custas e honorários, consoante disposição do art. 55 da Lei no 9.099/95</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_example(train_labels_hmm[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Training a Named Entity Recognition (NER) Model Using Weak Labels\n",
    "\n",
    "With the aggregated weak labels in place, we can now proceed to train a Named Entity Recognition (NER) model targeted at identifying drug entities in legal documents. This involves several key steps: selecting an appropriate model architecture, preparing the training data, and fine-tuning our model.\n",
    "\n",
    "#### Selecting the Model Architecture\n",
    "\n",
    "For this task, we will use a **BERT-based model**. **BERT (Bidirectional Encoder Representations from Transformers)** is known for its powerful performance on NER tasks due to its ability to capture bidirectional context in textual data. This capability is crucial for sequence labeling tasks like NER.\n",
    "\n",
    "#### Transfer Learning and Fine-tuning\n",
    "\n",
    "Instead of training a BERT model from scratch, we will use **transfer learning**. This involves leveraging a pre-trained BERT model trained on a large corpus of Portuguese text. This model will be fine-tuned on our specific task of drug entity recognition. Here are the steps involved:\n",
    "\n",
    "1. **Loading the Pre-trained Model**: Initialize the model with weights learned from a vast Portuguese text corpus.\n",
    "2. **Adapting to the Task**: Modify the model's output layer to predict drug entity labels.\n",
    "3. **Training on the Data**: Fine-tune the model using our weakly labeled datasets. This involves adjusting the parameters of the pre-trained model specifically for our NER task.\n",
    "\n",
    "#### Preparing the Training Data\n",
    "\n",
    "We will use three different versions of the annotated training data to fine-tune the BERT model:\n",
    "\n",
    "1. **HMM Dataset**:\n",
    "    - Contains labels aggregated using the Hidden Markov Model (HMM) method.\n",
    "    - Provides a probabilistic approach to label aggregation.\n",
    "\n",
    "2. **Majority Vote Dataset**:\n",
    "    - Uses the weak labels determined by the most common annotation among different labeling functions.\n",
    "    - A simple yet effective method for consensus labeling.\n",
    "\n",
    "3. **True Labels Dataset**:\n",
    "    - Includes manually annotated true labels, serving as the gold standard.\n",
    "    - This dataset is used only for educational purposes and allows us to benchmark the performance of our weakly supervised models. In real-world scenarios, we typically do not have access to such fully annotated data.\n",
    "\n",
    "#### Training the Model\n",
    "\n",
    "The training process will involve fine-tuning the BERT model on each of the three versions of the dataset. By comparing the performance of the models trained on HMM and Majority Vote datasets to the one trained on the True dataset, we can assess the effectiveness of our weak labeling strategies.\n",
    "\n",
    "##### Key Considerations\n",
    "\n",
    "- **Bidirectional Context**: BERT's ability to understand context from both directions is particularly advantageous for NER tasks.\n",
    "- **Transfer Learning**: Using a pre-trained model significantly reduces the computational resources and data required for training.\n",
    "- **Weak Labels**: Despite being weak, these labels provide considerable value and reduce the need for extensive manual annotation.\n",
    "- **Model Evaluation**: The True Labels dataset serves as a benchmark to understand the performance ceiling of our models trained with weak labels.\n",
    "\n",
    "> **Note**: The True dataset is included solely for educational insights. In practical applications, the focus is primarily on applying weak labels to minimize manual annotation efforts while still achieving high-performance NER models.\n",
    "\n",
    "By leveraging the powerful BERT architecture and carefully preparing the training data, we can effectively train an NER model to identify drug entities in legal documents using weakly supervised annotations. This approach is both cost-effective and demonstrates the practical application of advanced machine learning techniques in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.text import calculate_md5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some data preparation to make our dataset ready for 🤗 Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset with labels from a Parquet file\n",
    "df_train_with_labels = pd.read_parquet('data/ner/train_with_labels.parquet')\n",
    "\n",
    "# Convert the string representation of the labels to a list of tuples\n",
    "# The eval function evaluates the string as a Python expression, converting it to a list of tuples\n",
    "# Each tuple contains a token and its corresponding label\n",
    "df_train_with_labels['labels'] = df_train_with_labels['labels'].apply(lambda x: eval(x))\n",
    "\n",
    "# Extract the labels from the training dataset as a list of lists\n",
    "# Each inner list contains the labels for a single example in the training dataset\n",
    "train_labels_true_iob = df_train_with_labels['labels'].values.tolist()\n",
    "\n",
    "train_texts_true = df_train_with_labels['text'].values.tolist()\n",
    "train_texts_hash = [calculate_md5(text) for text in train_texts_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the labels from the validation dataset as a list of lists\n",
    "# Each inner list contains the labels for a single example in the validation dataset\n",
    "valid_labels_true_iob = df_valid['labels'].values.tolist()\n",
    "\n",
    "# Extract the labels from the test dataset as a list of lists\n",
    "# Each inner list contains the labels for a single example in the test dataset\n",
    "test_labels_true_iob = df_test['labels'].values.tolist()\n",
    "\n",
    "# Extract the text from the validation dataset as a list\n",
    "valid_texts_true = df_valid['text'].values.tolist()\n",
    "\n",
    "# Extract the text from the test dataset as a list\n",
    "test_texts_true = df_test['text'].values.tolist()\n",
    "\n",
    "# Calculate the MD5 hash for each text in the validation dataset\n",
    "valid_texts_hash = [calculate_md5(text) for text in valid_texts_true]\n",
    "\n",
    "# Calculate the MD5 hash for each text in the test dataset\n",
    "test_texts_hash = [calculate_md5(text) for text in test_texts_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and tags for each example in the training dataset\n",
    "# The list comprehension iterates over each example in train_labels_true_iob\n",
    "# For each example, it extracts the words and tags, creating two separate lists\n",
    "words_list = [[word for word, _ in example] for example in train_labels_true_iob]\n",
    "tags_list = [[tag for _, tag in example] for example in train_labels_true_iob]\n",
    "\n",
    "# Create a dictionary to store the tokens and named entity tags for the training dataset\n",
    "# 'tokens' contains the list of words for each example\n",
    "# 'ner_tags' contains the list of named entity tags for each example\n",
    "train_true_dicts = {\n",
    "    \"tokens\": words_list,\n",
    "    \"ner_tags\": tags_list,\n",
    "    \"text\": train_texts_true,\n",
    "    \"hash\": train_texts_hash\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and tags for each example in the validation dataset\n",
    "# The list comprehension iterates over each example in valid_labels_true_iob\n",
    "# For each example, it extracts the words and tags, creating two separate lists\n",
    "words_list = [[word for word, _ in example] for example in valid_labels_true_iob]\n",
    "tags_list = [[tag for _, tag in example] for example in valid_labels_true_iob]\n",
    "\n",
    "# Create a dictionary to store the tokens and named entity tags for the validation dataset\n",
    "# 'tokens' contains the list of words for each example\n",
    "# 'ner_tags' contains the list of named entity tags for each example\n",
    "valid_true_dicts = {\n",
    "    \"tokens\": words_list,\n",
    "    \"ner_tags\": tags_list,\n",
    "    \"text\": valid_texts_true,\n",
    "    \"hash\": valid_texts_hash\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate words and tags for each example in the test dataset\n",
    "# The list comprehension iterates over each example in test_labels_true_iob\n",
    "# For each example, it extracts the words and tags, creating two separate lists\n",
    "words_list = [[word for word, _ in example] for example in test_labels_true_iob]\n",
    "tags_list = [[tag for _, tag in example] for example in test_labels_true_iob]\n",
    "\n",
    "# Create a dictionary to store the tokens and named entity tags for the test dataset\n",
    "# 'tokens' contains the list of words for each example\n",
    "# 'ner_tags' contains the list of named entity tags for each example\n",
    "test_true_dicts = {\n",
    "    \"tokens\": words_list,\n",
    "    \"ner_tags\": tags_list,\n",
    "    \"text\": test_texts_true,\n",
    "    \"hash\": test_texts_hash\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the IOB format annotations from the HMM-labeled training data\n",
    "# The list comprehension iterates over each entry in train_labels_hmm\n",
    "# For each entry, it extracts the 'iob' field, which contains the IOB format annotations\n",
    "iob_train_hmm = [i['iob'] for i in train_labels_hmm]\n",
    "\n",
    "# Separate words and tags for each example in the HMM-labeled training data\n",
    "# The first list comprehension iterates over each example in iob_train_hmm\n",
    "# For each example, it extracts the words, creating a list of words for each example\n",
    "words_list = [[word for word, _ in example] for example in iob_train_hmm]\n",
    "\n",
    "# The second list comprehension iterates over each example in iob_train_hmm\n",
    "# For each example, it extracts the tags, creating a list of tags for each example\n",
    "tags_list = [[tag for _, tag in example] for example in iob_train_hmm]\n",
    "\n",
    "# Create a dictionary to store the tokens and named entity tags for the HMM-labeled training data\n",
    "# 'tokens' contains the list of words for each example\n",
    "# 'ner_tags' contains the list of named entity tags for each example\n",
    "train_hmm_dicts = {\n",
    "    \"tokens\": words_list,\n",
    "    \"ner_tags\": tags_list,\n",
    "    \"text\": train_texts_true,\n",
    "    \"hash\": train_texts_hash\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [i[0][0] for i in iob_train_hmm] == [i[0][0] for i in train_labels_true_iob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the IOB format annotations from the majority voter-labeled training data\n",
    "# The list comprehension iterates over each entry in train_labels_maj_voter\n",
    "# For each entry, it extracts the 'iob' field, which contains the IOB format annotations\n",
    "iob_train_maj_voter = [i['iob'] for i in train_labels_maj_voter]\n",
    "\n",
    "# Separate words and tags for each example in the majority voter-labeled training data\n",
    "# The first list comprehension iterates over each example in iob_train_maj_voter\n",
    "# For each example, it extracts the words, creating a list of words for each example\n",
    "words_list = [[word for word, _ in example] for example in iob_train_maj_voter]\n",
    "\n",
    "# The second list comprehension iterates over each example in iob_train_maj_voter\n",
    "# For each example, it extracts the tags, creating a list of tags for each example\n",
    "tags_list = [[tag for _, tag in example] for example in iob_train_maj_voter]\n",
    "\n",
    "# Create a dictionary to store the tokens and named entity tags for the majority voter-labeled training data\n",
    "# 'tokens' contains the list of words for each example\n",
    "# 'ner_tags' contains the list of named entity tags for each example\n",
    "train_maj_voter_dicts = {\n",
    "    \"tokens\": words_list,  # List of words for each example\n",
    "    \"ner_tags\": tags_list,  # List of named entity tags for each example\n",
    "    \"text\": train_texts_true,\n",
    "    \"hash\": train_texts_hash\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [i[0][0] for i in iob_train_maj_voter] == [i[0][0] for i in train_labels_true_iob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Dataset, Features, Sequence, Value  # Import necessary classes from the datasets library\n",
    "\n",
    "# Define the mapping from label names to label IDs\n",
    "# 'O' represents tokens that are not part of any named entity\n",
    "# 'B-MEDICAMENTO' represents the beginning of a 'MEDICAMENTO' entity\n",
    "# 'I-MEDICAMENTO' represents the inside of a 'MEDICAMENTO' entity\n",
    "label_to_id = {'O': 0, 'B-MEDICAMENTO': 1, 'I-MEDICAMENTO': 2}\n",
    "\n",
    "# Create the reverse mapping from label IDs to label names\n",
    "# This is useful for converting label IDs back to label names\n",
    "id_to_label = {v: k for k, v in label_to_id.items()}\n",
    "\n",
    "# Extract the label names from the mapping\n",
    "# This list will be used to define the ClassLabel feature in the dataset\n",
    "label_names = list(label_to_id.keys())\n",
    "\n",
    "# Define the dataset features\n",
    "# The Features class specifies the schema of the dataset\n",
    "# 'tokens' is a sequence of strings, representing the tokens in the text\n",
    "# 'ner_tags' is a sequence of ClassLabel, representing the named entity recognition tags for each token\n",
    "dataset_features = Features(\n",
    "    {\n",
    "        'tokens': Sequence(Value('string')),  # Sequence of tokens\n",
    "        'ner_tags': Sequence(ClassLabel(names=label_names)),  # Sequence of named entity recognition tags, \n",
    "        'text': Value('string'),  # The original text\n",
    "        'hash': Value('string')  # The MD5 hash of the text\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset  # Import the Dataset class from the datasets library\n",
    "\n",
    "# Create a Hugging Face Dataset from the HMM-labeled training data\n",
    "# The Dataset.from_dict method converts a dictionary to a Dataset object\n",
    "# train_hmm_dicts contains the tokens and named entity tags for the HMM-labeled training data\n",
    "# dataset_features specifies the schema of the dataset (tokens and ner_tags)\n",
    "hf_dataset_train_hmm = Dataset.from_dict(train_hmm_dicts, features=dataset_features)\n",
    "\n",
    "# Create a Hugging Face Dataset from the majority voter-labeled training data\n",
    "# train_maj_voter_dicts contains the tokens and named entity tags for the majority voter-labeled training data\n",
    "hf_dataset_train_maj_voter = Dataset.from_dict(train_maj_voter_dicts, features=dataset_features)\n",
    "\n",
    "# Create a Hugging Face Dataset from the true-labeled training data\n",
    "# train_true_dicts contains the tokens and named entity tags for the true-labeled training data\n",
    "hf_dataset_train_true = Dataset.from_dict(train_true_dicts, features=dataset_features)\n",
    "\n",
    "# Create a Hugging Face Dataset from the true-labeled validation data\n",
    "# valid_true_dicts contains the tokens and named entity tags for the true-labeled validation data\n",
    "hf_dataset_valid_true = Dataset.from_dict(valid_true_dicts, features=dataset_features)\n",
    "\n",
    "# Create a Hugging Face Dataset from the true-labeled test data\n",
    "# test_true_dicts contains the tokens and named entity tags for the true-labeled test data\n",
    "hf_dataset_test_true = Dataset.from_dict(test_true_dicts, features=dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d0ddde301f4f87becd63a26fc6153b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16284dc453b34e3f9062638e59f365ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f96615527954bbcaa9506985fb78d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ab6b22213b4976811c5a59d0c62c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9312e41ffc4fa7a35bdfd6767ff553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the Hugging Face Datasets to disk\n",
    "\n",
    "# Save the HMM-labeled training dataset to disk\n",
    "hf_dataset_train_hmm.save_to_disk('outputs/ner/hf_dataset_train_hmm')\n",
    "\n",
    "# Save the majority voter-labeled training dataset to disk\n",
    "hf_dataset_train_maj_voter.save_to_disk('outputs/ner/hf_dataset_train_maj_voter')\n",
    "\n",
    "# Save the true-labeled training dataset to disk\n",
    "hf_dataset_train_true.save_to_disk('outputs/ner/hf_dataset_train_true')\n",
    "\n",
    "# Save the true-labeled validation dataset to disk\n",
    "hf_dataset_valid_true.save_to_disk('outputs/ner/hf_dataset_valid_true')\n",
    "\n",
    "# Save the true-labeled test dataset to disk\n",
    "hf_dataset_test_true.save_to_disk('outputs/ner/hf_dataset_test_true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67035611047641c39b11c471a985534b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb66953056604251bffc5ff3949a7be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664f59057ed5406dac6f48bb20f231cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/826 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed809e15e7043f38b46057e1593338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1476cc7b2e478692202c9271a4a688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Dict, List  # Import necessary types for type annotations\n",
    "\n",
    "def tokenize_and_align_labels(examples: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input words and aligns the labels with the tokens.\n",
    "\n",
    "    Args:\n",
    "        examples: A dictionary containing the input words and the corresponding labels.\n",
    "                  - \"tokens\": List of words (tokens) for each example.\n",
    "                  - \"ner_tags\": List of named entity recognition tags for each token.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the tokenized input words and the aligned labels.\n",
    "        - \"input_ids\": List of token IDs for each example.\n",
    "        - \"attention_mask\": List of attention masks for each example.\n",
    "        - \"labels\": List of aligned labels for each token.\n",
    "    \"\"\"\n",
    "    label_all_tokens = True  # Whether to label all tokens or only the first token of each word\n",
    "    # Tokenize the input words\n",
    "    # truncation=True: Truncate sequences to the maximum length\n",
    "    # is_split_into_words=True: The input is already split into words\n",
    "    # max_length=512: Maximum length of the tokenized sequences\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n",
    "\n",
    "    aligned_labels = []  # List to store the aligned labels for each example\n",
    "    # Iterate over each example in the input data\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Get the word IDs for the current example\n",
    "        previous_word_idx = None  # Initialize the previous word index\n",
    "        label_ids = []  # List to store the aligned labels for the current example\n",
    "\n",
    "        # Iterate over each word ID in the tokenized input\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Append -100 for special tokens (e.g., [CLS], [SEP])\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])  # Append the label for the current word\n",
    "            else:\n",
    "                # Append the label for the current word if label_all_tokens is True, otherwise append -100\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx  # Update the previous word index\n",
    "\n",
    "        aligned_labels.append(label_ids)  # Append the aligned labels for the current example\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels  # Add the aligned labels to the tokenized inputs\n",
    "    return tokenized_inputs  # Return the tokenized inputs with aligned labels\n",
    "\n",
    "# Apply the tokenization and label alignment to the training and validation datasets\n",
    "# The map method applies the tokenize_and_align_labels function to each example in the dataset\n",
    "# batched=True: Process the examples in batches for efficiency\n",
    "tokenized_hf_dataset_train_hmm = hf_dataset_train_hmm.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_hf_dataset_train_maj_voter = hf_dataset_train_maj_voter.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_hf_dataset_train_true = hf_dataset_train_true.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_hf_dataset_valid_true = hf_dataset_valid_true.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_hf_dataset_test_true = hf_dataset_test_true.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-MEDICAMENTO', 'I-MEDICAMENTO'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the features of the 'ner_tags' field in the test dataset\n",
    "# The features attribute provides information about the schema of the dataset\n",
    "# 'ner_tags' is a sequence of named entity recognition tags for each token in the dataset\n",
    "# This line of code will show the details of the 'ner_tags' field, such as the possible tag values and their corresponding IDs\n",
    "hf_dataset_test_true.features['ner_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Import the NumPy library for numerical operations\n",
    "from typing import Tuple, Dict  # Import Tuple and Dict for type annotations\n",
    "import evaluate  # Import the evaluate library for computing evaluation metrics\n",
    "\n",
    "def compute_metrics_for_evaluation(predictions_and_labels: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes metrics for model evaluation.\n",
    "\n",
    "    Args:\n",
    "        predictions_and_labels: A tuple containing the model predictions and the true labels.\n",
    "                                - predictions: A NumPy array of shape (batch_size, sequence_length, num_labels)\n",
    "                                - labels: A NumPy array of shape (batch_size, sequence_length)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing precision, recall, f1, and accuracy metrics.\n",
    "    \"\"\"\n",
    "    predictions, labels = predictions_and_labels  # Unpack the predictions and labels from the input tuple\n",
    "\n",
    "    # Convert logits to actual predictions\n",
    "    # np.argmax(predictions, axis=2) selects the index of the maximum value along the last axis (num_labels)\n",
    "    # This converts the logits to the predicted label IDs\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Filter out the special tokens and convert IDs to labels\n",
    "    # true_predictions and true_labels will contain the predicted and true labels, excluding special tokens\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(prediction, label) if label != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for (pred, label) in zip(prediction, label) if label != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Load the seqeval metric for named entity recognition\n",
    "    # The seqeval metric computes precision, recall, f1, and accuracy for NER tasks\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "    \n",
    "    # Compute the evaluation metrics using the true predictions and true labels\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Return the computed metrics as a dictionary\n",
    "    # The dictionary contains the overall precision, recall, f1, and accuracy\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary classes from the transformers library\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"  # Specify the name of the pretrained BERT model\n",
    "\n",
    "# Load the tokenizer for the BERT model\n",
    "# The tokenizer will handle tokenization and padding/truncation of input sequences\n",
    "# 'neuralmind/bert-base-portuguese-cased' is the name of the pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Create a DataCollator for token classification tasks\n",
    "# The DataCollatorForTokenClassification dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "# The tokenizer is passed to the DataCollator to handle tokenization and padding\n",
    "token_classification_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer from the MODEL_NAME model\n",
    "# The model is configured for token classification with the specified number of labels\n",
    "pretrained_language_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=len(id_to_label),  # Number of unique labels in the dataset\n",
    "    id2label=id_to_label,         # Mapping from label IDs to label names\n",
    "    label2id=label_to_id          # Mapping from label names to label IDs\n",
    ")\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs/ner/bert-large-ner-true-labels',  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=7,  # Number of training epochs\n",
    "    per_device_train_batch_size=6,  # Batch size for training\n",
    "    per_device_eval_batch_size=6,   # Batch size for evaluation\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_f1\",  # Metric to determine the best model\n",
    "    greater_is_better=True,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy='epoch',  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=3e-5,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "# Create a Trainer instance to handle training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=pretrained_language_model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments defined above\n",
    "    train_dataset=tokenized_hf_dataset_train_true,  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_hf_dataset_valid_true,  # Tokenized validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for preprocessing the data\n",
    "    data_collator=token_classification_collator,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics_for_evaluation,  # Function to compute evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 04:16, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.063600</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>0.685925</td>\n",
       "      <td>0.916758</td>\n",
       "      <td>0.784718</td>\n",
       "      <td>0.982304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.752227</td>\n",
       "      <td>0.901972</td>\n",
       "      <td>0.820321</td>\n",
       "      <td>0.985769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.029915</td>\n",
       "      <td>0.766569</td>\n",
       "      <td>0.934283</td>\n",
       "      <td>0.842157</td>\n",
       "      <td>0.988515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.021161</td>\n",
       "      <td>0.868915</td>\n",
       "      <td>0.938390</td>\n",
       "      <td>0.902317</td>\n",
       "      <td>0.992855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.020430</td>\n",
       "      <td>0.884257</td>\n",
       "      <td>0.935104</td>\n",
       "      <td>0.908970</td>\n",
       "      <td>0.993209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.020196</td>\n",
       "      <td>0.888310</td>\n",
       "      <td>0.934283</td>\n",
       "      <td>0.910717</td>\n",
       "      <td>0.993258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=119, training_loss=0.05570434930757815, metrics={'train_runtime': 257.6219, 'train_samples_per_second': 22.444, 'train_steps_per_second': 0.462, 'total_flos': 1370250539182008.0, 'train_loss': 0.05570434930757815, 'epoch': 6.898550724637682})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process using the Trainer instance\n",
    "# This will train the model on the training dataset and evaluate it on the validation dataset\n",
    "# The training process will follow the configurations specified in the TrainingArguments\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.019314151257276535,\n",
       " 'eval_precision': 0.9295039164490861,\n",
       " 'eval_recall': 0.9523809523809523,\n",
       " 'eval_f1': 0.9408033826638478,\n",
       " 'eval_accuracy': 0.9940722112448357,\n",
       " 'eval_runtime': 2.2595,\n",
       " 'eval_samples_per_second': 44.258,\n",
       " 'eval_steps_per_second': 3.983,\n",
       " 'epoch': 6.898550724637682}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the tokenized test dataset using the Trainer instance\n",
    "# This will return a dictionary of evaluation metrics such as loss, precision, recall, and F1 score\n",
    "metrics_true_labels = trainer.evaluate(tokenized_hf_dataset_test_true)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "# These metrics help us understand the performance of the model on the test dataset\n",
    "metrics_true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # Import the garbage collection module\n",
    "import torch  # Import the PyTorch library\n",
    "\n",
    "# Set the pretrained language model and trainer to None\n",
    "# This helps in releasing the memory allocated to these objects\n",
    "pretrained_language_model = None\n",
    "trainer = None\n",
    "\n",
    "# Force the garbage collector to release unreferenced memory\n",
    "# This is useful to free up memory that is no longer needed\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache\n",
    "# This releases GPU memory that was allocated by PyTorch but is no longer needed\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "# The model is configured for token classification with the specified number of labels\n",
    "pretrained_language_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,  # Pretrained model name\n",
    "    num_labels=len(id_to_label),  # Number of unique labels in the dataset\n",
    "    id2label=id_to_label,  # Mapping from label IDs to label names\n",
    "    label2id=label_to_id  # Mapping from label names to label IDs\n",
    ")\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs/ner/bert-large-ner-hmm-labels',  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=7,  # Number of training epochs\n",
    "    per_device_train_batch_size=6,  # Batch size for training\n",
    "    per_device_eval_batch_size=6,  # Batch size for evaluation\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_f1\",  # Metric to determine the best model\n",
    "    greater_is_better=True,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy='epoch',  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=3e-5,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "# Create a Trainer instance to handle training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=pretrained_language_model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments defined above\n",
    "    train_dataset=tokenized_hf_dataset_train_hmm,  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_hf_dataset_valid_true,  # Tokenized validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for preprocessing the data\n",
    "    data_collator=token_classification_collator,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics_for_evaluation,  # Function to compute evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 04:18, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.055876</td>\n",
       "      <td>0.688912</td>\n",
       "      <td>0.910186</td>\n",
       "      <td>0.784240</td>\n",
       "      <td>0.982718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.046057</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.932092</td>\n",
       "      <td>0.804538</td>\n",
       "      <td>0.984302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.039361</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.952355</td>\n",
       "      <td>0.861958</td>\n",
       "      <td>0.988918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.038968</td>\n",
       "      <td>0.797115</td>\n",
       "      <td>0.953176</td>\n",
       "      <td>0.868188</td>\n",
       "      <td>0.988367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.041672</td>\n",
       "      <td>0.789809</td>\n",
       "      <td>0.950712</td>\n",
       "      <td>0.862823</td>\n",
       "      <td>0.987609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.041599</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.950712</td>\n",
       "      <td>0.862180</td>\n",
       "      <td>0.987688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=119, training_loss=0.06485164079408184, metrics={'train_runtime': 259.2494, 'train_samples_per_second': 22.303, 'train_steps_per_second': 0.459, 'total_flos': 1370250539182008.0, 'train_loss': 0.06485164079408184, 'epoch': 6.898550724637682})"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process using the Trainer instance\n",
    "# This will train the model on the training dataset and evaluate it on the validation dataset\n",
    "# The training process will follow the configurations specified in the TrainingArguments\n",
    "# During training, the model's parameters will be updated to minimize the loss function\n",
    "# The evaluation metrics will be logged at each step and at the end of each epoch\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.03901085630059242,\n",
       " 'eval_precision': 0.8185982592762254,\n",
       " 'eval_recall': 0.9561262707330123,\n",
       " 'eval_f1': 0.8820335636722606,\n",
       " 'eval_accuracy': 0.9868870127537274,\n",
       " 'eval_runtime': 2.1678,\n",
       " 'eval_samples_per_second': 46.13,\n",
       " 'eval_steps_per_second': 4.152,\n",
       " 'epoch': 6.898550724637682}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the tokenized test dataset using the Trainer instance\n",
    "# This will return a dictionary of evaluation metrics such as loss, precision, recall, and F1 score\n",
    "# The evaluation metrics help us understand how well the model performs on unseen data\n",
    "metrics_hmm_labels = trainer.evaluate(tokenized_hf_dataset_test_true)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "# These metrics provide insights into the model's performance and can be used to compare different models\n",
    "metrics_hmm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pretrained language model and trainer to None\n",
    "# This helps in releasing the memory allocated to these objects\n",
    "# By setting these variables to None, we remove their references, making them eligible for garbage collection\n",
    "pretrained_language_model = None\n",
    "trainer = None\n",
    "\n",
    "# Force the garbage collector to release unreferenced memory\n",
    "# This is useful to free up memory that is no longer needed\n",
    "# The garbage collector will clean up any objects that are no longer referenced in the code\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache\n",
    "# This releases GPU memory that was allocated by PyTorch but is no longer needed\n",
    "# Clearing the CUDA cache helps in managing GPU memory more efficiently, especially when working with large models\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model and tokenizer\n",
    "# The model is configured for token classification with the specified number of labels\n",
    "pretrained_language_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,  # Pretrained model name\n",
    "    num_labels=len(id_to_label),  # Number of unique labels in the dataset\n",
    "    id2label=id_to_label,  # Mapping from label IDs to label names\n",
    "    label2id=label_to_id  # Mapping from label names to label IDs\n",
    ")\n",
    "\n",
    "\n",
    "# Define the training arguments for the Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./outputs/ner/bert-large-ner-maj_voter-labels',  # Directory to save model checkpoints and logs\n",
    "    num_train_epochs=7,  # Number of training epochs\n",
    "    per_device_train_batch_size=6,  # Batch size for training\n",
    "    per_device_eval_batch_size=6,  # Batch size for evaluation\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    "    bf16=True,  # Use bfloat16 precision for training (if supported by hardware)\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    logging_steps=1,  # Log training metrics every step\n",
    "    eval_steps=1,  # Evaluate the model every step\n",
    "    save_steps=1,  # Save the model every step\n",
    "    metric_for_best_model=\"eval_f1\",  # Metric to determine the best model\n",
    "    greater_is_better=True,  # Higher metric value is better\n",
    "    logging_strategy=\"steps\",  # Log metrics at each step\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy='epoch',  # Save the model at the end of each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    do_train=True,  # Perform training\n",
    "    do_eval=True,  # Perform evaluation\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients over multiple steps\n",
    "    push_to_hub=False,  # Do not push the model to the Hugging Face Hub\n",
    "    learning_rate=3e-5,  # Learning rate for the optimizer\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    ")\n",
    "\n",
    "# Create a Trainer instance to handle training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=pretrained_language_model,  # The model to be trained\n",
    "    args=training_args,  # Training arguments defined above\n",
    "    train_dataset=tokenized_hf_dataset_train_maj_voter,  # Tokenized training dataset\n",
    "    eval_dataset=tokenized_hf_dataset_valid_true,  # Tokenized validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer for preprocessing the data\n",
    "    data_collator=token_classification_collator,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics_for_evaluation,  # Function to compute evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='119' max='119' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [119/119 04:16, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.057611</td>\n",
       "      <td>0.675343</td>\n",
       "      <td>0.916484</td>\n",
       "      <td>0.777649</td>\n",
       "      <td>0.982304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.050697</td>\n",
       "      <td>0.688613</td>\n",
       "      <td>0.942223</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.983702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.047453</td>\n",
       "      <td>0.757675</td>\n",
       "      <td>0.952903</td>\n",
       "      <td>0.844148</td>\n",
       "      <td>0.987580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.050153</td>\n",
       "      <td>0.779556</td>\n",
       "      <td>0.960570</td>\n",
       "      <td>0.860648</td>\n",
       "      <td>0.987855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.052997</td>\n",
       "      <td>0.766681</td>\n",
       "      <td>0.956462</td>\n",
       "      <td>0.851121</td>\n",
       "      <td>0.986861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.053237</td>\n",
       "      <td>0.766754</td>\n",
       "      <td>0.958653</td>\n",
       "      <td>0.852032</td>\n",
       "      <td>0.986901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=119, training_loss=0.06407456434167483, metrics={'train_runtime': 256.9282, 'train_samples_per_second': 22.504, 'train_steps_per_second': 0.463, 'total_flos': 1370250539182008.0, 'train_loss': 0.06407456434167483, 'epoch': 6.898550724637682})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training process using the Trainer instance\n",
    "# This will train the model on the training dataset and evaluate it on the validation dataset\n",
    "# The training process will follow the configurations specified in the TrainingArguments\n",
    "# During training, the model's parameters will be updated to minimize the loss function\n",
    "# The evaluation metrics will be logged at each step and at the end of each epoch\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.05154959857463837,\n",
       " 'eval_precision': 0.7837837837837838,\n",
       " 'eval_recall': 0.962011771000535,\n",
       " 'eval_f1': 0.8638001441268317,\n",
       " 'eval_accuracy': 0.984834098899125,\n",
       " 'eval_runtime': 2.2025,\n",
       " 'eval_samples_per_second': 45.402,\n",
       " 'eval_steps_per_second': 4.086,\n",
       " 'epoch': 6.898550724637682}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on the tokenized test dataset using the Trainer instance\n",
    "# This will return a dictionary of evaluation metrics such as loss, precision, recall, and F1 score\n",
    "# The evaluation metrics help us understand how well the model performs on unseen data\n",
    "metrics_maj_voter_labels = trainer.evaluate(tokenized_hf_dataset_test_true)\n",
    "\n",
    "# Display the evaluation metrics\n",
    "# These metrics provide insights into the model's performance and can be used to compare different models\n",
    "metrics_maj_voter_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pretrained language model and trainer to None\n",
    "# This helps in releasing the memory allocated to these objects\n",
    "# By setting these variables to None, we remove their references, making them eligible for garbage collection\n",
    "pretrained_language_model = None\n",
    "trainer = None\n",
    "\n",
    "# Force the garbage collector to release unreferenced memory\n",
    "# This is useful to free up memory that is no longer needed\n",
    "# The garbage collector will clean up any objects that are no longer referenced in the code\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache\n",
    "# This releases GPU memory that was allocated by PyTorch but is no longer needed\n",
    "# Clearing the CUDA cache helps in managing GPU memory more efficiently, especially when working with large models\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_precision</th>\n",
       "      <th>eval_recall</th>\n",
       "      <th>eval_f1</th>\n",
       "      <th>eval_accuracy</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>True Labels</th>\n",
       "      <td>0.019314</td>\n",
       "      <td>0.929504</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.940803</td>\n",
       "      <td>0.994072</td>\n",
       "      <td>2.2595</td>\n",
       "      <td>44.258</td>\n",
       "      <td>3.983</td>\n",
       "      <td>6.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HMM Labels</th>\n",
       "      <td>0.039011</td>\n",
       "      <td>0.818598</td>\n",
       "      <td>0.956126</td>\n",
       "      <td>0.882034</td>\n",
       "      <td>0.986887</td>\n",
       "      <td>2.1678</td>\n",
       "      <td>46.130</td>\n",
       "      <td>4.152</td>\n",
       "      <td>6.898551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maj Voter Labels</th>\n",
       "      <td>0.051550</td>\n",
       "      <td>0.783784</td>\n",
       "      <td>0.962012</td>\n",
       "      <td>0.863800</td>\n",
       "      <td>0.984834</td>\n",
       "      <td>2.2025</td>\n",
       "      <td>45.402</td>\n",
       "      <td>4.086</td>\n",
       "      <td>6.898551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  eval_loss  eval_precision  eval_recall   eval_f1  eval_accuracy  eval_runtime  eval_samples_per_second  eval_steps_per_second     epoch\n",
       "True Labels        0.019314        0.929504     0.952381  0.940803       0.994072        2.2595                   44.258                  3.983  6.898551\n",
       "HMM Labels         0.039011        0.818598     0.956126  0.882034       0.986887        2.1678                   46.130                  4.152  6.898551\n",
       "Maj Voter Labels   0.051550        0.783784     0.962012  0.863800       0.984834        2.2025                   45.402                  4.086  6.898551"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to compare evaluation metrics from different models\n",
    "# The DataFrame will contain metrics from three different models: True Labels, HMM Labels, and Maj Voter Labels\n",
    "\n",
    "# Create a DataFrame with metrics from the three models\n",
    "# Each row in the DataFrame corresponds to a different metric (e.g., loss, precision, recall, F1 score)\n",
    "# Each column corresponds to a different model (True Labels, HMM Labels, Maj Voter Labels)\n",
    "df_metrics = pd.DataFrame(\n",
    "    [metrics_true_labels, metrics_hmm_labels, metrics_maj_voter_labels],  # List of dictionaries containing metrics\n",
    "    index=['True Labels', 'HMM Labels', 'Maj Voter Labels']  # Index labels for the DataFrame\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "# This will show the evaluation metrics for each model in a tabular format\n",
    "# The DataFrame provides a clear and organized way to compare the performance of different models\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations\n",
    "\n",
    "In this class, we explored the **Skweak framework** for **weak supervision** in **Named Entity Recognition (NER)** tasks. This framework leverages various techniques to efficiently annotate text data and train NER models with minimal manual effort. Let's probe deeper into the key components and results of our approach.\n",
    "\n",
    "### Key Components of Skweak Framework\n",
    "\n",
    "1. **Labeling Functions**:\n",
    "    - These are user-defined functions that apply heuristic rules or external knowledge sources to label data automatically. Labeling functions can be tailored to specific tasks or domains, providing initial annotations without human intervention.\n",
    "\n",
    "2. **Generative Models**:\n",
    "    - Generative models in Skweak combine the outputs of multiple labeling functions to create a consensus label for each data instance. These models can learn the accuracy and correlations of labeling functions, thereby improving the quality of the annotations.\n",
    "\n",
    "3. **Majority Voting**:\n",
    "    - This is a simpler technique where the label assigned to a data instance is the one most frequently suggested by the labeling functions. While less sophisticated than generative models, majority voting is a quick way to aggregate labels.\n",
    "\n",
    "### Iterative Refinement Process\n",
    "\n",
    "The Skweak framework supports an iterative refinement process, allowing continuous improvement of model performance. This process involves:\n",
    "\n",
    "- **Initial Annotation**: Using labeling functions and weak supervision techniques to generate initial labels.\n",
    "- **Model Training**: Training NER models on these weakly labeled datasets.\n",
    "- **Evaluation and Feedback**: Assessing model performance and refining labeling functions or generative models based on feedback.\n",
    "- **Re-annotation**: Updating annotations with improved labeling functions and repeating the cycle.\n",
    "\n",
    "This iterative approach ensures that the NER model can adapt to evolving data requirements and improve over time.\n",
    "\n",
    "### Performance Evaluation\n",
    "\n",
    "We evaluated the performance of our approach using a test dataset. Here are the results:\n",
    "\n",
    "| Data | F1-Score | Loss|\n",
    "|----------|----------|----------|\n",
    "| **Real labels** | **0.940803** | **0.019314** |\n",
    "| HMM labels | 0.882034 | 0.039011 |\n",
    "| Majority vote labels | 0.863800 | 0.051550 |\n",
    "\n",
    "- **Real labels**: These are the gold standard manually annotated labels, providing the highest performance benchmark.\n",
    "- **HMM labels**: Labels generated by a Hidden Markov Model, which combines multiple labeling functions using a probabilistic approach.\n",
    "- **Majority vote labels**: Labels determined by the majority voting method.\n",
    "\n",
    "### Time Efficiency\n",
    "\n",
    "Manual labeling of data is time-consuming and often impractical for large datasets. In our case:\n",
    "\n",
    "- Each manual label takes approximately 144 seconds (72 seconds per person, with two people involved).\n",
    "- For 826 documents, this results in 826 x 144 seconds = 118,944 seconds, which translates to approximately 33 hours of human labor.\n",
    "\n",
    "By using weak supervision techniques, we can significantly reduce this time, making the annotation process more efficient and scalable.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Skweak framework provides a powerful and flexible approach for weak supervision in NER tasks. By leveraging labeling functions, generative models, and majority voting, we can efficiently annotate large datasets and continuously improve model performance through iterative refinement. This approach not only saves substantial time and effort but also enables the development of robust NER models adaptable to various domains and evolving data requirements.\n",
    "\n",
    "> **Note**: In real-world scenarios, the benefits of weak supervision become even more pronounced as the size of the dataset increases. The ability to efficiently and accurately label large volumes of data is crucial for the practical application of NER models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is Named Entity Recognition (NER) and what is its purpose?\n",
    "\n",
    "2. How can weak supervision techniques reduce the reliance on manual data labeling for NER tasks?\n",
    "\n",
    "3. What are some examples of methods used in labeling functions for NER?\n",
    "\n",
    "4. How does the Skweak framework contribute to improving the quality of annotations in NER?\n",
    "\n",
    "5. What is the significance of document-level labeling in the context of NER?\n",
    "\n",
    "6. How can transfer learning be leveraged to enhance NER performance, especially for specialized domains like legal documents?\n",
    "\n",
    "7. What is meant by \"iterative refinement\" in the context of NER models and their labeling functions?\n",
    "\n",
    "8. How does the time efficiency of weak supervision compare to traditional manual labeling for NER tasks?\n",
    "\n",
    "9. What key benefits does weak supervision offer in terms of cost-effectiveness and scalability for NER model development?\n",
    "\n",
    "10. Can models trained on weakly labeled data achieve comparable performance to those trained on fully annotated datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell.`\n",
    "\n",
    "<!--\n",
    "1. Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that identifies and classifies named entities mentioned in unstructured text into predefined categories such as person names, locations, organizations, dates, etc. This process essentially transforms unstructured text into structured data, which is invaluable for various downstream applications.\n",
    "\n",
    "2. Weak supervision techniques offer a way to efficiently annotate large datasets for NER tasks without the need for extensive manual labeling. Instead of manually labeling every word in a sentence, weak supervision leverages labeling functions that automatically generate noisy labels based on predefined rules, heuristics, or existing knowledge sources.\n",
    "\n",
    "3. Labeling functions can apply a variety of methods including: Gazetteer lookups (matching against lists of known entities), regular expression patterns, leveraging pre-trained transformer models like BERT, or employing zero-shot learning techniques like those used in tools such as GLiNER, NuNER, and LangChain.\n",
    "\n",
    "4. Skweak is a framework designed to improve the quality of annotations generated by these labeling functions. It combines the outputs (weak labels) from multiple labeling functions using generative models like Hidden Markov Models and applies techniques like majority voting to reconcile discrepancies, leading to more accurate and robust annotations.\n",
    "\n",
    "5. Document-level labeling functions play a crucial role in ensuring label consistency across an entire document. Instead of treating sentences in isolation, these functions consider the broader context and relationships between entities mentioned within a document. This complete approach leads to improved annotation accuracy, especially in cases where entity relationships and co-references are important for correct classification.\n",
    "\n",
    "6. Transfer learning is extremely valuable in NER, particularly when dealing with specialized domains like legal documents. Pre-trained language models like BERT capture rich linguistic representations from massive text corpora. Fine-tuning these models on a weakly labeled dataset of legal texts allows them to adapt their knowledge to the specific terminology and context of legal language, achieving high performance on tasks like drug entity recognition without requiring vast amounts of manually labeled legal data.\n",
    "\n",
    "7. Iterative refinement is a cyclical process of improvement. It involves analyzing the performance of the NER model, identifying areas where it makes mistakes, and then refining the labeling functions to address these weaknesses. This might involve adding new rules, modifying existing ones, or incorporating additional knowledge sources. The refined labels are then used to retrain or further fine-tune the model, leading to incremental improvements in its accuracy over time.\n",
    "\n",
    "8. Weak supervision offers significant time savings compared to manual labeling. By automating a substantial portion of the annotation process through labeling functions, weak supervision reduces the need for human annotators to meticulously label each data instance, especially in large-scale datasets. This efficiency is particularly valuable in real-world scenarios where data is abundant, and time constraints are often a major factor.\n",
    "\n",
    "9. From a cost-effectiveness standpoint, weak supervision reduces the reliance on expensive manual annotation, making it a more budget-friendly approach for developing NER models. Additionally, its iterative nature and ability to incorporate diverse labeling functions make it a scalable solution, adaptable to evolving data requirements and applicable across a wide range of domains.\n",
    "\n",
    "10. Yes, models trained on weakly labeled data, particularly when enhanced by techniques like transfer learning from pre-trained models and iterative refinement of labeling functions, can achieve performance comparable to models trained on fully annotated datasets, demonstrating the efficacy and practicality of weak supervision in real-world NER applications. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacentric_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
